{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laribar/SmartAITraderBot/blob/main/Vers%C3%A3o_Treino_80_20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧹 Resetar ambiente\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "# ✅ Reinstalar corretamente\n",
        "!pip install numpy==1.26.4\n",
        "!pip install ta yfinance python-binance xgboost==2.0.3"
      ],
      "metadata": {
        "id": "8S0MHRqVG9Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE tudo que estiver bugado\n",
        "!pip uninstall -y numpy pandas scipy scikit-learn tensorflow keras xgboost\n",
        "\n",
        "# Agora instala as versões corretas\n",
        "!pip install numpy==1.26.4 pandas==2.2.2 scipy==1.12.0 scikit-learn==1.4.2 tensorflow==2.18.0 keras==2.18.0 xgboost==2.0.3 ta yfinance python-binance\n"
      ],
      "metadata": {
        "id": "qwU99PKfIsOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzDCUpKv0IhK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9735716d-c940-4a53-dc90-b3b19bfd63ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Collecting mplfinance\n",
            "  Downloading mplfinance-0.12.10b0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mplfinance) (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from mplfinance) (2.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->mplfinance) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->mplfinance) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mplfinance) (1.17.0)\n",
            "Downloading mplfinance-0.12.10b0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mplfinance\n",
            "Successfully installed mplfinance-0.12.10b0\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.2.57-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting python-binance\n",
            "  Downloading python_binance-1.0.28-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting numpy (from ta)\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas (from ta)\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests>=2.31 (from yfinance)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting multitasking>=0.0.7 (from yfinance)\n",
            "  Downloading multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting platformdirs>=2.0.0 (from yfinance)\n",
            "  Downloading platformdirs-4.3.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pytz>=2022.5 (from yfinance)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting frozendict>=2.3.4 (from yfinance)\n",
            "  Downloading frozendict-2.4.6-py311-none-any.whl.metadata (23 kB)\n",
            "Collecting peewee>=3.16.2 (from yfinance)\n",
            "  Downloading peewee-3.18.0.tar.gz (948 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.0/949.0 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting beautifulsoup4>=4.11.1 (from yfinance)\n",
            "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting six (from python-binance)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting dateparser (from python-binance)\n",
            "  Downloading dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting aiohttp (from python-binance)\n",
            "  Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting websockets (from python-binance)\n",
            "  Downloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting pycryptodome (from python-binance)\n",
            "  Downloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4>=4.11.1->yfinance)\n",
            "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typing-extensions>=4.0.0 (from beautifulsoup4>=4.11.1->yfinance)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas->ta)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->ta)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests>=2.31->yfinance)\n",
            "  Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests>=2.31->yfinance)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.31->yfinance)\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests>=2.31->yfinance)\n",
            "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->python-binance)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->python-binance)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->python-binance)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->python-binance)\n",
            "  Downloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->python-binance)\n",
            "  Downloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->python-binance)\n",
            "  Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->python-binance)\n",
            "  Downloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex!=2019.02.19,!=2021.8.27,>=2015.06.24 (from dateparser->python-binance)\n",
            "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tzlocal>=0.2 (from dateparser->python-binance)\n",
            "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Downloading yfinance-0.2.57-py2.py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.8/113.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_binance-1.0.28-py2.py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.3/187.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozendict-2.4.6-py311-none-any.whl (16 kB)\n",
            "Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
            "Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading platformdirs-4.3.7-py3-none-any.whl (18 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dateparser-1.2.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.5/223.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.5/232.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
            "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.1/358.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: ta, peewee\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=fb02cf506c110fa4f2ab169a1d2d1e9e0fea2b6090b885bb8a41ac3736af0d6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "  Building wheel for peewee (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peewee: filename=peewee-3.18.0-cp311-cp311-linux_x86_64.whl size=886611 sha256=76ef7e8da46c2f43d04cdd2f401f3985d69c829a67f84014e805108485e16745\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/14/97/a6ac3d5b8971bf8ae2bf3c3037efdcf7fb5045a910a16fec00\n",
            "Successfully built ta peewee\n",
            "Installing collected packages: pytz, peewee, multitasking, websockets, urllib3, tzlocal, tzdata, typing-extensions, soupsieve, six, regex, pycryptodome, propcache, platformdirs, numpy, multidict, idna, frozenlist, frozendict, charset-normalizer, certifi, attrs, aiohappyeyeballs, yarl, requests, python-dateutil, beautifulsoup4, aiosignal, pandas, dateparser, aiohttp, yfinance, ta, python-binance\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: peewee\n",
            "    Found existing installation: peewee 3.17.9\n",
            "    Uninstalling peewee-3.17.9:\n",
            "      Successfully uninstalled peewee-3.17.9\n",
            "  Attempting uninstall: multitasking\n",
            "    Found existing installation: multitasking 0.0.11\n",
            "    Uninstalling multitasking-0.0.11:\n",
            "      Successfully uninstalled multitasking-0.0.11\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 5.3.1\n",
            "    Uninstalling tzlocal-5.3.1:\n",
            "      Successfully uninstalled tzlocal-5.3.1\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: soupsieve\n",
            "    Found existing installation: soupsieve 2.7\n",
            "    Uninstalling soupsieve-2.7:\n",
            "      Successfully uninstalled soupsieve-2.7\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: propcache\n",
            "    Found existing installation: propcache 0.3.1\n",
            "    Uninstalling propcache-0.3.1:\n",
            "      Successfully uninstalled propcache-0.3.1\n",
            "  Attempting uninstall: platformdirs\n",
            "    Found existing installation: platformdirs 4.3.7\n",
            "    Uninstalling platformdirs-4.3.7:\n",
            "      Successfully uninstalled platformdirs-4.3.7\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: multidict\n",
            "    Found existing installation: multidict 6.4.3\n",
            "    Uninstalling multidict-6.4.3:\n",
            "      Successfully uninstalled multidict-6.4.3\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: frozenlist\n",
            "    Found existing installation: frozenlist 1.6.0\n",
            "    Uninstalling frozenlist-1.6.0:\n",
            "      Successfully uninstalled frozenlist-1.6.0\n",
            "  Attempting uninstall: frozendict\n",
            "    Found existing installation: frozendict 2.4.6\n",
            "    Uninstalling frozendict-2.4.6:\n",
            "      Successfully uninstalled frozendict-2.4.6\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.1\n",
            "    Uninstalling charset-normalizer-3.4.1:\n",
            "      Successfully uninstalled charset-normalizer-3.4.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.1.31\n",
            "    Uninstalling certifi-2025.1.31:\n",
            "      Successfully uninstalled certifi-2025.1.31\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.3.0\n",
            "    Uninstalling attrs-25.3.0:\n",
            "      Successfully uninstalled attrs-25.3.0\n",
            "  Attempting uninstall: aiohappyeyeballs\n",
            "    Found existing installation: aiohappyeyeballs 2.6.1\n",
            "    Uninstalling aiohappyeyeballs-2.6.1:\n",
            "      Successfully uninstalled aiohappyeyeballs-2.6.1\n",
            "  Attempting uninstall: yarl\n",
            "    Found existing installation: yarl 1.20.0\n",
            "    Uninstalling yarl-1.20.0:\n",
            "      Successfully uninstalled yarl-1.20.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.13.4\n",
            "    Uninstalling beautifulsoup4-4.13.4:\n",
            "      Successfully uninstalled beautifulsoup4-4.13.4\n",
            "  Attempting uninstall: aiosignal\n",
            "    Found existing installation: aiosignal 1.3.2\n",
            "    Uninstalling aiosignal-1.3.2:\n",
            "      Successfully uninstalled aiosignal-1.3.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.11.15\n",
            "    Uninstalling aiohttp-3.11.15:\n",
            "      Successfully uninstalled aiohttp-3.11.15\n",
            "  Attempting uninstall: yfinance\n",
            "    Found existing installation: yfinance 0.2.56\n",
            "    Uninstalling yfinance-0.2.56:\n",
            "      Successfully uninstalled yfinance-0.2.56\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 attrs-25.3.0 beautifulsoup4-4.13.4 certifi-2025.4.26 charset-normalizer-3.4.1 dateparser-1.2.1 frozendict-2.4.6 frozenlist-1.6.0 idna-3.10 multidict-6.4.3 multitasking-0.0.11 numpy-2.2.5 pandas-2.2.3 peewee-3.18.0 platformdirs-4.3.7 propcache-0.3.1 pycryptodome-3.22.0 python-binance-1.0.28 python-dateutil-2.9.0.post0 pytz-2025.2 regex-2024.11.6 requests-2.32.3 six-1.17.0 soupsieve-2.7 ta-0.11.0 typing-extensions-4.13.2 tzdata-2025.2 tzlocal-5.3.1 urllib3-2.4.0 websockets-15.0.1 yarl-1.20.0 yfinance-0.2.57\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "charset_normalizer",
                  "dateutil",
                  "frozendict",
                  "multitasking",
                  "peewee",
                  "pytz",
                  "requests",
                  "six",
                  "yfinance"
                ]
              },
              "id": "2cbbf67626f5441291111e29b9a29a1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost==2.0.3\n",
            "  Downloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting numpy (from xgboost==2.0.3)\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting scipy (from xgboost==2.0.3)\n",
            "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, xgboost\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.2\n",
            "    Uninstalling scipy-1.15.2:\n",
            "      Successfully uninstalled scipy-1.15.2\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.1.4\n",
            "    Uninstalling xgboost-2.1.4:\n",
            "      Successfully uninstalled xgboost-2.1.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5 scipy-1.15.2 xgboost-2.0.3\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "^C\n",
            "^C\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy.strings'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-905a1d1fd187>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Mostra as versões instaladas para conferência\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracker\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrabit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m from .core import (\n\u001b[1;32m      9\u001b[0m     \u001b[0mBooster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/collective.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_LIB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_check_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_pystr_to_cstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mLOGGER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[xgboost.collective]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m from ._typing import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[0m\u001b[1;32m      6\u001b[0m                        \u001b[0mget_sum_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                        matrix, validateaxis, getdtype)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_sputils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_ulong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_namespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_docscrape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxp_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_sanity_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_mac_os_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.strings'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# ========================================================\n",
        "# 🚀 Instala ambiente correto para rodar o sistema\n",
        "# ========================================================\n",
        "\n",
        "!pip install numpy==1.26.4\n",
        "!pip install scikit-learn\n",
        "!pip install tensorflow\n",
        "!pip install mplfinance\n",
        "# Instala pacotes principais (sem travar em versões antigas quebradas)\n",
        "!pip install --upgrade --force-reinstall ta yfinance python-binance\n",
        "\n",
        "# Corrige versões específicas necessárias\n",
        "!pip install --upgrade --force-reinstall xgboost==2.0.3\n",
        "!pip install --upgrade --force-reinstall numpy==1.26.4\n",
        "!pip install --upgrade --force-reinstall scipy==1.12.0\n",
        "\n",
        "# Mostra as versões instaladas para conferência\n",
        "import xgboost\n",
        "import numpy\n",
        "import scipy\n",
        "print(\"✅ Instalação concluída!\")\n",
        "print(f\"xgboost version: {xgboost.__version__}\")\n",
        "print(f\"numpy version: {numpy.__version__}\")\n",
        "print(f\"scipy version: {scipy.__version__}\")\n",
        "\n",
        "# 🔄 (Opcional) Reinicia o runtime para carregar as libs novas:\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Caminho da pasta onde estão seus modelos\n",
        "folder_path = '/content/models'\n",
        "\n",
        "# Nome do arquivo zip que será criado\n",
        "zip_filename = 'models_backup'\n",
        "\n",
        "# Compactar toda a pasta /models em um arquivo .zip\n",
        "shutil.make_archive(zip_filename, 'zip', folder_path)\n",
        "\n",
        "print(f\"✅ Pasta '{folder_path}' compactada como '{zip_filename}.zip'.\")\n",
        "\n",
        "# Fazer download do zip para seu computador\n",
        "files.download(f'{zip_filename}.zip')\n"
      ],
      "metadata": {
        "id": "MHeKuQSrUdTR",
        "outputId": "251cc932-25c6-475c-9fb0-562b8e183093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Pasta '/content/models' compactada como 'models_backup.zip'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cf97cb65-0ce2-4c52-8c93-a240e8f2f432\", \"models_backup.zip\", 1664118)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 1. IMPORTAÇÕES\n",
        "# ====================================================\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ta\n",
        "import requests\n",
        "import time  # Para usar time.sleep()\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import pytz\n",
        "import glob\n",
        "import json\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import joblib\n",
        "import mplfinance as mpf\n",
        "from xgboost import XGBClassifier, callback\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import classification_report\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "# Fuso horário do Brasil\n",
        "BR_TZ = pytz.timezone(\"America/Sao_Paulo\")\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# BLOCO 1 - CONFIGURAÇÃO DE PASTAS E IMPORTS EXTRA\n",
        "# ====================================================\n",
        "import os\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Criar pasta onde os modelos serão salvos\n",
        "os.makedirs(\"/content/models\", exist_ok=True)\n",
        "# ====================================================\n",
        "# BLOCO 2 - SALVAR E CARREGAR MODELOS TREINADOS\n",
        "# ====================================================\n",
        "def get_model_path(asset, interval, model_type=\"xgb\"):\n",
        "  asset_clean = asset.replace(\"-\", \"\")\n",
        "  ext = \"joblib\" if model_type == \"xgb\" else \"h5\"\n",
        "  return f\"/content/models/{model_type}_model_{asset_clean}_{interval}.{ext}\"\n",
        "\n",
        "# --- XGBoost ---\n",
        "def save_xgb_model(model, asset, interval):\n",
        "  path = get_model_path(asset, interval, model_type=\"xgb\")\n",
        "  joblib.dump(model, path)\n",
        "  print(f\"💾 Modelo XGBoost salvo em: {path}\")\n",
        "\n",
        "def load_xgb_model(asset, interval):\n",
        "  path = get_model_path(asset, interval, model_type=\"xgb\")\n",
        "  if os.path.exists(path):\n",
        "      print(f\"📂 Modelo XGBoost carregado de: {path}\")\n",
        "      return joblib.load(path)\n",
        "  return None\n",
        "\n",
        "# --- LSTM ---\n",
        "def save_lstm_model(model, asset, interval):\n",
        "  path = get_model_path(asset, interval, model_type=\"lstm\")\n",
        "  model.save(path)\n",
        "  print(f\"💾 Modelo LSTM salvo em: {path}\")\n",
        "\n",
        "  # Salvar metadados no novo formato\n",
        "  meta_path = path.replace(\".h5\", \"_meta.pkl\").replace(\".keras\", \"_meta.pkl\")\n",
        "  joblib.dump({\n",
        "      \"scaler_x\": model.scaler_x,\n",
        "      \"scaler_y\": model.scaler_y,\n",
        "      \"feature_cols\": model.feature_cols,\n",
        "      \"target_cols\": model.target_cols,\n",
        "      \"window_size\": model.window_size\n",
        "  }, meta_path)\n",
        "  print(f\"📦 Metadados salvos em: {meta_path}\")\n",
        "\n",
        "\n",
        "\n",
        "def load_lstm_model(asset, interval, window_size=20):\n",
        "  from tensorflow.keras.models import load_model\n",
        "  import joblib\n",
        "  import os\n",
        "\n",
        "  model_path = get_model_path(asset, interval, model_type=\"lstm\")\n",
        "  meta_path = model_path.replace(\".h5\", \"_meta.pkl\").replace(\".keras\", \"_meta.pkl\")\n",
        "\n",
        "  if not os.path.exists(model_path):\n",
        "      print(f\"🚫 Modelo LSTM NÃO encontrado em: {model_path}\")\n",
        "      return None\n",
        "\n",
        "  try:\n",
        "      model = load_model(model_path, compile=False)\n",
        "      print(f\"📂 Modelo LSTM encontrado em: {model_path}\")\n",
        "  except Exception as e:\n",
        "      print(f\"❌ Erro ao carregar modelo LSTM de {model_path}: {e}\")\n",
        "      return None\n",
        "\n",
        "  # Carrega os metadados\n",
        "  if os.path.exists(meta_path):\n",
        "      try:\n",
        "          meta = joblib.load(meta_path)\n",
        "          model.scaler_x = meta.get(\"scaler_x\")\n",
        "          model.scaler_y = meta.get(\"scaler_y\")\n",
        "          model.feature_cols = meta.get(\"feature_cols\")\n",
        "          model.target_cols = meta.get(\"target_cols\", [\"High\", \"Low\", \"Close\"])\n",
        "          model.window_size = meta.get(\"window_size\", window_size)\n",
        "\n",
        "          # ✅ Compatibilidade com códigos antigos\n",
        "          model.scaler = model.scaler_x\n",
        "\n",
        "          print(f\"📦 Metadados carregados de: {meta_path}\")\n",
        "      except Exception as e:\n",
        "          print(f\"⚠️ Erro ao carregar metadados de {meta_path}: {e}\")\n",
        "          model.scaler_x = None\n",
        "          model.scaler_y = None\n",
        "          model.scaler = None\n",
        "          model.feature_cols = None\n",
        "          model.target_cols = [\"High\", \"Low\", \"Close\"]\n",
        "          model.window_size = window_size\n",
        "  else:\n",
        "      print(f\"⚠️ Metadados não encontrados em: {meta_path}\")\n",
        "      model.scaler_x = None\n",
        "      model.scaler_y = None\n",
        "      model.scaler = None\n",
        "      model.feature_cols = None\n",
        "      model.target_cols = [\"High\", \"Low\", \"Close\"]\n",
        "      model.window_size = window_size\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 2. CONFIGURAÇÕES\n",
        "# ====================================================\n",
        "ASSETS = [\"BTC-USD\", \"ETH-USD\", \"SOL-USD\", \"XRP-USD\", \"AVAX-USD\"]# \"AAVE-USD\", \"DOT-USD\", \"NEAR-USD\", \"ADA-USD\", \"VIRTUAL-USD\", \"PENDLE-USD\"]\n",
        "\n",
        "\n",
        "TIMEFRAMES = [\n",
        "  {\"interval\": \"15m\", \"period\": \"30d\", \"atr\": 0.02},\n",
        "  {\"interval\": \"1h\", \"period\": \"90d\", \"atr\": 0.03},\n",
        "  {\"interval\": \"1d\", \"period\": \"1000d\", \"atr\": 0.05},\n",
        "  {\"interval\": \"1wk\", \"period\": \"max\", \"atr\": 0.08}  # 👈 Adicionado o semanal\n",
        "]\n",
        "\n",
        "TELEGRAM_TOKEN = \"8142008777:AAHvP5uHzEmQqR4xKyu_bfm0Vf3C8cYbmj0\"\n",
        "TELEGRAM_CHAT_ID = \"-4744645054\"\n",
        "ALERTA_VARIACAO_MINIMA = {\n",
        "  \"15m\": 1.0,\n",
        "  \"1h\": 2.0,\n",
        "  \"1d\": 5.0,\n",
        "  \"1wk\": 5.0\n",
        "}\n",
        "\n",
        "ENVIAR_ALERTAS = False  # ✅ True = enviar alertas / False = não enviar alertas\n",
        "MODO_EXECUCAO_CONTINUA = True  # True = Roda 24/7, False = Só manualmente\n",
        "\n",
        "# ====================================================\n",
        "# 3. COLETA DE DADOS\n",
        "# ====================================================\n",
        "def get_stock_data(asset, interval=\"15m\", period=\"30d\", max_retries=3, sleep_sec=5):\n",
        "    import time\n",
        "    import pandas as pd\n",
        "    import yfinance as yf\n",
        "\n",
        "    # Definir datas específicas para timeframes longos\n",
        "    usar_datas = interval in [\"1d\", \"1wk\"]\n",
        "    start_date = \"2015-01-01\"\n",
        "    end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            if usar_datas:\n",
        "                print(f\"📅 Usando start/end para {asset} ({interval}): {start_date} ➔ {end_date}\")\n",
        "                data = yf.download(asset, start=start_date, end=end_date, interval=interval, progress=False, auto_adjust=False)\n",
        "            else:\n",
        "                print(f\"⏳ Usando period para {asset} ({interval}): {period}\")\n",
        "                data = yf.download(asset, period=period, interval=interval, progress=False, auto_adjust=False)\n",
        "\n",
        "            if data.empty:\n",
        "                raise ValueError(f\"⚠️ Dados vazios recebidos de {asset} ({interval})\")\n",
        "\n",
        "            if isinstance(data.columns, pd.MultiIndex):\n",
        "                data.columns = data.columns.get_level_values(0)\n",
        "            data.columns = [col.split()[-1] if \" \" in col else col for col in data.columns]\n",
        "            data = data.loc[:, ~data.columns.duplicated()]\n",
        "            col_map = {col: std_col for col in data.columns for std_col in [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"] if std_col.lower() in col.lower()}\n",
        "            data = data.rename(columns=col_map)\n",
        "            data = data[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
        "            if not all(col in data.columns for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]):\n",
        "                raise ValueError(f\"⚠️ Colunas necessárias ausentes em {asset} ({interval})\")\n",
        "\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Falha na tentativa {attempt+1} para {asset} ({interval}): {e}\")\n",
        "            time.sleep(sleep_sec)\n",
        "\n",
        "    raise RuntimeError(f\"❌ Falha ao baixar dados de {asset} ({interval}) após {max_retries} tentativas.\")\n",
        "\n",
        "\n",
        "\n",
        "def safe_read_csv(filepath):\n",
        "  import os\n",
        "  import pandas as pd\n",
        "\n",
        "  if not os.path.exists(filepath):\n",
        "      print(f\"⚠️ Arquivo não encontrado: {filepath}\")\n",
        "      return None\n",
        "  if os.path.getsize(filepath) == 0:\n",
        "      print(f\"⚠️ Arquivo está vazio: {filepath}\")\n",
        "      return None\n",
        "  try:\n",
        "      df = pd.read_csv(filepath)\n",
        "      if df.empty or len(df.columns) == 0:\n",
        "          print(f\"⚠️ Arquivo inválido (sem colunas): {filepath}\")\n",
        "          return None\n",
        "      return df\n",
        "  except pd.errors.EmptyDataError:\n",
        "      print(f\"⚠️ Erro: arquivo sem colunas: {filepath}\")\n",
        "      return None\n",
        "  except Exception as e:\n",
        "      print(f\"⚠️ Erro inesperado ao ler CSV: {e}\")\n",
        "      return None\n",
        "\n",
        "def criar_prediction_log_padrao(filepath=\"/content/prediction_log.csv\", backup_dir=\"/content/prediction_backups\"):\n",
        "    import pandas as pd\n",
        "    import os\n",
        "    from datetime import datetime\n",
        "\n",
        "    colunas_padroes = [\n",
        "        \"Asset\", \"Timeframe\", \"Date\", \"Price\", \"Signal\", \"Confidence\", \"AdjustedProb\",\n",
        "        \"TP1\", \"TP2\", \"SL\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\",\n",
        "        \"LSTM_Predicted\", \"TargetPrice\",\n",
        "        \"LSTM_High_Predicted\", \"LSTM_Low_Predicted\",\n",
        "        \"Entry\", \"Acertou\", \"Resultado\", \"PrecoSaida\", \"LucroEstimado\", \"DuracaoMin\", \"Capital Atual\"\n",
        "    ]\n",
        "\n",
        "    # Cria a pasta de backups se não existir\n",
        "    os.makedirs(backup_dir, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"📄 Criando novo prediction_log.csv em: {filepath}\")\n",
        "        df_vazio = pd.DataFrame(columns=colunas_padroes)\n",
        "        df_vazio.to_csv(filepath, index=False)\n",
        "    else:\n",
        "        try:\n",
        "            df_existente = pd.read_csv(filepath)\n",
        "            missing_cols = [col for col in colunas_padroes if col not in df_existente.columns]\n",
        "            if missing_cols:\n",
        "                print(f\"⚙️ Adicionando colunas faltantes: {missing_cols}\")\n",
        "                for col in missing_cols:\n",
        "                    df_existente[col] = None\n",
        "                df_existente.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erro ao ler log existente. Recriando vazio. Erro: {e}\")\n",
        "            df_vazio = pd.DataFrame(columns=colunas_padroes)\n",
        "            df_vazio.to_csv(filepath, index=False)\n",
        "\n",
        "    # 🎯 Backup automático para segurança\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    backup_path = os.path.join(backup_dir, f\"prediction_log_{timestamp}.csv\")\n",
        "    try:\n",
        "        import shutil\n",
        "        shutil.copy(filepath, backup_path)\n",
        "        print(f\"✅ Backup do log salvo em: {backup_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Falha ao criar backup do log: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # ====================================================\n",
        "  # 4. INDICADORES TÉCNICOS\n",
        "  # ====================================================\n",
        "def calculate_indicators(data):\n",
        "    data = data.copy().reset_index(drop=True)\n",
        "    for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]:\n",
        "        data[col] = data[col].astype(float)\n",
        "\n",
        "    # Indicadores clássicos\n",
        "    try:\n",
        "        data[\"RSI\"] = ta.momentum.RSIIndicator(close=data[\"Close\"], window=14).rsi()\n",
        "        data[\"SMA_50\"] = ta.trend.SMAIndicator(close=data[\"Close\"], window=50).sma_indicator()\n",
        "        data[\"SMA_200\"] = ta.trend.SMAIndicator(close=data[\"Close\"], window=200).sma_indicator()\n",
        "\n",
        "        macd = ta.trend.MACD(close=data[\"Close\"])\n",
        "        data[\"MACD\"] = macd.macd()\n",
        "        data[\"MACD_Signal\"] = macd.macd_signal()\n",
        "\n",
        "        bb = ta.volatility.BollingerBands(close=data[\"Close\"], window=20)\n",
        "        data[\"Bollinger_Upper\"] = bb.bollinger_hband()\n",
        "        data[\"Bollinger_Lower\"] = bb.bollinger_lband()\n",
        "\n",
        "        adx = ta.trend.ADXIndicator(high=data[\"High\"], low=data[\"Low\"], close=data[\"Close\"], window=14)\n",
        "        data[\"ADX\"] = adx.adx()\n",
        "\n",
        "        stoch = ta.momentum.StochasticOscillator(high=data[\"High\"], low=data[\"Low\"], close=data[\"Close\"], window=14)\n",
        "        data[\"Stoch_K\"] = stoch.stoch()\n",
        "        data[\"Stoch_D\"] = stoch.stoch_signal()\n",
        "\n",
        "        # Indicadores adicionais\n",
        "        data[\"ATR\"] = ta.volatility.AverageTrueRange(high=data[\"High\"], low=data[\"Low\"], close=data[\"Close\"]).average_true_range()\n",
        "        data[\"ROC\"] = ta.momentum.ROCIndicator(close=data[\"Close\"], window=12).roc()\n",
        "        data[\"OBV\"] = ta.volume.OnBalanceVolumeIndicator(close=data[\"Close\"], volume=data[\"Volume\"]).on_balance_volume()\n",
        "        data[\"CCI\"] = ta.trend.CCIIndicator(high=data[\"High\"], low=data[\"Low\"], close=data[\"Close\"], window=20).cci()\n",
        "\n",
        "        ichimoku = ta.trend.IchimokuIndicator(high=data[\"High\"], low=data[\"Low\"], window1=9, window2=26)\n",
        "        data[\"Tenkan_Sen\"] = ichimoku.ichimoku_conversion_line()\n",
        "        data[\"Kijun_Sen\"] = ichimoku.ichimoku_base_line()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Erro ao calcular indicadores: {e}\")\n",
        "\n",
        "    # VWAP\n",
        "    try:\n",
        "        data[\"TP\"] = (data[\"High\"] + data[\"Low\"] + data[\"Close\"]) / 3\n",
        "        data[\"VWAP\"] = (data[\"TP\"] * data[\"Volume\"]).cumsum() / (data[\"Volume\"].replace(0, np.nan).cumsum())\n",
        "        data.drop(\"TP\", axis=1, inplace=True)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Erro ao calcular VWAP: {e}\")\n",
        "\n",
        "    # Candlestick patterns\n",
        "    try:\n",
        "        data[\"Doji\"] = ((abs(data[\"Close\"] - data[\"Open\"]) / (data[\"High\"] - data[\"Low\"] + 1e-9)) < 0.1).astype(int)\n",
        "        data[\"Engulfing\"] = ((data[\"Open\"].shift(1) > data[\"Close\"].shift(1)) & (data[\"Open\"] < data[\"Close\"]) &\n",
        "                            (data[\"Close\"] > data[\"Open\"].shift(1)) & (data[\"Open\"] < data[\"Close\"].shift(1))).astype(int)\n",
        "        data[\"Hammer\"] = (((data[\"High\"] - data[\"Low\"]) > 3 * abs(data[\"Open\"] - data[\"Close\"])) &\n",
        "                        ((data[\"Close\"] - data[\"Low\"]) / (data[\"High\"] - data[\"Low\"] + 1e-9) > 0.6) &\n",
        "                        ((data[\"Open\"] - data[\"Low\"]) / (data[\"High\"] - data[\"Low\"] + 1e-9) > 0.6)).astype(int)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Erro ao calcular padrões de candle: {e}\")\n",
        "        # Garante que existam\n",
        "        data[\"Doji\"] = 0\n",
        "        data[\"Engulfing\"] = 0\n",
        "        data[\"Hammer\"] = 0\n",
        "\n",
        "    data.dropna(inplace=True)\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 4. MODELOS DE MACHINE LEARNING (XGBoost + LSTM)\n",
        "# ====================================================\n",
        "\n",
        "def get_feature_columns(df, include_lstm_pred=False):\n",
        "    \"\"\"\n",
        "    Retorna a lista de colunas de features para os modelos.\n",
        "    Se include_lstm_pred=True, inclui a coluna LSTM_PRED para uso no XGBoost.\n",
        "    \"\"\"\n",
        "    base_features = [\n",
        "        'Open', 'High', 'Low', 'Close', 'Volume',\n",
        "        'SMA_5', 'SMA_20', 'EMA_12', 'EMA_26',\n",
        "        'RSI', 'MACD', 'MACD_signal', 'MACD_hist',\n",
        "        'BB_upper', 'BB_middle', 'BB_lower',\n",
        "        'ATR', 'CCI', 'ROC', 'OBV'\n",
        "    ]\n",
        "    if include_lstm_pred:\n",
        "        base_features.append(\"LSTM_PRED\")\n",
        "    return [col for col in base_features if col in df.columns]\n",
        "\n",
        "\n",
        "def get_lstm_feature_columns():\n",
        "    return [\n",
        "        \"Close\", \"High\", \"Low\",  # 🟢 Agora inclui as três colunas principais como features também\n",
        "        \"RSI\", \"MACD\", \"MACD_Signal\", \"SMA_50\", \"SMA_200\",\n",
        "        \"Bollinger_Upper\", \"Bollinger_Lower\",\n",
        "        \"ADX\", \"Stoch_K\", \"Stoch_D\",\n",
        "        \"ATR\", \"ROC\", \"OBV\", \"CCI\",\n",
        "        \"Tenkan_Sen\", \"Kijun_Sen\", \"VWAP\",\n",
        "        \"Doji\", \"Engulfing\", \"Hammer\"\n",
        "    ]\n",
        "\n",
        "\n",
        "def prepare_lstm_data(data, feature_cols=None, target_cols=[\"High\", \"Low\", \"Close\"], window_size=20):\n",
        "    if feature_cols is None:\n",
        "        feature_cols = get_lstm_feature_columns()\n",
        "\n",
        "    missing = [col for col in feature_cols + target_cols if col not in data.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"❌ Colunas ausentes no DataFrame: {missing}\")\n",
        "\n",
        "    df = data[feature_cols + target_cols].dropna().astype(float)\n",
        "    if len(df) < window_size + 1:\n",
        "        raise ValueError(f\"⚠️ Dados insuficientes: {len(df)} rows, necessário mínimo {window_size + 1}\")\n",
        "\n",
        "    # Escalonamento separado\n",
        "    scaler_x = MinMaxScaler()\n",
        "    scaler_y = MinMaxScaler()\n",
        "\n",
        "    scaled_X = scaler_x.fit_transform(df[feature_cols])\n",
        "    scaled_y = scaler_y.fit_transform(df[target_cols])\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_X[i - window_size:i])\n",
        "        y.append(scaled_y[i])  # Previsão para o instante i\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    print(f\"✅ prepare_lstm_data | X.shape: {X.shape}, y.shape: {y.shape}\")\n",
        "    return X, y, scaler_x, scaler_y\n",
        "\n",
        "\n",
        "import time  # já está importado no seu código\n",
        "\n",
        "def train_lstm_model(df, *, asset, interval, window_size=20, force_retrain=False):\n",
        "    feature_cols = get_lstm_feature_columns()\n",
        "    target_cols = [\"High\", \"Low\", \"Close\"]\n",
        "\n",
        "    df = df.dropna(subset=feature_cols + target_cols)\n",
        "\n",
        "    if len(df) <= window_size:\n",
        "        raise ValueError(\"Dados insuficientes para treino do LSTM.\")\n",
        "\n",
        "    df_features = df[feature_cols]\n",
        "    df_targets = df[target_cols]\n",
        "\n",
        "    scaler_x = MinMaxScaler()\n",
        "    scaler_y = MinMaxScaler()\n",
        "\n",
        "    scaled_features = scaler_x.fit_transform(df_features)\n",
        "    scaled_targets = scaler_y.fit_transform(df_targets)\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_features[i - window_size:i])\n",
        "        y.append(scaled_targets[i])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    print(f\"✅ train_lstm_model | X.shape: {X.shape}, y.shape: {y.shape}\")\n",
        "\n",
        "    model_path = get_model_path(asset, interval, model_type=\"lstm\")\n",
        "    meta_path = model_path.replace(\".h5\", \"_meta.pkl\")\n",
        "\n",
        "    if not force_retrain and os.path.exists(model_path):\n",
        "        model = load_lstm_model(asset, interval)\n",
        "        if model and all(hasattr(model, attr) for attr in [\"scaler_x\", \"scaler_y\", \"feature_cols\", \"window_size\", \"target_cols\"]):\n",
        "            return model\n",
        "        else:\n",
        "            print(\"⚠️ Modelo existente não contém atributos. Será refeito.\")\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(32, return_sequences=False))\n",
        "    model.add(Dense(3))  # 3 saídas: High, Low, Close\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # ⏱️ Cronometrar o treinamento\n",
        "    start_time = time.time()\n",
        "\n",
        "    es = EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss')\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6, verbose=1)\n",
        "\n",
        "    history = model.fit(\n",
        "        X, y,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[es, reduce_lr],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    final_loss = history.history['loss'][-1]\n",
        "    print(f\"✅ Treinamento LSTM concluído em {elapsed_time:.2f}s | Loss final: {final_loss:.6f}\")\n",
        "\n",
        "    model.scaler_x = scaler_x\n",
        "    model.scaler_y = scaler_y\n",
        "    model.scaler = scaler_x\n",
        "    model.feature_cols = feature_cols\n",
        "    model.target_cols = target_cols\n",
        "    model.window_size = window_size\n",
        "\n",
        "    model.save(model_path)\n",
        "    joblib.dump({\n",
        "        \"scaler_x\": scaler_x,\n",
        "        \"scaler_y\": scaler_y,\n",
        "        \"feature_cols\": feature_cols,\n",
        "        \"target_cols\": target_cols,\n",
        "        \"window_size\": window_size\n",
        "    }, meta_path)\n",
        "\n",
        "    print(f\"💾 Modelo LSTM salvo em: {model_path}\")\n",
        "    print(f\"📦 Metadados salvos em: {meta_path}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_lstm_model_diario(df, *, asset, interval, window_size=60, force_retrain=False):\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    import os\n",
        "    import time\n",
        "    import joblib\n",
        "    import numpy as np\n",
        "\n",
        "    feature_cols = get_lstm_feature_columns()\n",
        "    target_cols = [\"High\", \"Low\", \"Close\"]\n",
        "\n",
        "    df = df.dropna(subset=feature_cols + target_cols)\n",
        "\n",
        "    if len(df) <= window_size:\n",
        "        raise ValueError(\"Dados insuficientes para treino do LSTM.\")\n",
        "\n",
        "    df_features = df[feature_cols]\n",
        "    df_targets = df[target_cols]\n",
        "\n",
        "    scaler_x = MinMaxScaler()\n",
        "    scaler_y = MinMaxScaler()\n",
        "\n",
        "    scaled_features = scaler_x.fit_transform(df_features)\n",
        "    scaled_targets = scaler_y.fit_transform(df_targets)\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_features[i - window_size:i])\n",
        "        y.append(scaled_targets[i])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    model_path = get_model_path(asset, interval, model_type=\"lstm\")\n",
        "    meta_path = model_path.replace(\".h5\", \"_meta.pkl\")\n",
        "\n",
        "    if not force_retrain and os.path.exists(model_path):\n",
        "        model = load_lstm_model(asset, interval)\n",
        "        if model and all(hasattr(model, attr) for attr in [\"scaler_x\", \"scaler_y\", \"feature_cols\", \"window_size\", \"target_cols\"]):\n",
        "            return model\n",
        "        else:\n",
        "            print(\"⚠️ Modelo existente não contém atributos corretos. Será refeito.\")\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(LSTM(64, return_sequences=False))\n",
        "    model.add(Dense(3))  # High, Low, Close\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Cronometrar o treino\n",
        "    start_time = time.time()\n",
        "\n",
        "    es = EarlyStopping(patience=20, restore_best_weights=True, monitor='val_loss')\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n",
        "\n",
        "    history = model.fit(\n",
        "        X, y,\n",
        "        epochs=300,\n",
        "        batch_size=64,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[es, reduce_lr],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"✅ Treinamento LSTM Diário concluído em {elapsed_time:.2f}s | Loss final: {history.history['loss'][-1]:.6f}\")\n",
        "\n",
        "    # Atribuir atributos\n",
        "    model.scaler_x = scaler_x\n",
        "    model.scaler_y = scaler_y\n",
        "    model.scaler = scaler_x\n",
        "    model.feature_cols = feature_cols\n",
        "    model.target_cols = target_cols\n",
        "    model.window_size = window_size\n",
        "\n",
        "    # Salvar\n",
        "    model.save(model_path)\n",
        "    joblib.dump({\n",
        "        \"scaler_x\": scaler_x,\n",
        "        \"scaler_y\": scaler_y,\n",
        "        \"feature_cols\": feature_cols,\n",
        "        \"target_cols\": target_cols,\n",
        "        \"window_size\": window_size\n",
        "    }, meta_path)\n",
        "\n",
        "    print(f\"💾 Modelo LSTM Diário salvo em: {model_path}\")\n",
        "    print(f\"📦 Metadados salvos em: {meta_path}\")\n",
        "    return model\n",
        "\n",
        "def train_ml_model(data, asset=None, interval=None, verbose=False, force_retrain=False):\n",
        "    from xgboost import XGBClassifier\n",
        "    from sklearn.model_selection import TimeSeriesSplit\n",
        "    from sklearn.metrics import classification_report\n",
        "    import time\n",
        "\n",
        "    if asset and interval:\n",
        "        if not force_retrain:\n",
        "            existing_model = load_xgb_model(asset, interval)\n",
        "            if existing_model is not None:\n",
        "                print(f\"✅ Modelo XGBoost já existente para {asset} ({interval}), carregado.\")\n",
        "                return existing_model\n",
        "\n",
        "    if len(data) < 100:\n",
        "        return None\n",
        "\n",
        "    df = data.copy()\n",
        "    df = calculate_indicators(df)\n",
        "\n",
        "    try:\n",
        "        lstm_model = train_lstm_model(df, asset=asset, interval=interval, window_size=20, force_retrain=force_retrain)\n",
        "\n",
        "        if lstm_model:\n",
        "            print(\"✅ Features usadas no LSTM:\")\n",
        "            print(lstm_model.feature_cols)\n",
        "\n",
        "            print(\"✅ Últimos dados de entrada:\")\n",
        "            print(df[lstm_model.feature_cols].tail(3))\n",
        "\n",
        "            print(\"✅ Valores mínimos do scaler X:\")\n",
        "            print(lstm_model.scaler_x.data_min_)\n",
        "            print(\"✅ Valores máximos do scaler X:\")\n",
        "            print(lstm_model.scaler_x.data_max_)\n",
        "\n",
        "        if lstm_model is not None:\n",
        "            lstm_preds = []\n",
        "            for i in range(len(df)):\n",
        "                sub_df = df.iloc[:i+1]\n",
        "                if len(sub_df) < lstm_model.window_size:\n",
        "                    lstm_preds.append(np.nan)\n",
        "                else:\n",
        "                    try:\n",
        "                        pred = predict_with_lstm(lstm_model, sub_df)\n",
        "                        lstm_preds.append(pred.get(\"Close\", np.nan))\n",
        "                    except Exception as e:\n",
        "                        print(f\"⚠️ Erro ao prever com LSTM: {e}\")\n",
        "                        lstm_preds.append(np.nan)\n",
        "            df[\"LSTM_PRED\"] = lstm_preds\n",
        "        else:\n",
        "            df[\"LSTM_PRED\"] = np.nan\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Erro ao gerar LSTM_PRED: {e}\")\n",
        "        df[\"LSTM_PRED\"] = np.nan\n",
        "\n",
        "    if \"LSTM_PRED\" not in df.columns:\n",
        "        print(\"❌ Coluna 'LSTM_PRED' não foi gerada. Abortando treino do XGBoost.\")\n",
        "        return None\n",
        "\n",
        "    df[\"Future_Close\"] = df[\"Close\"].shift(-5)\n",
        "    df[\"Future_Return\"] = df[\"Future_Close\"] / df[\"Close\"] - 1\n",
        "    df = df[(df[\"Future_Return\"] > 0.015) | (df[\"Future_Return\"] < -0.015)].copy()\n",
        "    df[\"Signal\"] = np.where(df[\"Future_Return\"] > 0.015, 1, 0)\n",
        "\n",
        "    features = get_feature_columns(df, include_lstm_pred=True)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    missing_features = [f for f in features if f not in df.columns]\n",
        "    if missing_features:\n",
        "        print(f\"❌ Features ausentes: {missing_features}\")\n",
        "        return None\n",
        "\n",
        "    X = df[features]\n",
        "    y = df[\"Signal\"]\n",
        "\n",
        "    if len(np.unique(y)) < 2:\n",
        "        return None\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    for train_index, val_index in tscv.split(X):\n",
        "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "        break\n",
        "\n",
        "    if len(np.unique(y_train)) < 2:\n",
        "        return None\n",
        "\n",
        "    scale_pos_weight = len(y_train[y_train == 0]) / max(1, len(y_train[y_train == 1]))\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric=\"logloss\",\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # ⏱️ Cronometrar o treinamento\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"✅ Treinamento XGBoost concluído em {elapsed_time:.2f}s\")\n",
        "\n",
        "    y_pred = model.predict(X_val)\n",
        "    report = classification_report(y_val, y_pred, output_dict=True, zero_division=0)\n",
        "\n",
        "    model.validation_score = {\n",
        "        \"accuracy\": report.get(\"accuracy\"),\n",
        "        \"precision\": report.get(\"1\", {}).get(\"precision\"),\n",
        "        \"recall\": report.get(\"1\", {}).get(\"recall\"),\n",
        "        \"f1\": report.get(\"1\", {}).get(\"f1-score\")\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"✅ Validação: {model.validation_score}\")\n",
        "\n",
        "    if asset and interval:\n",
        "        save_xgb_model(model, asset, interval)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict_with_lstm(model, df, asset=\"N/A\", interval=\"N/A\"):\n",
        "    \"\"\"\n",
        "    Faz a previsão com LSTM usando a última janela de dados,\n",
        "    corrige problemas comuns automaticamente e registra previsões descartadas.\n",
        "    \"\"\"\n",
        "    if not all(hasattr(model, attr) for attr in ['scaler_x', 'scaler_y', 'feature_cols', 'window_size']):\n",
        "        raise AttributeError(\"❌ O modelo LSTM não possui os atributos necessários (scaler_x, scaler_y, feature_cols, window_size).\")\n",
        "\n",
        "    df = df.copy().dropna(subset=model.feature_cols)\n",
        "    if len(df) < model.window_size:\n",
        "        raise ValueError(\"⚠️ Dados insuficientes para previsão com LSTM.\")\n",
        "\n",
        "    last_window = df[model.feature_cols].values[-model.window_size:]\n",
        "    scaled_window = model.scaler_x.transform(last_window)\n",
        "    X_input = np.expand_dims(scaled_window, axis=0)\n",
        "\n",
        "    pred_scaled = model.predict(X_input, verbose=0)[0].reshape(1, -1)\n",
        "    pred_descaled = model.scaler_y.inverse_transform(pred_scaled)[0]\n",
        "\n",
        "    high, low, close = float(pred_descaled[0]), float(pred_descaled[1]), float(pred_descaled[2])\n",
        "\n",
        "    # 🔒 Correção 1: High deve ser maior ou igual ao Low\n",
        "    if low > high:\n",
        "        print(f\"⚠️ Corrigindo inversão de High/Low na previsão LSTM. High={high:.2f}, Low={low:.2f}\")\n",
        "        high, low = max(high, low), min(high, low)\n",
        "\n",
        "    # 🔒 Correção 2: Close deve ficar entre Low e High\n",
        "    if close < low or close > high:\n",
        "        print(f\"⚠️ Ajustando Close fora da faixa. Antes: {close:.2f}\")\n",
        "        close = max(min(close, high), low)\n",
        "        print(f\"✅ Close ajustado para: {close:.2f}\")\n",
        "\n",
        "    # 🔒 Correção 3: Previsões absurdas (variação maior que 50%)\n",
        "    preco_atual = df[\"Close\"].iloc[-1]\n",
        "    if preco_atual > 0:\n",
        "        variacao_permitida = 0.5  # 50%\n",
        "        if abs(close - preco_atual) / preco_atual > variacao_permitida:\n",
        "            print(f\"❌ Previsão absurda detectada. Atual={preco_atual:.2f} Previsto={close:.2f}\")\n",
        "            log_previsao_absurda(asset=asset, interval=interval, preco_atual=preco_atual, close_previsto=close)\n",
        "            return {\"High\": None, \"Low\": None, \"Close\": None}\n",
        "\n",
        "    return {\n",
        "        \"High\": round(high, 4),\n",
        "        \"Low\": round(low, 4),\n",
        "        \"Close\": round(close, 4)\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_feature_importance(model, feature_names, top_n=15):\n",
        "    \"\"\"\n",
        "    Plota a importância das features do modelo XGBoost.\n",
        "    \"\"\"\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        importances = model.feature_importances_\n",
        "    else:\n",
        "        print(\"❌ O modelo não possui 'feature_importances_'.\")\n",
        "        return\n",
        "\n",
        "    indices = np.argsort(importances)[-top_n:][::-1]\n",
        "    top_features = [feature_names[i] for i in indices]\n",
        "    top_importances = importances[indices]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(top_features[::-1], top_importances[::-1])\n",
        "    plt.xlabel('Importância')\n",
        "    plt.title('Top Features Importantes')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 5. UTILITÁRIOS\n",
        "# ====================================================\n",
        "# ====================================================\n",
        "# FUNÇÃO GLOBAL DE CONVERSÃO ESCALAR\n",
        "# ====================================================\n",
        "def to_scalar(val):\n",
        "    try:\n",
        "        if isinstance(val, pd.Series):\n",
        "            return float(val.iloc[0])\n",
        "        elif isinstance(val, (np.ndarray, list)):\n",
        "            return float(val[0])\n",
        "        elif pd.isna(val):\n",
        "            return np.nan\n",
        "        else:\n",
        "            return float(val)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Falha ao converter valor escalar: {val} | erro: {e}\")\n",
        "        return np.nan\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def limpar_model_results():\n",
        "    arquivos = glob.glob(\"/content/model_results_*.csv\")\n",
        "    if not arquivos:\n",
        "        print(\"📂 Nenhum arquivo model_results_*.csv encontrado.\")\n",
        "        return\n",
        "\n",
        "def plot_entrada_lstm(df, feature_cols):\n",
        "    import matplotlib.pyplot as plt\n",
        "    df_plot = df[feature_cols].tail(100).copy()\n",
        "    df_plot.plot(figsize=(12, 5), title=\"📊 Últimas 100 entradas das features LSTM\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def gerar_resumo_ultimos_sinais(asset, interval, n=15, path=\"/content/prediction_log.csv\"):\n",
        "    df_log = safe_read_csv(path)\n",
        "    if df_log is None or df_log.empty:\n",
        "        return \"📭 Sem sinais anteriores registrados.\"\n",
        "\n",
        "    df_log = df_log[(df_log[\"Asset\"] == asset) & (df_log[\"Timeframe\"] == interval)].copy()\n",
        "    df_log[\"Date\"] = pd.to_datetime(df_log[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "    df_log = df_log.sort_values(\"Date\", ascending=False).head(n)\n",
        "\n",
        "    linhas = []\n",
        "    for _, row in df_log.iterrows():\n",
        "        data_str = row[\"Date\"].strftime(\"%d/%m %H:%M\")\n",
        "        sinal = \"COMPRA\" if row[\"Signal\"] == 1 else \"VENDA\" if row[\"Signal\"] == 0 else \"NEUTRO\"\n",
        "        emoji = \"✔️\" if row.get(\"Resultado\") == \"TP1\" else \"❌\" if row.get(\"Resultado\") == \"SL\" else \"➖\"\n",
        "        lucro = row.get(\"LucroEstimado\", \"\")\n",
        "        lucro_str = f\" | Lucro: {lucro:+.2f}\" if pd.notna(lucro) else \"\"\n",
        "        linhas.append(f\"{emoji} {sinal} | {row['Asset']} | {data_str}{lucro_str}\")\n",
        "\n",
        "    # Acurácia dos últimos sinais (apenas TP1 e SL considerados)\n",
        "    df_valid = df_log[df_log[\"Resultado\"].isin([\"TP1\", \"SL\"])]\n",
        "    if not df_valid.empty:\n",
        "        acertos = (df_valid[\"Resultado\"] == \"TP1\").sum()\n",
        "        total = len(df_valid)\n",
        "        acuracia = round(100 * acertos / total, 2)\n",
        "        linhas.append(f\"\\n📈 <b>Acurácia:</b> {acuracia}% ({acertos}/{total})\")\n",
        "\n",
        "    return \"📊 <b>Últimos Sinais:</b>\\n\" + \"\\n\".join(linhas)\n",
        "\n",
        "\n",
        "def gerar_ranking_lucro(path=\"/content/prediction_log.csv\", top_n=5):\n",
        "    df = safe_read_csv(path)\n",
        "    if df is None or df.empty or \"LucroEstimado\" not in df.columns:\n",
        "        return \"📭 Sem dados de lucro disponíveis.\"\n",
        "\n",
        "    df = df.dropna(subset=[\"Asset\", \"LucroEstimado\"])\n",
        "    df = df[df[\"Resultado\"].isin([\"TP1\", \"SL\", \"Sem alvo\"])]\n",
        "    df_grouped = df.groupby(\"Asset\")[\"LucroEstimado\"].sum().sort_values(ascending=False).head(top_n)\n",
        "\n",
        "    linhas = [\"🏆 <b>Top Ativos por Lucro Total:</b>\"]\n",
        "    for ativo, lucro in df_grouped.items():\n",
        "        emoji = \"🟢\" if lucro > 0 else \"🔴\" if lucro < 0 else \"⚪\"\n",
        "        linhas.append(f\"{emoji} {ativo}: ${lucro:+.2f}\")\n",
        "    return \"\\n\".join(linhas)\n",
        "\n",
        "def gerar_resumo_por_padrao(asset, interval, path=\"/content/prediction_log.csv\"):\n",
        "    df = safe_read_csv(path)\n",
        "    if df is None or df.empty:\n",
        "        return \"📭 Sem sinais anteriores registrados.\"\n",
        "\n",
        "    df = df[(df[\"Asset\"] == asset) & (df[\"Timeframe\"] == interval)]\n",
        "    subset_cols = [\"Doji\", \"Engulfing\", \"Hammer\"]\n",
        "    subset_cols = [col for col in subset_cols if col in df.columns]\n",
        "    if subset_cols:\n",
        "        df = df.dropna(subset=subset_cols)\n",
        "\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "\n",
        "    ultimos = df.sort_values(\"Date\", ascending=False).head(50)\n",
        "\n",
        "    contagem = {\n",
        "        \"Doji\": ultimos[\"Doji\"].sum() if \"Doji\" in ultimos.columns else 0,\n",
        "        \"Engolfo\": ultimos[\"Engulfing\"].sum() if \"Engulfing\" in ultimos.columns else 0,\n",
        "        \"Martelo\": ultimos[\"Hammer\"].sum() if \"Hammer\" in ultimos.columns else 0\n",
        "    }\n",
        "\n",
        "\n",
        "    linhas = [\"🔎 <b>Padrões Recentes Detectados:</b>\"]\n",
        "    for nome, qtd in contagem.items():\n",
        "        if qtd > 0:\n",
        "            linhas.append(f\"• {nome}: {int(qtd)} ocorrência(s)\")\n",
        "    return \"\\n\".join(linhas) if len(linhas) > 1 else \"⚪ Nenhum padrão técnico detectado recentemente.\"\n",
        "\n",
        "def generate_explanation(row, prediction, feature_importance=None):\n",
        "    \"\"\"\n",
        "    Gera explicação técnica com base em indicadores e no sinal previsto (compra/venda).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        explicacao = []\n",
        "\n",
        "        if prediction == 1:\n",
        "            explicacao.append(\"🟢 O modelo prevê uma tendência de ALTA (compra).\")\n",
        "        elif prediction == 0:\n",
        "            explicacao.append(\"🔴 O modelo prevê uma tendência de BAIXA (venda).\")\n",
        "        else:\n",
        "            explicacao.append(\"⚪ Tendência neutra — sem sinal claro.\")\n",
        "\n",
        "        # Indicadores clássicos\n",
        "        if \"RSI\" in row:\n",
        "            if row[\"RSI\"] < 30:\n",
        "                explicacao.append(\"• RSI indica sobrevenda (RSI < 30).\")\n",
        "            elif row[\"RSI\"] > 70:\n",
        "                explicacao.append(\"• RSI indica sobrecompra (RSI > 70).\")\n",
        "\n",
        "        if \"MACD\" in row and \"MACD_Signal\" in row:\n",
        "            if row[\"MACD\"] > row[\"MACD_Signal\"]:\n",
        "                explicacao.append(\"• MACD cruzando para cima da linha de sinal (potencial alta).\")\n",
        "            else:\n",
        "                explicacao.append(\"• MACD abaixo da linha de sinal (potencial queda).\")\n",
        "\n",
        "        if \"SMA_50\" in row and \"SMA_200\" in row:\n",
        "            if row[\"SMA_50\"] > row[\"SMA_200\"]:\n",
        "                explicacao.append(\"• SMA 50 acima da 200 (tendência de alta no médio prazo).\")\n",
        "            else:\n",
        "                explicacao.append(\"• SMA 50 abaixo da 200 (tendência de baixa no médio prazo).\")\n",
        "\n",
        "        if \"ADX\" in row and row[\"ADX\"] > 20:\n",
        "            explicacao.append(\"• ADX > 20 (tendência direcional presente).\")\n",
        "\n",
        "        # Padrões de candle\n",
        "        if row.get(\"Doji\") == 1:\n",
        "            explicacao.append(\"• Padrão Doji detectado (possível reversão).\")\n",
        "        if row.get(\"Engulfing\") == 1:\n",
        "            explicacao.append(\"• Padrão de engolfo detectado (reversão possível).\")\n",
        "        if row.get(\"Hammer\") == 1:\n",
        "            explicacao.append(\"• Padrão de martelo identificado (alta possível).\")\n",
        "\n",
        "        # Importância de features (se disponível)\n",
        "        if feature_importance:\n",
        "            explicacao.append(\"\\n📊 Principais influências do modelo:\")\n",
        "            top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "            for name, weight in top_features:\n",
        "                explicacao.append(f\"• {name}: peso {weight:.3f}\")\n",
        "\n",
        "        return \"\\n\".join(explicacao)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"⚠️ Erro ao gerar explicação: {str(e)}\"\n",
        "\n",
        "def enviar_grafico_previsao_futura(df_previsao, timeframe, asset):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.dates as mdates\n",
        "    import os\n",
        "    import pandas as pd\n",
        "\n",
        "    if df_previsao is None or not all(k in df_previsao for k in [\"Date\", \"High\", \"Low\", \"Close\"]):\n",
        "        print(f\"⚠️ Dados de previsão futura incompletos para {asset} ({timeframe})\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(df_previsao)\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        date = df[\"Date\"].iloc[i]\n",
        "        high = df[\"High\"].iloc[i]\n",
        "        low = df[\"Low\"].iloc[i]\n",
        "        close = df[\"Close\"].iloc[i]\n",
        "\n",
        "        plt.vlines(date, ymin=low, ymax=high, color=\"blue\", linewidth=2, label=\"Projeção\" if i == 0 else \"\")\n",
        "        plt.plot(date, close, marker=\"o\", color=\"blue\")\n",
        "        plt.annotate(f\"{close:.0f}\", (date, close), xytext=(0, 8),\n",
        "                     textcoords=\"offset points\", ha='center', fontsize=8, color=\"blue\")\n",
        "\n",
        "    plt.title(f\"🔮 Projeção Futura (LSTM) — {asset} ({timeframe})\")\n",
        "    plt.xlabel(\"Data\")\n",
        "    plt.ylabel(\"Preço Projetado\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d/%m %H:%M', tz=BR_TZ))\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # 🔥 Novo: salvar o gráfico em /content/\n",
        "    image_path = f\"/content/projecao_futura_{asset.replace('-', '')}_{timeframe}.png\"\n",
        "    plt.savefig(image_path)\n",
        "    print(f\"💾 Gráfico salvo em: {image_path}\")\n",
        "\n",
        "    # 🔥 Novo: mostrar o gráfico no log\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # 📤 Opcional: enviar para o Telegram (já implementado se quiser)\n",
        "    # if os.path.exists(image_path):\n",
        "    #     with open(image_path, \"rb\") as img:\n",
        "    #         url = f\"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendPhoto\"\n",
        "    #         files = {\"photo\": img}\n",
        "    #         data = {\n",
        "    #             \"chat_id\": TELEGRAM_CHAT_ID,\n",
        "    #             \"caption\": f\"🔮 Projeção Futura — {asset} ({timeframe})\"\n",
        "    #         }\n",
        "    #         response = requests.post(url, data=data, files=files)\n",
        "    #         if response.status_code == 200:\n",
        "    #             print(\"✅ Gráfico de projeção futura enviado ao Telegram.\")\n",
        "    #         else:\n",
        "    #             print(f\"❌ Erro ao enviar gráfico: {response.status_code} - {response.text}\")\n",
        "\n",
        "\n",
        "\n",
        "import mplfinance as mpf\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "\n",
        "import mplfinance as mpf\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "\n",
        "import mplfinance as mpf\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotar_candles_com_previsao(\n",
        "    df_candles,\n",
        "    pred_lstm_dicts,\n",
        "    title=\"📊 Histórico + Previsão LSTM\",\n",
        "    asset=\"BTC-USD\",\n",
        "    timeframe=\"15m\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Plota 10 candles reais + previsões futuras com mplfinance (candlestick),\n",
        "    salva o gráfico no /content/ e também exibe no terminal.\n",
        "    \"\"\"\n",
        "\n",
        "    # 🛠️ Garantir que 'Date' é datetime\n",
        "    df_candles = df_candles.copy()\n",
        "    if \"Date\" not in df_candles.columns:\n",
        "        df_candles[\"Date\"] = df_candles.index\n",
        "    df_candles[\"Date\"] = pd.to_datetime(df_candles[\"Date\"])\n",
        "\n",
        "    df_plot = df_candles.tail(10).reset_index(drop=True)[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\"]].copy()\n",
        "    df_plot[\"Volume\"] = 0  # placeholder para evitar erro do mplfinance\n",
        "\n",
        "    last_date = df_plot[\"Date\"].iloc[-1]\n",
        "\n",
        "    timeframe_delta = {\n",
        "        \"15m\": timedelta(minutes=15),\n",
        "        \"1h\": timedelta(hours=1),\n",
        "        \"1d\": timedelta(days=1),\n",
        "        \"1wk\": timedelta(weeks=1)\n",
        "    }.get(timeframe, timedelta(hours=1))\n",
        "\n",
        "    # 🔮 Adiciona os candles futuros previstos\n",
        "    for i, pred in enumerate(pred_lstm_dicts):\n",
        "        if any(pred.get(k) is None for k in [\"High\", \"Low\", \"Close\"]):\n",
        "            continue\n",
        "\n",
        "        future_time = last_date + timeframe_delta * (i + 1)\n",
        "\n",
        "        candle = {\n",
        "            \"Date\": future_time,\n",
        "            \"Open\": df_plot[\"Close\"].iloc[-1] if i == 0 else pred_lstm_dicts[i - 1][\"Close\"],\n",
        "            \"High\": pred[\"High\"],\n",
        "            \"Low\": pred[\"Low\"],\n",
        "            \"Close\": pred[\"Close\"],\n",
        "            \"Volume\": 0\n",
        "        }\n",
        "\n",
        "        df_plot = pd.concat([df_plot, pd.DataFrame([candle])], ignore_index=True)\n",
        "\n",
        "    df_plot.set_index(\"Date\", inplace=True)\n",
        "    df_plot.index = pd.to_datetime(df_plot.index)\n",
        "\n",
        "    # 🎨 Estilo visual do gráfico\n",
        "    mc = mpf.make_marketcolors(up='g', down='r', inherit=True)\n",
        "    s = mpf.make_mpf_style(marketcolors=mc, gridstyle=':', facecolor='white')\n",
        "\n",
        "    # 📁 Caminho para salvar no /content/\n",
        "    save_path = f\"/content/candle_proj_{asset.replace('-', '')}_{timeframe}.png\"\n",
        "\n",
        "    # 📈 Salvar e também exibir\n",
        "    mpf.plot(df_plot, type='candle', style=s,\n",
        "             title=f\"{title} — {asset} ({timeframe})\",\n",
        "             ylabel='Preço', volume=False, savefig=save_path)\n",
        "\n",
        "    print(f\"💾 Gráfico de previsão salvo em: {save_path}\")\n",
        "\n",
        "    # Agora exibe o gráfico no terminal\n",
        "    mpf.plot(df_plot, type='candle', style=s,\n",
        "             title=f\"{title} — {asset} ({timeframe})\",\n",
        "             ylabel='Preço', volume=False)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calculate_targets(price_row, signal, atr_multiplier=0.02, min_dist_percent=0.001):\n",
        "    \"\"\"\n",
        "    Calcula TP e SL com base em Low como entrada e High como alvo.\n",
        "    Usa atr_multiplier para margem no SL.\n",
        "    Se distância Entry-SL for muito pequena, invalida o trade.\n",
        "    \"\"\"\n",
        "    if signal == 1:  # Compra\n",
        "        entry = price_row.get(\"Low\", None)\n",
        "        tp1 = price_row.get(\"High\", None)\n",
        "        sl = entry - (entry * atr_multiplier) if entry is not None else None\n",
        "    elif signal == 0:  # Venda\n",
        "        entry = price_row.get(\"High\", None)\n",
        "        tp1 = price_row.get(\"Low\", None)\n",
        "        sl = entry + (entry * atr_multiplier) if entry is not None else None\n",
        "    else:\n",
        "        return {\"Entry\": None, \"TP1\": None, \"TP2\": None, \"SL\": None}\n",
        "\n",
        "    # 📋 Proteções de segurança\n",
        "    if any(v is None or np.isnan(v) for v in [entry, tp1, sl]):\n",
        "        print(\"⚠️ Targets inválidos detectados — retornando None.\")\n",
        "        return {\"Entry\": None, \"TP1\": None, \"TP2\": None, \"SL\": None}\n",
        "\n",
        "    # ⚡ Verifica distância mínima entre Entry e SL\n",
        "    if entry != 0 and abs(entry - sl) / entry < min_dist_percent:\n",
        "        print(f\"🚫 Trade descartado: distância Entry-SL muito pequena ({abs(entry - sl):.2f} | {abs(entry - sl) / entry:.4%})\")\n",
        "        return {\"Entry\": None, \"TP1\": None, \"TP2\": None, \"SL\": None}\n",
        "\n",
        "    # 🛡️ Cálculo de TP2 baseado na distância\n",
        "    distancia = abs(entry - sl)\n",
        "    tp2 = entry + 2 * distancia if signal == 1 else entry - 2 * distancia\n",
        "\n",
        "    return {\n",
        "        \"Entry\": round(entry, 4),\n",
        "        \"TP1\": round(tp1, 4),\n",
        "        \"TP2\": round(tp2, 4),\n",
        "        \"SL\": round(sl, 4)\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def log_previsao_absurda(asset, interval, preco_atual, close_previsto):\n",
        "    try:\n",
        "        path = \"/content/previsoes_descartadas.csv\"\n",
        "        row = {\n",
        "            \"Data\": datetime.now(BR_TZ).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"Asset\": asset,\n",
        "            \"Timeframe\": interval,\n",
        "            \"PrecoAtual\": preco_atual,\n",
        "            \"PrevistoClose\": close_previsto,\n",
        "            \"Variacao(%)\": round((close_previsto - preco_atual) / preco_atual * 100, 2)\n",
        "        }\n",
        "        df = pd.DataFrame([row])\n",
        "        if os.path.exists(path):\n",
        "            df.to_csv(path, mode=\"a\", header=False, index=False)\n",
        "        else:\n",
        "            df.to_csv(path, index=False)\n",
        "        print(f\"🧾 Previsão absurda registrada em: {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Falha ao logar previsão absurda: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def send_telegram_message(message):\n",
        "    url = f\"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage\"\n",
        "    payload = {\"chat_id\": TELEGRAM_CHAT_ID, \"text\": message, \"parse_mode\": \"HTML\"}\n",
        "    response = requests.post(url, json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        print(\"📨 Mensagem enviada com sucesso!\")\n",
        "    else:\n",
        "        print(f\"❌ Erro ao enviar mensagem: {response.status_code} - {response.text}\")\n",
        "\n",
        "def predict_next_closes(data, n_steps=5):\n",
        "    df = data.copy().reset_index(drop=True)\n",
        "    features = get_feature_columns(df)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    X = df[features]\n",
        "    y = df[\"Close\"].shift(-1).dropna()\n",
        "    X = X.loc[y.index]\n",
        "\n",
        "    if len(X) < 100:\n",
        "        return [None] * n_steps\n",
        "\n",
        "    model = RandomForestRegressor(n_estimators=200, max_depth=8, random_state=42)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    last_row = df[features].iloc[-1].copy()\n",
        "    preds = []\n",
        "\n",
        "    for step in range(n_steps):\n",
        "        X_input = pd.DataFrame([last_row], columns=features)\n",
        "        next_close = model.predict(X_input)[0]\n",
        "        preds.append(round(next_close, 2))\n",
        "\n",
        "        # Simula avanço do mercado\n",
        "        last_row[\"Close\"] = next_close\n",
        "        if \"SMA_50\" in last_row:\n",
        "            last_row[\"SMA_50\"] = last_row[\"SMA_50\"] * 0.9 + next_close * 0.1\n",
        "        if \"SMA_200\" in last_row:\n",
        "            last_row[\"SMA_200\"] = last_row[\"SMA_200\"] * 0.95 + next_close * 0.05\n",
        "        if \"VWAP\" in last_row:\n",
        "            last_row[\"VWAP\"] = last_row[\"VWAP\"] * 0.95 + next_close * 0.05\n",
        "        if \"RSI\" in last_row:\n",
        "            last_row[\"RSI\"] = min(100, max(0, last_row[\"RSI\"] + np.random.normal(0, 0.5)))\n",
        "        if \"MACD\" in last_row:\n",
        "            last_row[\"MACD\"] += np.random.normal(0, 0.3)\n",
        "        if \"MACD_Signal\" in last_row:\n",
        "            last_row[\"MACD_Signal\"] += np.random.normal(0, 0.2)\n",
        "\n",
        "        last_row = last_row[features]\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "def evaluate_past_predictions(results_file=\"/content/prediction_log.csv\", lookahead_candles=5):\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    import yfinance as yf\n",
        "    import matplotlib.pyplot as plt\n",
        "    from datetime import timedelta\n",
        "\n",
        "    df = safe_read_csv(results_file)\n",
        "    if df is None or df.empty:\n",
        "        print(\"📭 Nenhum log de previsão encontrado ou o arquivo está vazio.\")\n",
        "        return\n",
        "\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "    print(f\"📊 Avaliando {len(df)} previsões salvas...\")\n",
        "\n",
        "    evaluation = []\n",
        "\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        asset = row[\"Asset\"]\n",
        "        interval = row[\"Timeframe\"]\n",
        "        prediction_time = row[\"Date\"]\n",
        "        predicted_signal = row[\"Signal\"]\n",
        "        predicted_target = row.get(\"TargetPrice\", None)\n",
        "\n",
        "        try:\n",
        "            candles = yf.download(asset, start=prediction_time, interval=interval, progress=False)\n",
        "            candles = candles[candles.index > prediction_time]\n",
        "\n",
        "            if candles.empty or len(candles) < lookahead_candles:\n",
        "                continue\n",
        "\n",
        "            candles = candles.head(lookahead_candles)\n",
        "            final_close = candles[\"Close\"].iloc[-1]\n",
        "\n",
        "            if predicted_signal == 1:\n",
        "                result = \"Acertou\" if final_close >= predicted_target else \"Errou\"\n",
        "            elif predicted_signal == 0:\n",
        "                result = \"Acertou\" if final_close <= predicted_target else \"Errou\"\n",
        "            else:\n",
        "                result = \"Neutro\"\n",
        "\n",
        "            if predicted_target:\n",
        "                perc_change = ((final_close - predicted_target) / predicted_target) * 100\n",
        "                abs_error = final_close - predicted_target\n",
        "            else:\n",
        "                perc_change = None\n",
        "                abs_error = None\n",
        "\n",
        "            acertou = 1 if result == \"Acertou\" else 0\n",
        "\n",
        "            evaluation.append({\n",
        "                \"Ativo\": asset,\n",
        "                \"Timeframe\": interval,\n",
        "                \"Data Previsão\": prediction_time.strftime(\"%Y-%m-%d %H:%M\"),\n",
        "                \"Sinal Previsto\": \"Compra\" if predicted_signal == 1 else \"Venda\" if predicted_signal == 0 else \"Neutro\",\n",
        "                \"Valor Projetado (LSTM)\": round(predicted_target, 2) if predicted_target else None,\n",
        "                \"Resultado\": result,\n",
        "                \"Valor Real\": round(final_close, 2),\n",
        "                \"Variação Real\": f\"{perc_change:+.2f}%\" if perc_change is not None else \"N/A\",\n",
        "                \"Erro Absoluto\": f\"{abs_error:+.2f}\" if abs_error is not None else \"N/A\",\n",
        "                \"Acertou\": acertou\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erro ao avaliar {asset} em {prediction_time}: {e}\")\n",
        "            continue\n",
        "\n",
        "    df_eval = pd.DataFrame(evaluation)\n",
        "\n",
        "    # 📊 Resumo de acertos e erros\n",
        "    resumo = df_eval.groupby([\"Ativo\", \"Timeframe\", \"Resultado\"]).size().unstack(fill_value=0)\n",
        "    resumo[\"Total\"] = resumo.sum(axis=1)\n",
        "    resumo[\"Acurácia (%)\"] = (resumo.get(\"Acertou\", 0) / resumo[\"Total\"] * 100).round(2)\n",
        "    display(resumo)\n",
        "\n",
        "    # 📈 Gráfico de barras\n",
        "    resumo_plot = resumo[[\"Acertou\", \"Errou\"]] if \"Errou\" in resumo.columns else resumo[[\"Acertou\"]]\n",
        "    resumo_plot.plot(kind=\"bar\", figsize=(10, 5), title=\"📊 Acertos vs Erros por Ativo e Timeframe\")\n",
        "    plt.ylabel(\"Quantidade de Sinais\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis=\"y\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 📄 Tabela completa das previsões\n",
        "    display(df_eval)\n",
        "\n",
        "    # 🔄 Atualizar o prediction_log.csv com a coluna 'Acertou'\n",
        "    try:\n",
        "        df_log = safe_read_csv(results_file)\n",
        "        df_log[\"Date\"] = pd.to_datetime(df_log[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "\n",
        "\n",
        "        for _, row in df_eval.iterrows():\n",
        "            dt = pd.to_datetime(row[\"Data Previsão\"])\n",
        "            mask = (df_log[\"Date\"] == dt) & (df_log[\"Asset\"] == row[\"Ativo\"]) & (df_log[\"Timeframe\"] == row[\"Timeframe\"])\n",
        "            df_log.loc[mask, \"Acertou\"] = row[\"Acertou\"]\n",
        "\n",
        "        df_log.to_csv(results_file, index=False)\n",
        "        print(\"✅ Log de previsões atualizado com coluna 'Acertou'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao atualizar o prediction_log.csv com 'Acertou': {e}\")\n",
        "\n",
        "    return df_eval\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def clear_models(model_dir=\"/content/models\"):\n",
        "    import shutil\n",
        "\n",
        "    if os.path.exists(model_dir):\n",
        "        print(f\"🧹 Limpando todos os modelos salvos em: {model_dir}\")\n",
        "        shutil.rmtree(model_dir)\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        print(\"✅ Modelos deletados com sucesso.\")\n",
        "    else:\n",
        "        print(\"📂 Nenhuma pasta de modelos encontrada para limpar.\")\n",
        "\n",
        "\n",
        "\n",
        "def plot_prediction_performance_por_timeframe(log_path=\"/content/prediction_log.csv\"):\n",
        "    if not os.path.exists(log_path):\n",
        "        print(\"📭 Nenhum log encontrado.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(log_path)\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "    df = df.dropna(subset=[\"TargetPrice\", \"Price\", \"Timeframe\"])\n",
        "\n",
        "    for timeframe in df[\"Timeframe\"].unique():\n",
        "        df_tf = df[df[\"Timeframe\"] == timeframe].copy()\n",
        "        df_tf[\"Erro\"] = df_tf[\"Price\"] - df_tf[\"TargetPrice\"]\n",
        "        df_tf[\"AbsError\"] = abs(df_tf[\"Erro\"])\n",
        "        df_tf[\"Dia\"] = df_tf[\"Date\"].dt.date\n",
        "\n",
        "        if df_tf.empty:\n",
        "            continue\n",
        "\n",
        "        # Erro absoluto médio por dia\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        df_grouped = df_tf.groupby(\"Dia\")[\"AbsError\"].mean()\n",
        "        plt.plot(df_grouped.index, df_grouped.values, marker=\"o\")\n",
        "        plt.title(f\"📈 Erro Absoluto Médio por Dia - {timeframe}\")\n",
        "        plt.xlabel(\"Data\")\n",
        "        plt.ylabel(\"Erro ($)\")\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"/tmp/erro_absoluto_{timeframe}.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Dispersão do valor previsto x real\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.scatter(df_tf[\"TargetPrice\"], df_tf[\"Price\"], alpha=0.6)\n",
        "        plt.plot([df_tf[\"TargetPrice\"].min(), df_tf[\"TargetPrice\"].max()],\n",
        "                [df_tf[\"TargetPrice\"].min(), df_tf[\"TargetPrice\"].max()], 'r--', label=\"Perfeito\")\n",
        "        plt.title(f\"🎯 Previsão LSTM vs Preço Real - {timeframe}\")\n",
        "        plt.xlabel(\"Valor Previsto\")\n",
        "        plt.ylabel(\"Valor Real\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        path_img = f\"/tmp/previsao_vs_real_{timeframe}.png\"\n",
        "        plt.savefig(path_img)\n",
        "        plt.close()\n",
        "        print(f\"✅ Gráfico salvo: {path_img}\")\n",
        "\n",
        "def enviar_graficos_desempenho_por_timeframe():\n",
        "    import glob\n",
        "    from pathlib import Path\n",
        "\n",
        "    timeframes = [\"15m\", \"1h\", \"1d\"]  # Edite se tiver outros\n",
        "    path_base = \"/tmp\"\n",
        "\n",
        "    for tf in timeframes:\n",
        "        # Gráfico 1: Previsão vs Real\n",
        "        grafico_pred = f\"{path_base}/previsao_vs_real_{tf}.png\"\n",
        "        if os.path.exists(grafico_pred):\n",
        "            with open(grafico_pred, \"rb\") as img:\n",
        "                url = f\"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendPhoto\"\n",
        "                files = {\"photo\": img}\n",
        "                data = {\n",
        "                    \"chat_id\": TELEGRAM_CHAT_ID,\n",
        "                    \"caption\": f\"📈 Previsão LSTM vs Real — {tf}\"\n",
        "                }\n",
        "                r = requests.post(url, data=data, files=files)\n",
        "                print(f\"✅ Enviado: previsao_vs_real_{tf}.png\")\n",
        "\n",
        "        # Gráfico 2: Erro absoluto por dia\n",
        "        grafico_erro = f\"{path_base}/erro_absoluto_{tf}.png\"\n",
        "        if os.path.exists(grafico_erro):\n",
        "            with open(grafico_erro, \"rb\") as img:\n",
        "                url = f\"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendPhoto\"\n",
        "                files = {\"photo\": img}\n",
        "                data = {\n",
        "                    \"chat_id\": TELEGRAM_CHAT_ID,\n",
        "                    \"caption\": f\"📊 Erro Absoluto por Dia — {tf}\"\n",
        "                }\n",
        "                r = requests.post(url, data=data, files=files)\n",
        "                print(f\"✅ Enviado: erro_absoluto_{tf}.png\")\n",
        "\n",
        "def enviar_grafico_lucro_por_confianca(log_path=\"/content/prediction_log.csv\"):\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    if not os.path.exists(log_path):\n",
        "        print(\"📭 Nenhum log encontrado.\")\n",
        "        return\n",
        "\n",
        "    df = safe_read_csv(log_path)\n",
        "    if \"AdjustedProb\" not in df.columns or \"TP1\" not in df.columns or \"Price\" not in df.columns:\n",
        "        print(\"⚠️ Colunas necessárias não encontradas no log.\")\n",
        "        return\n",
        "\n",
        "    df = df.dropna(subset=[\"AdjustedProb\", \"TP1\", \"Price\"])\n",
        "    df[\"LucroEstimado\"] = df[\"TP1\"] - df[\"Price\"]\n",
        "    df[\"FaixaConfiança\"] = pd.cut(df[\"AdjustedProb\"], bins=[0, 0.6, 0.7, 0.8, 0.9, 1.0], labels=[\"≤60%\", \"60-70%\", \"70-80%\", \"80-90%\", \">90%\"])\n",
        "\n",
        "    lucro_medio = df.groupby(\"FaixaConfiança\")[\"LucroEstimado\"].mean()\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    lucro_medio.plot(kind=\"bar\", color=\"skyblue\")\n",
        "    plt.title(\"📊 Lucro Estimado Médio por Faixa de Confiança\")\n",
        "    plt.ylabel(\"Lucro Estimado ($)\")\n",
        "    plt.xlabel(\"Faixa de Confiança Ajustada\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    path = \"/tmp/lucro_por_confianca.png\"\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "    with open(path, \"rb\") as img:\n",
        "        url = f\"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendPhoto\"\n",
        "        files = {\"photo\": img}\n",
        "        data = {\n",
        "            \"chat_id\": TELEGRAM_CHAT_ID,\n",
        "            \"caption\": \"📊 Lucro médio estimado por faixa de confiança ajustada\"\n",
        "        }\n",
        "        response = requests.post(url, data=data, files=files)\n",
        "        if response.status_code == 200:\n",
        "            print(\"✅ Gráfico de lucro por confiança enviado.\")\n",
        "        else:\n",
        "            print(f\"❌ Falha ao enviar gráfico: {response.status_code} - {response.text}\")\n",
        "\n",
        "def adjust_signal_based_on_history(asset, timeframe, max_lookback=20, min_signals=5):\n",
        "    try:\n",
        "        df = safe_read_csv(\"prediction_log.csv\")\n",
        "        if df is None:\n",
        "            print(\"⚠️ Ignorando leitura do prediction_log.csv pois está vazio ou ausente.\")\n",
        "            return 1.0  # Retorna confiança padrão\n",
        "\n",
        "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "\n",
        "        df = df[(df[\"Asset\"] == asset) & (df[\"Timeframe\"] == timeframe)]\n",
        "\n",
        "        if len(df) < min_signals or \"Acertou\" not in df.columns:\n",
        "            return 1.0\n",
        "\n",
        "        recent = df.sort_values(\"Date\", ascending=False).head(max_lookback)\n",
        "        acuracia = recent[\"Acertou\"].mean()\n",
        "        return acuracia\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Erro ao ajustar com histórico: {e}\")\n",
        "        return 1.0\n",
        "\n",
        "def gerar_grafico_previsao_vs_real(log_path=\"/content/prediction_log.csv\", output_path=\"/tmp/previsao_vs_real.png\"):\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    df = safe_read_csv(log_path)\n",
        "    if df is None or df.empty or \"TargetPrice\" not in df.columns or \"Price\" not in df.columns:\n",
        "        print(\"⚠️ Log inválido ou colunas ausentes.\")\n",
        "        return None\n",
        "\n",
        "    df = df.dropna(subset=[\"TargetPrice\", \"Price\"]).tail(20)  # últimos 20 sinais\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(df[\"Date\"], df[\"Price\"], label=\"📈 Preço Real\", marker=\"o\")\n",
        "    plt.plot(df[\"Date\"], df[\"TargetPrice\"], label=\"🔮 Previsão LSTM\", marker=\"x\")\n",
        "    plt.title(\"📊 Previsão LSTM vs Preço Real\")\n",
        "    plt.xlabel(\"Data\")\n",
        "    plt.ylabel(\"Preço\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "    print(f\"✅ Gráfico salvo em: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "def enviar_grafico_previsao_real(df, timeframe, asset):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.dates as mdates\n",
        "    import os\n",
        "\n",
        "    df = df[df[\"Asset\"] == asset].copy()\n",
        "    if df.empty:\n",
        "        print(f\"⚠️ Nenhum dado para {asset} ({timeframe}) no gráfico.\")\n",
        "        return\n",
        "\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        date = df[\"Date\"].iloc[i]\n",
        "        high = df[\"LSTM_High_Predicted\"].iloc[i]\n",
        "        low = df[\"LSTM_Low_Predicted\"].iloc[i]\n",
        "        close = df[\"TargetPrice\"].iloc[i]\n",
        "\n",
        "        cor = \"blue\"\n",
        "        plt.vlines(date, ymin=low, ymax=high, color=cor, linewidth=2)\n",
        "        plt.plot(date, close, marker=\"o\", color=cor)\n",
        "\n",
        "    plt.plot(df[\"Date\"], df[\"Price\"], label=\"📈 Preço Real\", marker=\"x\", color=\"black\")\n",
        "    plt.title(f\"📊 Projeção LSTM (High/Low/Close) — {asset} ({timeframe})\")\n",
        "    plt.xlabel(\"Data\")\n",
        "    plt.ylabel(\"Preço\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d/%m %H:%M', tz=BR_TZ))\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    path = f\"/tmp/previsao_vs_real_{asset.replace('-', '')}_{timeframe}.png\"\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        with open(path, \"rb\") as img:\n",
        "            url = f\"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendPhoto\"\n",
        "            files = {\"photo\": img}\n",
        "            data = {\n",
        "                \"chat_id\": TELEGRAM_CHAT_ID,\n",
        "                \"caption\": f\"📊 Projeção LSTM — {asset} ({timeframe})\"\n",
        "            }\n",
        "            response = requests.post(url, data=data, files=files)\n",
        "            if response.status_code == 200:\n",
        "                print(\"✅ Gráfico enviado ao Telegram.\")\n",
        "            else:\n",
        "                print(f\"❌ Erro ao enviar gráfico: {response.status_code} - {response.text}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def enviar_grafico_carteira():\n",
        "    image_path = \"/tmp/evolucao_carteira.png\"\n",
        "    if os.path.exists(image_path):\n",
        "        with open(image_path, \"rb\") as img:\n",
        "            url = f\"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendPhoto\"\n",
        "            files = {\"photo\": img}\n",
        "            data = {\n",
        "                \"chat_id\": TELEGRAM_CHAT_ID,\n",
        "                \"caption\": \"💼 Evolução da carteira virtual com base nos sinais do bot\"\n",
        "            }\n",
        "            response = requests.post(url, data=data, files=files)\n",
        "            if response.status_code == 200:\n",
        "                print(\"✅ Gráfico da carteira enviado ao Telegram.\")\n",
        "            else:\n",
        "                print(f\"❌ Erro ao enviar imagem: {response.status_code} - {response.text}\")\n",
        "\n",
        "# 📊 Cálculo automático do atr_multiplier baseado nos últimos candles\n",
        "def calcular_atr_auto(dataframe, intervalo=\"15m\", n=50, fator_ajuste=1.2):\n",
        "    try:\n",
        "        df_recent = dataframe.tail(n)\n",
        "        if df_recent.empty or not all(col in df_recent.columns for col in [\"High\", \"Low\", \"Close\"]):\n",
        "            return 0.03  # valor padrão caso não tenha dados\n",
        "\n",
        "        # Cálculo da média da variação percentual entre High e Low\n",
        "        media_range_pct = ((df_recent[\"High\"] - df_recent[\"Low\"]) / df_recent[\"Close\"]).mean()\n",
        "        atr_multiplier = round(media_range_pct * fator_ajuste, 4)\n",
        "\n",
        "        # Valor mínimo e máximo para manter limites razoáveis\n",
        "        atr_multiplier = max(0.01, min(atr_multiplier, 0.08))\n",
        "        return atr_multiplier\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Erro no ajuste automático do ATR: {e}\")\n",
        "        return 0.03  # fallback padrão\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 5.1 CARTEIRA VIRTUAL PARA SIMULAÇÃO\n",
        "# ====================================================\n",
        "# ====================================================\n",
        "# 5.1 CARTEIRA VIRTUAL PARA SIMULAÇÃO\n",
        "# ====================================================\n",
        "\n",
        "carteira_virtual = {\n",
        "    \"capital_inicial\": 10000.0,\n",
        "    \"capital_atual\": 10000.0,\n",
        "    \"capital_maximo\": 10000.0,  # para cálculo de drawdown\n",
        "    \"historico_capital\": [],    # track evolução do capital\n",
        "    \"em_operacao\": False,\n",
        "}\n",
        "\n",
        "\n",
        "def to_scalar(val):\n",
        "    try:\n",
        "        if isinstance(val, pd.Series):\n",
        "            return float(val.iloc[0])\n",
        "        elif isinstance(val, (np.ndarray, list)):\n",
        "            return float(val[0])\n",
        "        elif val is None:\n",
        "            return np.nan\n",
        "        else:\n",
        "            return float(val)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Falha ao converter valor escalar: {val} | erro: {e}\")\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "def salvar_carteira_virtual(filepath=\"/content/carteira_virtual.json\"):\n",
        "    with open(filepath, \"w\") as f:\n",
        "        json.dump(carteira_virtual, f)\n",
        "    print(f\"💾 Carteira virtual salva em: {filepath}\")\n",
        "\n",
        "\n",
        "def carregar_carteira_virtual(filepath=\"/content/carteira_virtual.json\"):\n",
        "    global carteira_virtual\n",
        "    if os.path.exists(filepath):\n",
        "        with open(filepath, \"r\") as f:\n",
        "            carteira_virtual = json.load(f)\n",
        "        print(f\"📂 Carteira virtual carregada de: {filepath}\")\n",
        "    else:\n",
        "        print(f\"⚠️ Arquivo de carteira não encontrado. Usando valores padrões.\")\n",
        "\n",
        "def exibir_status_carteira():\n",
        "    ci = carteira_virtual.get(\"capital_inicial\", 10000.0)\n",
        "    ca = carteira_virtual.get(\"capital_atual\", 10000.0)\n",
        "    cm = carteira_virtual.get(\"capital_maximo\", ca)\n",
        "\n",
        "    roi = ((ca / ci) - 1) * 100\n",
        "    drawdown = (1 - (ca / cm)) * 100\n",
        "\n",
        "    print(\"\\n💰 Status da Carteira Virtual:\")\n",
        "    print(f\"• Capital Inicial: ${ci:,.2f}\")\n",
        "    print(f\"• Capital Atual  : ${ca:,.2f}\")\n",
        "    print(f\"• ROI Acumulado  : {roi:+.2f}%\")\n",
        "    print(f\"• Drawdown Atual : {drawdown:.2f}%\\n\")\n",
        "\n",
        "def simular_trade(row, df):\n",
        "    try:\n",
        "        asset = row[\"Asset\"]\n",
        "        timeframe = row[\"Timeframe\"]\n",
        "        signal_time = pd.to_datetime(row[\"Date\"], utc=True).astimezone(BR_TZ)\n",
        "\n",
        "        preco_entrada = float(row[\"Entry\"])\n",
        "        tp1 = float(row[\"TP1\"])\n",
        "        sl = float(row[\"SL\"])\n",
        "\n",
        "        if df.index.tz is None:\n",
        "            df.index = df.index.tz_localize(pytz.UTC).tz_convert(BR_TZ)\n",
        "        else:\n",
        "            df.index = df.index.tz_convert(BR_TZ)\n",
        "\n",
        "        df_future = df[df.index > signal_time]\n",
        "        if df_future.empty or not all(col in df_future.columns for col in [\"High\", \"Low\", \"Close\"]):\n",
        "            raise ValueError(\"Candles futuros indisponíveis ou incompletos.\")\n",
        "\n",
        "        # 🟡 Taxas e slippage\n",
        "        taxa_percentual = 0.001  # 0.1% por operação\n",
        "        slippage_percentual = 0.002  # 0.2% por operação\n",
        "\n",
        "        entrada_executada = False\n",
        "        for i, (idx, candle) in enumerate(df_future.iterrows()):\n",
        "            high = float(candle[\"High\"])\n",
        "            low = float(candle[\"Low\"])\n",
        "\n",
        "            if not entrada_executada:\n",
        "                if row[\"Signal\"] == 1 and low <= preco_entrada:\n",
        "                    preco_real_entrada = low\n",
        "                    entrada_executada = True\n",
        "                    entrada_idx = idx\n",
        "                elif row[\"Signal\"] == 0 and high >= preco_entrada:\n",
        "                    preco_real_entrada = high\n",
        "                    entrada_executada = True\n",
        "                    entrada_idx = idx\n",
        "                continue\n",
        "\n",
        "            if entrada_executada:\n",
        "                preco_max = float(candle[\"High\"])\n",
        "                preco_min = float(candle[\"Low\"])\n",
        "\n",
        "                if row[\"Signal\"] == 1:\n",
        "                    if preco_min <= sl:\n",
        "                        resultado = \"SL\"\n",
        "                        preco_saida = sl\n",
        "                        break\n",
        "                    elif preco_max >= tp1:\n",
        "                        resultado = \"TP1\"\n",
        "                        preco_saida = tp1\n",
        "                        break\n",
        "                elif row[\"Signal\"] == 0:\n",
        "                    if preco_max >= sl:\n",
        "                        resultado = \"SL\"\n",
        "                        preco_saida = sl\n",
        "                        break\n",
        "                    elif preco_min <= tp1:\n",
        "                        resultado = \"TP1\"\n",
        "                        preco_saida = tp1\n",
        "                        break\n",
        "\n",
        "        else:\n",
        "            if entrada_executada:\n",
        "                resultado = \"Sem alvo\"\n",
        "                preco_saida = df_future[\"Close\"].iloc[-1]\n",
        "            else:\n",
        "                return {\n",
        "                    \"Resultado\": \"Sem execução\",\n",
        "                    \"PrecoSaida\": None,\n",
        "                    \"LucroEstimado\": None,\n",
        "                    \"DuracaoMin\": None,\n",
        "                    \"Capital Atual\": carteira_virtual[\"capital_atual\"],\n",
        "                    \"Quantidade\": None,\n",
        "                    \"ROI\": None,\n",
        "                    \"Drawdown\": None\n",
        "                }\n",
        "\n",
        "        capital_disponivel = carteira_virtual[\"capital_atual\"]\n",
        "        risco_por_trade = 0.01\n",
        "        risco_trade = abs(preco_real_entrada - sl)\n",
        "\n",
        "        if risco_trade <= 0:\n",
        "            capital_por_trade = capital_disponivel * 0.01\n",
        "        else:\n",
        "            capital_por_trade = (capital_disponivel * risco_por_trade) / risco_trade\n",
        "\n",
        "        quantidade = capital_por_trade\n",
        "        if quantidade * preco_real_entrada > capital_disponivel * 0.10:\n",
        "            quantidade = (capital_disponivel * 0.10) / preco_real_entrada\n",
        "\n",
        "        quantidade = max(quantidade, 0.0001)\n",
        "\n",
        "        # 🔥 Cálculo de lucro com taxa + slippage\n",
        "        if row[\"Signal\"] == 1:\n",
        "            lucro_total = (preco_saida - preco_real_entrada) * quantidade\n",
        "        else:\n",
        "            lucro_total = (preco_real_entrada - preco_saida) * quantidade\n",
        "\n",
        "        custo_total = (preco_real_entrada + preco_saida) * quantidade * (taxa_percentual + slippage_percentual)\n",
        "        lucro_total -= custo_total\n",
        "\n",
        "        carteira_virtual[\"capital_atual\"] += lucro_total\n",
        "        carteira_virtual[\"historico_capital\"].append(carteira_virtual[\"capital_atual\"])\n",
        "\n",
        "        if carteira_virtual[\"capital_atual\"] > carteira_virtual[\"capital_maximo\"]:\n",
        "            carteira_virtual[\"capital_maximo\"] = carteira_virtual[\"capital_atual\"]\n",
        "\n",
        "        drawdown = 1 - (carteira_virtual[\"capital_atual\"] / carteira_virtual[\"capital_maximo\"])\n",
        "        roi = (carteira_virtual[\"capital_atual\"] / carteira_virtual[\"capital_inicial\"]) - 1\n",
        "        duracao = (idx - entrada_idx).total_seconds() / 60 if entrada_executada else None\n",
        "\n",
        "        return {\n",
        "            \"Resultado\": resultado,\n",
        "            \"PrecoSaida\": preco_saida,\n",
        "            \"LucroEstimado\": round(lucro_total, 2),\n",
        "            \"DuracaoMin\": round(duracao, 1) if duracao is not None else None,\n",
        "            \"Capital Atual\": round(carteira_virtual[\"capital_atual\"], 2),\n",
        "            \"Quantidade\": round(quantidade, 6),\n",
        "            \"ROI\": round(roi * 100, 2),\n",
        "            \"Drawdown\": round(drawdown * 100, 2)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro inesperado na simulação: {e}\")\n",
        "        return {\n",
        "            \"Resultado\": \"Erro\",\n",
        "            \"PrecoSaida\": None,\n",
        "            \"LucroEstimado\": None,\n",
        "            \"DuracaoMin\": None,\n",
        "            \"Capital Atual\": carteira_virtual[\"capital_atual\"],\n",
        "            \"Quantidade\": None,\n",
        "            \"ROI\": None,\n",
        "            \"Drawdown\": None\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "def plotar_grafico_previsao_futura(df_previsao, timeframe, asset):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.dates as mdates\n",
        "\n",
        "    if df_previsao is None or not all(k in df_previsao for k in [\"Date\", \"High\", \"Low\", \"Close\"]):\n",
        "        print(f\"⚠️ Dados de previsão futura incompletos para {asset} ({timeframe})\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(df_previsao)\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        date = df[\"Date\"].iloc[i]\n",
        "        high = df[\"High\"].iloc[i]\n",
        "        low = df[\"Low\"].iloc[i]\n",
        "        close = df[\"Close\"].iloc[i]\n",
        "\n",
        "        plt.vlines(date, ymin=low, ymax=high, color=\"blue\", linewidth=2)\n",
        "        plt.plot(date, close, marker=\"o\", color=\"blue\")\n",
        "        plt.annotate(f\"{close:.0f}\", (date, close), xytext=(0, 8),\n",
        "                     textcoords=\"offset points\", ha='center', fontsize=8, color=\"blue\")\n",
        "\n",
        "    plt.title(f\"🔮 Projeção Futura (LSTM) — {asset} ({timeframe})\")\n",
        "    plt.xlabel(\"Data\")\n",
        "    plt.ylabel(\"Preço Projetado\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d/%m %H:%M', tz=BR_TZ))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "def plotar_grafico_previsao_real(df, timeframe, asset):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.dates as mdates\n",
        "\n",
        "    df = df[df[\"Asset\"] == asset].copy()\n",
        "    if df.empty:\n",
        "        print(f\"⚠️ Nenhum dado para {asset} ({timeframe}) no gráfico.\")\n",
        "        return\n",
        "\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        date = df[\"Date\"].iloc[i]\n",
        "        high = df[\"LSTM_High_Predicted\"].iloc[i]\n",
        "        low = df[\"LSTM_Low_Predicted\"].iloc[i]\n",
        "        close = df[\"TargetPrice\"].iloc[i]\n",
        "        real = df[\"Price\"].iloc[i]\n",
        "\n",
        "        cor = \"green\" if close >= real else \"red\"\n",
        "        plt.vlines(date, ymin=low, ymax=high, color=cor, linewidth=2)\n",
        "        plt.plot(date, close, marker=\"o\", color=cor)\n",
        "\n",
        "        plt.annotate(f\"{close:.0f}\", (date, close), xytext=(0, 8),\n",
        "                     textcoords=\"offset points\", ha='center', fontsize=8, color=cor)\n",
        "        plt.annotate(f\"{real:.0f}\", (date, real), xytext=(0, -12),\n",
        "                     textcoords=\"offset points\", ha='center', fontsize=8, color=\"black\")\n",
        "\n",
        "    plt.plot(df[\"Date\"], df[\"Price\"], label=\"📈 Preço Real\", marker=\"x\", color=\"black\")\n",
        "    plt.title(f\"📊 Projeção LSTM (High/Low/Close) — {asset} ({timeframe})\")\n",
        "    plt.xlabel(\"Data\")\n",
        "    plt.ylabel(\"Preço\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d/%m %H:%M', tz=BR_TZ))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plotar_grafico_carteira_virtual(log_path=\"/content/prediction_log.csv\"):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import os\n",
        "\n",
        "    if not os.path.exists(log_path):\n",
        "        print(\"❌ Arquivo de log não encontrado.\")\n",
        "        return\n",
        "\n",
        "    df = safe_read_csv(log_path)\n",
        "    if df is None or df.empty:\n",
        "        print(\"⚠️ Log de previsões vazio ou inválido.\")\n",
        "        return\n",
        "\n",
        "    df = df.dropna(subset=[\"Date\", \"Capital Atual\", \"Resultado\"])\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "    df = df[df[\"Resultado\"].isin([\"TP1\", \"SL\", \"Sem alvo\"])]\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"⚠️ Nenhuma simulação válida para exibir no gráfico.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    cor_map = {\"TP1\": \"green\", \"SL\": \"red\", \"Sem alvo\": \"orange\"}\n",
        "    cores = df[\"Resultado\"].map(cor_map).fillna(\"gray\")\n",
        "\n",
        "    plt.scatter(df[\"Date\"], df[\"Capital Atual\"], c=cores, edgecolors=\"black\", s=70)\n",
        "    plt.plot(df[\"Date\"], df[\"Capital Atual\"], linestyle=\"--\", color=\"blue\", alpha=0.7)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        plt.annotate(f\"${row['Capital Atual']:.0f}\", (row[\"Date\"], row[\"Capital Atual\"]),\n",
        "                     textcoords=\"offset points\", xytext=(0, 6), ha='center', fontsize=8)\n",
        "\n",
        "    plt.title(\"💰 Evolução da Carteira Virtual\")\n",
        "    plt.xlabel(\"Data (BR)\")\n",
        "    plt.ylabel(\"Capital ($)\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # 🔥 Novo: salvar o gráfico em /content/\n",
        "    path = \"/content/evolucao_carteira_virtual.png\"\n",
        "    plt.savefig(path)\n",
        "    print(f\"💾 Gráfico da carteira salvo em: {path}\")\n",
        "\n",
        "    # 🔥 Novo: mostrar no terminal\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plotar_grafico_lucro(df):\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    df_valid = df[df[\"Resultado\"].isin([\"TP1\", \"SL\", \"Sem alvo\"])].copy()\n",
        "    if df_valid.empty:\n",
        "        print(\"⚠️ Nenhum resultado válido para gráfico.\")\n",
        "        return\n",
        "\n",
        "    df_valid[\"FaixaConfiança\"] = pd.cut(\n",
        "        df_valid[\"AdjustedProb\"].fillna(0.5),\n",
        "        bins=[0, 0.6, 0.75, 0.9, 1.01],\n",
        "        labels=[\"<60%\", \"60-75%\", \"75-90%\", \">90%\"]\n",
        "    )\n",
        "\n",
        "    lucro_medio = df_valid.groupby(\"FaixaConfiança\")[\"LucroEstimado\"].mean()\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    lucro_medio.plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n",
        "    plt.title(\"📊 Lucro Médio por Faixa de Confiança\")\n",
        "    plt.ylabel(\"Lucro Estimado\")\n",
        "    plt.xlabel(\"Faixa de Confiança\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    path = \"lucro_por_faixa.png\"\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "    # ✅ ADICIONE ISTO para mostrar o gráfico no log também\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    lucro_medio.plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n",
        "    plt.title(\"📊 Lucro Médio por Faixa de Confiança (visualização)\")\n",
        "    plt.ylabel(\"Lucro Estimado\")\n",
        "    plt.xlabel(\"Faixa de Confiança\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"✅ Gráfico de lucro por confiança enviado.\")\n",
        "\n",
        "\n",
        "\n",
        "def simular_todos_trades(prediction_log_path=\"prediction_log.csv\", df_candles=None, timeframe=\"15m\"):\n",
        "    print(\"📊 Rodando simulação de carteira virtual com sinais do log...\")\n",
        "\n",
        "    if not os.path.exists(prediction_log_path):\n",
        "        print(\"⚠️ Log de previsões não encontrado.\")\n",
        "        return\n",
        "\n",
        "    df_log = safe_read_csv(prediction_log_path)\n",
        "    if df_log is None or df_log.empty:\n",
        "        print(\"⚠️ Log vazio.\")\n",
        "        return\n",
        "\n",
        "    df_log[\"Date\"] = pd.to_datetime(df_log[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "\n",
        "    intervalo_futuro = {\n",
        "        \"15m\": timedelta(minutes=15 * 5),\n",
        "        \"1h\": timedelta(hours=5),\n",
        "        \"4h\": timedelta(hours=20),\n",
        "        \"1d\": timedelta(days=5),\n",
        "        \"1wk\": timedelta(weeks=5)\n",
        "    }.get(timeframe, timedelta(hours=1))\n",
        "\n",
        "    now = datetime.now(BR_TZ)\n",
        "    resultados = []\n",
        "\n",
        "    for _, row in df_log.iterrows():\n",
        "        signal_time = pd.to_datetime(row[\"Date\"], utc=True).tz_convert(BR_TZ)\n",
        "\n",
        "        if (now - signal_time) < intervalo_futuro:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            resultado = simular_trade_com_entradas_em_grade(row, df_candles)\n",
        "            for key, value in resultado.items():\n",
        "                row[key] = value\n",
        "            resultados.append(row)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erro inesperado na simulação: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not resultados:\n",
        "        print(\"📭 Nenhum trade foi simulado (ainda).\")\n",
        "        return\n",
        "\n",
        "    df_resultados = pd.DataFrame(resultados)\n",
        "    df_resultados.to_csv(prediction_log_path, index=False)\n",
        "    salvar_carteira_virtual()\n",
        "\n",
        "    print(f\"📋 Log de previsões atualizado com resultados e capital: {prediction_log_path}\")\n",
        "    plotar_grafico_lucro(df_resultados)\n",
        "    salvar_grafico_evolucao()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def salvar_grafico_evolucao(log_path=\"prediction_log.csv\"):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import os\n",
        "\n",
        "    if not os.path.exists(log_path):\n",
        "        print(\"❌ Arquivo de log não encontrado.\")\n",
        "        return\n",
        "\n",
        "    df = safe_read_csv(log_path)\n",
        "    if df is None or df.empty:\n",
        "        print(\"⚠️ Log de previsões vazio ou inválido.\")\n",
        "        return\n",
        "\n",
        "    # 🛡️ Nova proteção: checa se as colunas necessárias existem\n",
        "    if \"Capital Atual\" not in df.columns or \"Resultado\" not in df.columns:\n",
        "        print(\"📭 Sem dados de simulação para gerar gráfico de evolução.\")\n",
        "        return\n",
        "\n",
        "    df = df.dropna(subset=[\"Date\", \"Capital Atual\", \"Resultado\"])\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "    df = df[df[\"Resultado\"].isin([\"TP1\", \"SL\", \"Sem alvo\"])]\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"⚠️ Nenhuma simulação válida para exibir no gráfico.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Cores por tipo de resultado\n",
        "    cor_map = {\"TP1\": \"green\", \"SL\": \"red\", \"Sem alvo\": \"orange\"}\n",
        "    cores = df[\"Resultado\"].map(cor_map).fillna(\"gray\")\n",
        "\n",
        "    # Gráfico de pontos\n",
        "    plt.scatter(df[\"Date\"], df[\"Capital Atual\"], c=cores, edgecolors=\"black\", s=70)\n",
        "\n",
        "    # Linha de evolução do capital\n",
        "    plt.plot(df[\"Date\"], df[\"Capital Atual\"], linestyle=\"--\", color=\"blue\", alpha=0.7)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        plt.annotate(f\"${row['Capital Atual']:.0f}\", (row[\"Date\"], row[\"Capital Atual\"]),\n",
        "                     textcoords=\"offset points\", xytext=(0, 6), ha='center', fontsize=8)\n",
        "\n",
        "    plt.title(\"💰 Evolução da Carteira Virtual\")\n",
        "    plt.xlabel(\"Data (BR)\")\n",
        "    plt.ylabel(\"Capital ($)\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    path = \"/tmp/evolucao_carteira.png\"\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"✅ Gráfico da carteira salvo: {path}\")\n",
        "\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 6. EXECUÇÃO DAS ANÁLISES E ALERTAS\n",
        "# ====================================================\n",
        "def run_analysis(\n",
        "    selected_timeframes=None,\n",
        "    plot_timeframes=[\"15m\", \"1h\"],\n",
        "    alert_timeframes=[\"15m\", \"1h\", \"1d\", \"1wk\"],\n",
        "    retrain_models=False\n",
        "):\n",
        "    criar_prediction_log_padrao()\n",
        "    carregar_carteira_virtual()\n",
        "\n",
        "    log_path = \"/content/prediction_log.csv\"\n",
        "    df_log_old = safe_read_csv(log_path)\n",
        "    if df_log_old is None:\n",
        "        df_log_old = pd.DataFrame(columns=[\"Asset\", \"Timeframe\", \"Date\"])\n",
        "\n",
        "    results = []\n",
        "\n",
        "    if selected_timeframes is None:\n",
        "        selected_timeframes = TIMEFRAMES\n",
        "\n",
        "    for asset in ASSETS:\n",
        "        try:\n",
        "            models = {}\n",
        "            lstm_models = {}\n",
        "            data = {}\n",
        "\n",
        "            for tf in selected_timeframes:\n",
        "                interval = tf['interval']\n",
        "                period = tf['period']\n",
        "\n",
        "                df = get_stock_data(asset, interval, period)\n",
        "                df = calculate_indicators(df)\n",
        "                data[interval] = df\n",
        "\n",
        "                if retrain_models:\n",
        "                    print(f\"🛠️ Forçando treinamento dos modelos para {asset} ({interval})\")\n",
        "                    models[interval] = train_ml_model(df, asset=asset, interval=interval, verbose=True, force_retrain=True)\n",
        "\n",
        "                    if interval in [\"1d\", \"1wk\"]:\n",
        "                        lstm_models[interval] = train_lstm_model_diario(\n",
        "                            df, asset=asset, interval=interval, window_size=60, force_retrain=True\n",
        "                        )\n",
        "                    else:\n",
        "                        lstm_models[interval] = train_lstm_model(\n",
        "                            df, asset=asset, interval=interval, window_size=20, force_retrain=True\n",
        "                        )\n",
        "                else:\n",
        "                    models[interval] = load_xgb_model(asset, interval)\n",
        "                    if models[interval] is None:\n",
        "                        models[interval] = train_ml_model(df, asset=asset, interval=interval, verbose=True, force_retrain=True)\n",
        "\n",
        "                    lstm_models[interval] = load_lstm_model(asset, interval)\n",
        "                    if lstm_models[interval] is None:\n",
        "                        if interval in [\"1d\", \"1wk\"]:\n",
        "                            lstm_models[interval] = train_lstm_model_diario(\n",
        "                                df, asset=asset, interval=interval, window_size=60, force_retrain=True\n",
        "                            )\n",
        "                        else:\n",
        "                            lstm_models[interval] = train_lstm_model(\n",
        "                                df, asset=asset, interval=interval, window_size=20, force_retrain=True\n",
        "                            )\n",
        "\n",
        "            if all(model is None for model in models.values()):\n",
        "                print(f\"⚠️ Nenhum modelo foi treinado para {asset}.\")\n",
        "                continue\n",
        "\n",
        "            for tf in selected_timeframes:\n",
        "                interval = tf['interval']\n",
        "                latest_data = data[interval].iloc[-1]\n",
        "                current_price = data[interval][\"Close\"].iloc[-1]\n",
        "\n",
        "                predicted_price_lstm = None\n",
        "                pred_high = None\n",
        "                pred_low = None\n",
        "\n",
        "                try:\n",
        "                    lstm_model = lstm_models.get(interval)\n",
        "                    if lstm_model:\n",
        "                        pred_lstm = predict_with_lstm(lstm_model, data[interval], asset=asset, interval=interval)\n",
        "\n",
        "                        if pred_lstm is None or any(pred_lstm.get(k) is None for k in [\"High\", \"Low\", \"Close\"]):\n",
        "                            print(f\"⚠️ Previsão LSTM inválida para {asset} ({interval}) — pulando análise.\")\n",
        "                            continue\n",
        "\n",
        "                        predicted_price_lstm = pred_lstm[\"Close\"]\n",
        "                        pred_high = pred_lstm[\"High\"]\n",
        "                        pred_low = pred_lstm[\"Low\"]\n",
        "\n",
        "                        print(f\"\\n🔍 {asset} ({interval})\")\n",
        "                        print(f\"   Preço atual: ${current_price:,.2f}\")\n",
        "                        print(f\"   Previsão LSTM: Close=${predicted_price_lstm:,.2f} | High=${pred_high:,.2f} | Low=${pred_low:,.2f}\")\n",
        "\n",
        "                        # 📏 Verificação dinâmica da distância mínima\n",
        "                        ultimos_n_candles = 50\n",
        "                        df_candles_hist = data[interval].tail(ultimos_n_candles)\n",
        "                        media_range_real = (df_candles_hist[\"High\"] - df_candles_hist[\"Low\"]).mean()\n",
        "                        range_previsto = pred_high - pred_low\n",
        "\n",
        "                        rr_ratio = None\n",
        "                        if range_previsto < media_range_real * 0.8:\n",
        "                            print(f\"⚠️ Range previsto pequeno ({range_previsto:.2f} < {media_range_real*0.8:.2f}), ajustando TP1.\")\n",
        "\n",
        "                            # Ajustar Entry e TP1\n",
        "                            entry_price = pred_low\n",
        "                            tp1 = entry_price + media_range_real\n",
        "                            sl = entry_price - (entry_price * 0.01)\n",
        "\n",
        "                            # Lucro líquido mínimo\n",
        "                            taxa_percentual = 0.001\n",
        "                            slippage_percentual = 0.002\n",
        "                            custo_total_percent = taxa_percentual + slippage_percentual\n",
        "\n",
        "                            lucro_bruto = tp1 - entry_price\n",
        "                            lucro_liquido = lucro_bruto * (1 - custo_total_percent)\n",
        "                            lucro_percentual_liquido = lucro_liquido / entry_price\n",
        "\n",
        "                            if lucro_percentual_liquido < 0.002:\n",
        "                                print(f\"🚫 Lucro líquido estimado muito baixo ({lucro_percentual_liquido*100:.2f}%), descartando trade.\")\n",
        "                                results.append({\n",
        "                                    \"Asset\": asset,\n",
        "                                    \"Timeframe\": interval,\n",
        "                                    \"Date\": datetime.now(),\n",
        "                                    \"Price\": current_price,\n",
        "                                    \"Signal\": \"Descartado\",\n",
        "                                    \"Reason\": \"Lucro líquido insuficiente após ajuste\",\n",
        "                                    \"Predicted_Close\": predicted_price_lstm,\n",
        "                                    \"Predicted_High\": pred_high,\n",
        "                                    \"Predicted_Low\": pred_low\n",
        "                                })\n",
        "                                continue\n",
        "                            else:\n",
        "                                print(f\"✅ Trade ajustado aceito: lucro líquido = {lucro_percentual_liquido*100:.2f}%\")\n",
        "                        else:\n",
        "                            entry_price = pred_low\n",
        "                            tp1 = pred_high\n",
        "                            sl = entry_price - (entry_price * 0.01)\n",
        "\n",
        "                            distancia = abs(entry_price - sl)\n",
        "                            tp2 = entry_price + 2 * distancia\n",
        "\n",
        "                            print(f\"📈 Entrada: {entry_price:.2f} | TP1: {tp1:.2f} | SL: {sl:.2f} | TP2: {tp2:.2f}\")\n",
        "\n",
        "                            # 🧮 Calcula o Risk/Reward sem travar\n",
        "                            rr_ratio = (tp1 - entry_price) / (entry_price - sl) if (entry_price - sl) != 0 else None\n",
        "\n",
        "                            if rr_ratio is not None:\n",
        "                                print(f\"📊 R/R estimado: {rr_ratio:.2f}\")\n",
        "                            else:\n",
        "                                print(\"📊 R/R não pôde ser calculado (divisão por zero)\")\n",
        "\n",
        "                            # 🔵 NOVO: apenas registra o R/R e continua normalmente!\n",
        "                            # (não cancela mais o trade se o R/R for baixo)\n",
        "\n",
        "                            # (a partir daqui o fluxo continua aceitando o trade normalmente)\n",
        "\n",
        "\n",
        "                        rr_ratio = round(rr_ratio, 2)\n",
        "\n",
        "\n",
        "                        # === Entradas em grade automática ===\n",
        "                        grade_steps = 3\n",
        "                        grade_spacing = (entry_price - sl) / grade_steps if entry_price > sl else (sl - entry_price) / grade_steps\n",
        "\n",
        "                        entry_levels = []\n",
        "                        for i in range(grade_steps):\n",
        "                            if pred_high > pred_low:\n",
        "                                entry_n = round(entry_price - i * grade_spacing, 2)\n",
        "                            else:\n",
        "                                entry_n = round(entry_price + i * grade_spacing, 2)\n",
        "                            entry_levels.append(entry_n)\n",
        "\n",
        "                        entry1, entry2, entry3 = entry_levels if len(entry_levels) == 3 else (entry_price, entry_price, entry_price)\n",
        "                        # 🧠 Ensemble LSTM + XGBoost\n",
        "                        model_xgb = models.get(interval)\n",
        "                        xgb_signal = 1  # Default: aceita\n",
        "\n",
        "                        if model_xgb is not None:\n",
        "                            try:\n",
        "                                # ➡️ Gerar LSTM_PRED para o último dado\n",
        "                                features_lstm = get_lstm_feature_columns()\n",
        "                                df_features_now = data[interval].iloc[[-1]].copy()\n",
        "\n",
        "                                if len(df_features_now) >= lstm_model.window_size:\n",
        "                                    pred_lstm_now = predict_with_lstm(lstm_model, data[interval], asset=asset, interval=interval)\n",
        "                                    if pred_lstm_now and pred_lstm_now.get(\"Close\") is not None:\n",
        "                                        df_features_now[\"LSTM_PRED\"] = pred_lstm_now[\"Close\"]\n",
        "                                    else:\n",
        "                                        df_features_now[\"LSTM_PRED\"] = np.nan\n",
        "                                else:\n",
        "                                    df_features_now[\"LSTM_PRED\"] = np.nan\n",
        "\n",
        "                                # ➡️ Agora monta as features corretas\n",
        "                                features_xgb = get_feature_columns(df_features_now, include_lstm_pred=True)\n",
        "\n",
        "                                # Ajusta para apenas colunas válidas\n",
        "                                df_features_now = df_features_now[features_xgb].fillna(0)\n",
        "\n",
        "                                xgb_signal = model_xgb.predict(df_features_now)[0]\n",
        "                                print(f\"📈 XGBoost sinalizou: {'COMPRA' if xgb_signal == 1 else 'VENDA'}\")\n",
        "                            except Exception as e:\n",
        "                                print(f\"⚠️ Erro ao usar XGBoost para validar: {e}\")\n",
        "\n",
        "                        if xgb_signal != 1:\n",
        "                            print(f\"🚫 Trade cancelado pelo XGBoost.\")\n",
        "                            results.append({\n",
        "                                \"Asset\": asset,\n",
        "                                \"Timeframe\": interval,\n",
        "                                \"Date\": datetime.now(),\n",
        "                                \"Price\": current_price,\n",
        "                                \"Signal\": \"Descartado\",\n",
        "                                \"Reason\": \"XGBoost não confirmou o sinal\",\n",
        "                                \"Predicted_Close\": predicted_price_lstm,\n",
        "                                \"Predicted_High\": pred_high,\n",
        "                                \"Predicted_Low\": pred_low\n",
        "                            })\n",
        "                            continue\n",
        "\n",
        "\n",
        "                        # ✅ Se passou tudo: sinal aceito\n",
        "                        print(f\"✅ Sinal final aceito com RR {rr_ratio}.\")\n",
        "\n",
        "                        ajuste = adjust_signal_based_on_history(asset, interval)\n",
        "                        val_score = model_xgb.validation_score if model_xgb and hasattr(model_xgb, \"validation_score\") else {}\n",
        "\n",
        "                        results.append({\n",
        "                            \"Asset\": asset,\n",
        "                            \"Timeframe\": interval,\n",
        "                            \"Date\": datetime.now(),\n",
        "                            \"Price\": current_price,\n",
        "                            \"Signal\": 1,\n",
        "                            \"Reason\": \"Aceito\",\n",
        "                            \"Confidence\": None,\n",
        "                            \"AdjustedProb\": round(ajuste, 2),\n",
        "                            \"TP1\": tp1,\n",
        "                            \"TP2\": tp2,\n",
        "                            \"SL\": sl,\n",
        "                            \"Entry\": entry_price,\n",
        "                            \"Entry1\": entry1,\n",
        "                            \"Entry2\": entry2,\n",
        "                            \"Entry3\": entry3,\n",
        "                            \"Accuracy\": val_score.get(\"accuracy\"),\n",
        "                            \"Precision\": val_score.get(\"precision\"),\n",
        "                            \"Recall\": val_score.get(\"recall\"),\n",
        "                            \"F1\": val_score.get(\"f1\"),\n",
        "                            \"LSTM_Predicted\": predicted_price_lstm,\n",
        "                            \"TargetPrice\": predicted_price_lstm,\n",
        "                            \"LSTM_High_Predicted\": pred_high,\n",
        "                            \"LSTM_Low_Predicted\": pred_low\n",
        "                        })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"[!] Erro inesperado na previsão/sinal: {e}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erro inesperado ao processar {asset}: {e}\")\n",
        "            continue\n",
        "\n",
        "    df_results = pd.DataFrame(results)\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    filename = f\"model_results_{timestamp}.csv\"\n",
        "    df_results.to_csv(filename, index=False)\n",
        "    print(f\"\\n📁 Resultados salvos em: {filename}\")\n",
        "\n",
        "    if df_log_old is not None:\n",
        "        df_log_combined = pd.concat([df_log_old, df_results], ignore_index=True).fillna(\"\")\n",
        "        df_log_combined.to_csv(log_path, index=False)\n",
        "        print(f\"📋 Log de previsões atualizado em: {log_path}\")\n",
        "    else:\n",
        "        df_results.to_csv(log_path, index=False)\n",
        "        print(f\"🔄 Log de previsões criado em: {log_path}\")\n",
        "\n",
        "    salvar_carteira_virtual()\n",
        "    exibir_status_carteira()\n",
        "\n",
        "    # 🔚 Após a execução principal, envia os gráficos da previsão e da carteira\n",
        "    for tf in selected_timeframes:\n",
        "        interval = tf['interval']\n",
        "        for asset in ASSETS:\n",
        "            try:\n",
        "                enviar_grafico_previsao_real(df_results, interval, asset)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Erro ao enviar gráfico de previsão para {asset} ({interval}): {e}\")\n",
        "\n",
        "    try:\n",
        "        enviar_grafico_carteira()\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Erro ao enviar gráfico da carteira: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 7. AGENDAMENTO E EXECUÇÃO AUTOMÁTICA COM THREADS\n",
        "# ====================================================\n",
        "import time\n",
        "import threading\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Função que checa se é hora de rodar\n",
        "def is_time_to_run(interval):\n",
        "    now = datetime.now(BR_TZ)\n",
        "\n",
        "    if interval == \"15m\" and now.minute % 15 == 0:\n",
        "        return True\n",
        "    elif interval == \"1h\" and now.minute == 0:\n",
        "        return True\n",
        "    elif interval == \"1d\" and now.hour == 0 and now.minute == 0:\n",
        "        return True\n",
        "    elif interval == \"1wk\" and now.weekday() == 0 and now.hour == 0 and now.minute == 0:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Função de agendamento para cada timeframe\n",
        "def agendar_analise_timeframe(tf_config):\n",
        "    interval = tf_config[\"interval\"]\n",
        "    ultimo_print = datetime.now(BR_TZ)\n",
        "\n",
        "    while True:\n",
        "        now = datetime.now(BR_TZ)\n",
        "\n",
        "        if is_time_to_run(interval):\n",
        "            print(f\"\\n🚀 [{interval}] Rodando análise às {now.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "            try:\n",
        "                run_analysis(\n",
        "                    selected_timeframes=[tf_config],\n",
        "                    plot_timeframes=[\"1h\"],\n",
        "                    alert_timeframes=[\"15m\", \"1h\", \"1d\", \"1wk\"],\n",
        "                    retrain_models=False\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Erro durante análise de {interval}: {e}\")\n",
        "            time.sleep(60)  # Aguarda 1 minuto após rodar\n",
        "        else:\n",
        "            if (now - ultimo_print).total_seconds() > 300:\n",
        "                print(f\"⏳ [{interval}] Aguardando próxima execução... {now.strftime('%H:%M:%S')}\")\n",
        "                ultimo_print = now\n",
        "            time.sleep(30)\n",
        "\n",
        "# 🔥 Proteção para iniciar threads apenas uma vez\n",
        "if \"threads_iniciadas\" not in globals():\n",
        "    print(\"🧵 Iniciando threads para execução contínua (modo 24/7)...\")\n",
        "\n",
        "    threads = []\n",
        "    for tf_config in TIMEFRAMES:\n",
        "        t = threading.Thread(target=agendar_analise_timeframe, args=(tf_config,), daemon=True)\n",
        "        t.start()\n",
        "        threads.append(t)\n",
        "\n",
        "    threads_iniciadas = True\n",
        "\n",
        "    print(\"✅ Threads iniciadas com sucesso. Sistema aguardando próximos horários de execução...\")\n",
        "    # ⏳ Mantém o programa vivo mesmo depois de iniciar as threads\n",
        "    while True:\n",
        "        time.sleep(60)\n",
        "\n",
        "else:\n",
        "    print(\"⚠️ Threads já estavam iniciadas — sistema aguardando próximas execuções...\")\n",
        "    # ⏳ Também mantém o programa vivo aqui\n",
        "    while True:\n",
        "        time.sleep(60)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FRgd1-k06BB",
        "outputId": "58c70fe2-ad93-4beb-9a51-8ba238c8ff09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Threads já estavam iniciadas — sistema aguardando próximas execuções...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Escolha o timeframe que você quer simular\n",
        "timeframe = \"15m\"  # Pode ser \"15m\", \"1h\", \"1d\", etc.\n",
        "\n",
        "# 2️⃣ Escolha o ativo que você quer simular\n",
        "asset = \"BTC-USD\"\n",
        "\n",
        "# 3️⃣ Baixar os candles correspondentes ao timeframe\n",
        "if timeframe in [\"1d\", \"1wk\"]:\n",
        "    # Para diário ou semanal, usar start/end para pegar mais dados\n",
        "    start_date = \"2015-01-01\"\n",
        "    end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    df_candles = yf.download(asset, start=start_date, end=end_date, interval=timeframe, progress=False, auto_adjust=False)\n",
        "else:\n",
        "    # Para 15m e 1h, usar period\n",
        "    df_candles = yf.download(asset, period=\"60d\", interval=timeframe, progress=False, auto_adjust=False)\n",
        "\n",
        "# 4️⃣ Corrigir colunas, se necessário\n",
        "if isinstance(df_candles.columns, pd.MultiIndex):\n",
        "    df_candles.columns = df_candles.columns.get_level_values(0)\n",
        "df_candles.columns = [col.split()[-1] if \" \" in col else col for col in df_candles.columns]\n",
        "df_candles = df_candles.loc[:, ~df_candles.columns.duplicated()]\n",
        "col_map = {col: std_col for col in df_candles.columns for std_col in [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"] if std_col.lower() in col.lower()}\n",
        "df_candles = df_candles.rename(columns=col_map)\n",
        "df_candles = df_candles[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
        "\n",
        "# 5️⃣ Agora simular todos os trades com a nova lógica de compra no Low e venda no High\n",
        "simular_todos_trades(prediction_log_path=\"/content/prediction_log.csv\", df_candles=df_candles, timeframe=timeframe)\n",
        "\n",
        "# 6️⃣ Depois de simular, você verá o gráfico de lucro e a carteira atualizada\n"
      ],
      "metadata": {
        "id": "ekhEltOT71bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📊 Avaliação dos desvios das previsões LSTM passadas\n",
        "from datetime import timedelta\n",
        "\n",
        "def evaluate_past_predictions(results_file=\"/content/prediction_log.csv\", lookahead_candles=5):\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    import yfinance as yf\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    if not os.path.exists(results_file):\n",
        "        print(\"📭 Nenhum log de previsão encontrado.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(results_file)\n",
        "    if df.empty or \"Asset\" not in df.columns:\n",
        "        print(\"⚠️ Log vazio ou inválido.\")\n",
        "        return\n",
        "\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "    print(f\"📊 Avaliando {len(df)} previsões registradas...\")\n",
        "\n",
        "    evaluation = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        asset = row[\"Asset\"]\n",
        "        timeframe = row[\"Timeframe\"]\n",
        "        prediction_time = row[\"Date\"]\n",
        "        predicted_target = row.get(\"TargetPrice\", None)\n",
        "\n",
        "        if pd.isna(predicted_target):\n",
        "            continue\n",
        "\n",
        "        # Define intervalo do Yahoo com base no timeframe\n",
        "        if timeframe == \"15m\":\n",
        "            intervalo = \"15m\"\n",
        "        elif timeframe == \"1h\":\n",
        "            intervalo = \"60m\"\n",
        "        else:\n",
        "            continue  # só 15m e 1h por enquanto\n",
        "\n",
        "        try:\n",
        "            # Coletar candles futuros\n",
        "            df_future = yf.download(asset, start=prediction_time, interval=intervalo, progress=False)\n",
        "            df_future = df_future[df_future.index > prediction_time]\n",
        "\n",
        "            if df_future.empty or len(df_future) < lookahead_candles:\n",
        "                continue\n",
        "\n",
        "            df_future = df_future.head(lookahead_candles)\n",
        "            preco_real = df_future[\"Close\"].iloc[-1]\n",
        "\n",
        "            erro = preco_real - predicted_target\n",
        "            perc_erro = (erro / predicted_target) * 100\n",
        "\n",
        "            evaluation.append({\n",
        "                \"Ativo\": asset,\n",
        "                \"Timeframe\": timeframe,\n",
        "                \"Data Previsão\": prediction_time.strftime(\"%Y-%m-%d %H:%M\"),\n",
        "                \"Preço Previsto\": round(predicted_target, 2),\n",
        "                \"Preço Real\": round(preco_real, 2),\n",
        "                \"Erro Absoluto ($)\": round(erro, 2),\n",
        "                \"Erro Percentual (%)\": round(perc_erro, 2)\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erro ao avaliar {asset} ({timeframe}) em {prediction_time}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Criar DataFrame final\n",
        "    df_eval = pd.DataFrame(evaluation)\n",
        "\n",
        "    if df_eval.empty:\n",
        "        print(\"📭 Nenhuma avaliação foi possível ainda (precisa de mais candles futuros).\")\n",
        "        return\n",
        "\n",
        "    # Exibir resultados\n",
        "    print(\"\\n📈 Resultados de Desvio das Previsões:\")\n",
        "    display(df_eval)\n",
        "\n",
        "    # Mostrar médias\n",
        "    print(\"\\n📊 Desvios Médios:\")\n",
        "    print(df_eval.groupby(\"Timeframe\").agg({\n",
        "        \"Erro Absoluto ($)\": \"mean\",\n",
        "        \"Erro Percentual (%)\": \"mean\"\n",
        "    }).round(2))\n",
        "\n",
        "    # Gráfico de Erro Absoluto\n",
        "    df_eval.groupby(\"Timeframe\")[\"Erro Absoluto ($)\"].mean().plot(kind=\"barh\", title=\"📈 Erro Absoluto Médio por Timeframe\", xlabel=\"Erro ($)\", ylabel=\"Timeframe\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Gráfico de Erro Percentual\n",
        "    df_eval.groupby(\"Timeframe\")[\"Erro Percentual (%)\"].mean().plot(kind=\"barh\", title=\"📉 Erro Percentual Médio por Timeframe\", xlabel=\"Erro (%)\", ylabel=\"Timeframe\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# 🚀 Rodar avaliação\n",
        "evaluate_past_predictions()\n"
      ],
      "metadata": {
        "id": "s3YJvyy3Ct3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Escolha o timeframe que você quer simular\n",
        "timeframe = \"15m\"  # Pode ser \"15m\", \"1h\", \"1d\", etc.\n",
        "\n",
        "# 2️⃣ Escolha o ativo que você quer simular\n",
        "asset = \"BTC-USD\"\n",
        "\n",
        "# 3️⃣ Baixar os candles correspondentes ao timeframe\n",
        "if timeframe in [\"1d\", \"1wk\"]:\n",
        "    # Para diário ou semanal, usar start/end para pegar mais dados\n",
        "    start_date = \"2015-01-01\"\n",
        "    end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    df_candles = yf.download(asset, start=start_date, end=end_date, interval=timeframe, progress=False, auto_adjust=False)\n",
        "else:\n",
        "    # Para 15m e 1h, usar period\n",
        "    df_candles = yf.download(asset, period=\"60d\", interval=timeframe, progress=False, auto_adjust=False)\n",
        "\n",
        "# 4️⃣ Corrigir colunas, se necessário\n",
        "if isinstance(df_candles.columns, pd.MultiIndex):\n",
        "    df_candles.columns = df_candles.columns.get_level_values(0)\n",
        "df_candles.columns = [col.split()[-1] if \" \" in col else col for col in df_candles.columns]\n",
        "df_candles = df_candles.loc[:, ~df_candles.columns.duplicated()]\n",
        "col_map = {col: std_col for col in df_candles.columns for std_col in [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"] if std_col.lower() in col.lower()}\n",
        "df_candles = df_candles.rename(columns=col_map)\n",
        "df_candles = df_candles[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
        "\n",
        "# 5️⃣ Agora simular todos os trades com a nova lógica de compra no Low e venda no High\n",
        "simular_todos_trades(prediction_log_path=\"/content/prediction_log.csv\", df_candles=df_candles, timeframe=timeframe)\n",
        "\n",
        "# 6️⃣ Depois de simular, você verá o gráfico de lucro e a carteira atualizada\n"
      ],
      "metadata": {
        "id": "049npxu9CtB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 📚 Imports adicionais\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import os\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# 🌎 Timezone Brasil\n",
        "BR_TZ = pytz.timezone(\"America/Sao_Paulo\")\n",
        "\n",
        "# 📂 Pasta dos modelos salvos\n",
        "MODELS_DIR = \"/content/models\"\n",
        "\n",
        "# 📄 Função para carregar modelo e metadados\n",
        "def carregar_lstm_completo(asset, interval):\n",
        "    asset_clean = asset.replace(\"-\", \"\")\n",
        "    path_h5 = f\"{MODELS_DIR}/lstm_model_{asset_clean}_{interval}.h5\"\n",
        "    path_pkl = f\"{MODELS_DIR}/lstm_model_{asset_clean}_{interval}_meta.pkl\"\n",
        "\n",
        "    if not os.path.exists(path_h5) or not os.path.exists(path_pkl):\n",
        "        return None\n",
        "\n",
        "    model = load_model(path_h5, compile=False)\n",
        "    meta = joblib.load(path_pkl)\n",
        "\n",
        "    model.scaler_x = meta.get(\"scaler_x\")\n",
        "    model.scaler_y = meta.get(\"scaler_y\")\n",
        "    model.feature_cols = meta.get(\"feature_cols\")\n",
        "    model.target_cols = meta.get(\"target_cols\", [\"High\", \"Low\", \"Close\"])\n",
        "    model.window_size = meta.get(\"window_size\", 20)\n",
        "\n",
        "    return model\n",
        "\n",
        "# 📈 Avaliador\n",
        "def avaliar_modelo(asset, interval, periodo=\"60d\", verbose=True):\n",
        "    try:\n",
        "        model = carregar_lstm_completo(asset, interval)\n",
        "        if model is None:\n",
        "            if verbose: print(f\"⚠️ Modelo não encontrado para {asset} ({interval})\")\n",
        "            return None\n",
        "\n",
        "        df = yf.download(asset, period=periodo, interval=interval, progress=False)\n",
        "        if df.empty or len(df) < model.window_size + 1:\n",
        "            if verbose: print(f\"⚠️ Dados insuficientes para {asset} ({interval})\")\n",
        "            return None\n",
        "\n",
        "        # Corrigir colunas\n",
        "        if isinstance(df.columns, pd.MultiIndex):\n",
        "            df.columns = df.columns.get_level_values(0)\n",
        "        df = df.rename(columns=lambda x: x.split()[-1] if \" \" in x else x)\n",
        "        df = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].dropna()\n",
        "\n",
        "        # Aplicar indicadores\n",
        "        df = calculate_indicators(df)\n",
        "\n",
        "        # Preparar dados\n",
        "        from sklearn.preprocessing import MinMaxScaler\n",
        "        feature_cols = model.feature_cols\n",
        "        target_cols = model.target_cols\n",
        "\n",
        "        X_list, y_list = [], []\n",
        "        scaler_x = model.scaler_x\n",
        "        scaler_y = model.scaler_y\n",
        "\n",
        "        df_features = df[feature_cols].dropna()\n",
        "        df_targets = df[target_cols].dropna()\n",
        "        min_len = min(len(df_features), len(df_targets))\n",
        "\n",
        "        if min_len <= model.window_size:\n",
        "            if verbose: print(f\"⚠️ Dados insuficientes após indicadores para {asset} ({interval})\")\n",
        "            return None\n",
        "\n",
        "        scaled_X = scaler_x.transform(df_features.values)\n",
        "        scaled_y = scaler_y.transform(df_targets.values)\n",
        "\n",
        "        for i in range(model.window_size, min_len):\n",
        "            X_list.append(scaled_X[i-model.window_size:i])\n",
        "            y_list.append(scaled_y[i])\n",
        "\n",
        "        X_array = np.array(X_list)\n",
        "        y_array = np.array(y_list)\n",
        "\n",
        "        preds_scaled = model.predict(X_array, verbose=0)\n",
        "        preds = scaler_y.inverse_transform(preds_scaled)\n",
        "        y_true = scaler_y.inverse_transform(y_array)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for idx, col in enumerate([\"High\", \"Low\", \"Close\"]):\n",
        "            y_true_col = y_true[:, idx]\n",
        "            preds_col = preds[:, idx]\n",
        "\n",
        "            r2 = r2_score(y_true_col, preds_col)\n",
        "            mae = mean_absolute_error(y_true_col, preds_col)\n",
        "            rmse = np.sqrt(mean_squared_error(y_true_col, preds_col))\n",
        "            mape = np.mean(np.abs((y_true_col - preds_col) / y_true_col)) * 100\n",
        "\n",
        "            results[f\"R2_{col}\"] = round(r2, 4)\n",
        "            results[f\"MAE_{col}\"] = round(mae, 4)\n",
        "            results[f\"RMSE_{col}\"] = round(rmse, 4)\n",
        "            results[f\"MAPE_{col}\"] = round(mape, 2)\n",
        "\n",
        "        return {\n",
        "            \"Asset\": asset,\n",
        "            \"Timeframe\": interval,\n",
        "            **results\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao avaliar {asset} ({interval}): {e}\")\n",
        "        return None\n",
        "\n",
        "# 🚀 Avaliar todos\n",
        "def avaliar_todos(assets=None, timeframes=None, periodo=\"60d\", salvar_csv=True):\n",
        "    if assets is None:\n",
        "        assets = [\"BTC-USD\", \"ETH-USD\", \"SOL-USD\", \"XRP-USD\", \"AVAX-USD\", \"AAVE-USD\", \"DOT-USD\", \"NEAR-USD\", \"ADA-USD\", \"VIRTUAL-USD\", \"PENDLE-USD\"]\n",
        "    if timeframes is None:\n",
        "        timeframes = [\"15m\", \"1h\", \"1d\", \"1wk\"]\n",
        "\n",
        "    resultados = []\n",
        "\n",
        "    for asset in assets:\n",
        "        for tf in timeframes:\n",
        "            r = avaliar_modelo(asset, tf, periodo)\n",
        "            if r is not None:\n",
        "                resultados.append(r)\n",
        "\n",
        "    df_avaliacao = pd.DataFrame(resultados)\n",
        "\n",
        "    print(\"\\n📋 Avaliação de Performance dos Modelos:\\n\")\n",
        "    display(df_avaliacao)\n",
        "\n",
        "    if salvar_csv:\n",
        "        df_avaliacao.to_csv(\"/content/model_evaluation.csv\", index=False)\n",
        "        print(\"\\n✅ Resultados salvos em: /content/model_evaluation.csv\")\n",
        "\n",
        "    return df_avaliacao\n",
        "\n",
        "# Rodar Avaliação\n",
        "avaliar_todos()"
      ],
      "metadata": {
        "id": "HyTIVPYS9FrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📚 Primeiro: carregar os dados de 1d completos\n",
        "asset = \"BTC-USD\"\n",
        "interval = \"1d\"\n",
        "start_date = \"2015-01-01\"\n",
        "end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Usa o get_stock_data com start/end\n",
        "df = yf.download(asset, start=start_date, end=end_date, interval=interval, progress=False, auto_adjust=False)\n",
        "\n",
        "# Corrige colunas se necessário\n",
        "if isinstance(df.columns, pd.MultiIndex):\n",
        "    df.columns = df.columns.get_level_values(0)\n",
        "df.columns = [col.split()[-1] if \" \" in col else col for col in df.columns]\n",
        "df = df.loc[:, ~df.columns.duplicated()]\n",
        "col_map = {col: std_col for col in df.columns for std_col in [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"] if std_col.lower() in col.lower()}\n",
        "df = df.rename(columns=col_map)\n",
        "df = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
        "\n",
        "# Calcula os indicadores técnicos\n",
        "df = calculate_indicators(df)\n",
        "\n",
        "# 📈 Agora: treina o modelo LSTM Diário especial\n",
        "model = train_lstm_model_diario(\n",
        "    df,\n",
        "    asset=asset,\n",
        "    interval=interval,\n",
        "    window_size=60,\n",
        "    force_retrain=True  # 👈 força recriar mesmo se já tiver salvo\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "N7hasnIPneQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def exibir_carteira_virtual(filepath=\"/content/carteira_virtual.json\"):\n",
        "    try:\n",
        "        with open(filepath, \"r\") as f:\n",
        "            carteira = json.load(f)\n",
        "\n",
        "        print(\"💼 Carteira Virtual:\")\n",
        "        for chave, valor in carteira.items():\n",
        "            if isinstance(valor, float):\n",
        "                print(f\"• {chave}: {valor:.2f}\")\n",
        "            else:\n",
        "                print(f\"• {chave}: {valor}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao exibir carteira: {e}\")\n",
        "\n",
        "exibir_carteira_virtual()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvdwA0p13GjX",
        "outputId": "8b2cbcb6-a49e-4545-b900-51612ebb4752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💼 Carteira Virtual:\n",
            "• capital_inicial: 10000.00\n",
            "• capital_atual: 10000.00\n",
            "• capital_maximo: 10000.00\n",
            "• historico_capital: []\n",
            "• em_operacao: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exibir_prediction_log()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qidyHubD3Ybb",
        "outputId": "a121b961-9a62-41b5-a350-d2caa66e5aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📭 O prediction_log está vazio.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_analysis(\n",
        "    selected_timeframes=[\n",
        "        {\"interval\": \"1h\", \"period\": \"120d\", \"atr\": 0.03}\n",
        "    ],\n",
        "    plot_timeframes=[\"1h\"],\n",
        "    alert_timeframes=[\"1h\"],\n",
        "    retrain_models=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "3lZAmWtI3rf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_models(\"/content/models\")\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def limpar_model_results():\n",
        "    arquivos = glob.glob(\"/content/model_results_*.csv\")\n",
        "    if not arquivos:\n",
        "        print(\"📂 Nenhum arquivo model_results_*.csv encontrado.\")\n",
        "        return\n",
        "\n",
        "    for arquivo in arquivos:\n",
        "        try:\n",
        "            os.remove(arquivo)\n",
        "            print(f\"🧹 Arquivo deletado: {arquivo}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erro ao deletar {arquivo}: {e}\")\n",
        "\n",
        "    print(\"✅ Todos os arquivos model_results_*.csv foram removidos.\")\n",
        "\n",
        "limpar_model_results()\n",
        "def limpar_prediction_log(path=\"prediction_log.csv\"):\n",
        "    if not os.path.exists(path):\n",
        "        print(\"⚠️ Arquivo de log não encontrado.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.dropna(subset=[\"Date\"])\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "    df = df[df[\"Date\"].dt.year >= 2000]\n",
        "    df.to_csv(path, index=False)\n",
        "    print(\"✅ Log limpo com sucesso. Entradas de 1970 removidas!\")\n",
        "\n",
        "limpar_prediction_log()\n",
        "import os\n",
        "os.remove(\"/content/prediction_log.csv\")\n",
        "import os\n",
        "if os.path.exists(\"prediction_log.csv\"):\n",
        "    os.remove(\"prediction_log.csv\")\n",
        "    print(\"🧹 Carteira virtual resetada com sucesso.\")"
      ],
      "metadata": {
        "id": "ctphA5Oq6AQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f /content/prediction_log.csv\n",
        "!rm -f /content/model_results_*.csv\n"
      ],
      "metadata": {
        "id": "-TAnU-WJ730p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simular_todos_trades(\n",
        "    prediction_log_path=\"/content/prediction_log.csv\",\n",
        "    df_candles=None,\n",
        "    timeframe=\"1h\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "IXxxZItbV0Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para treinar e comparar splits para BTC-USD no 1h\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "def train_lstm_btc_1h(df):\n",
        "    feature_cols = get_lstm_feature_columns()\n",
        "    target_cols = [\"High\", \"Low\", \"Close\"]\n",
        "\n",
        "    df = df.dropna(subset=feature_cols + target_cols).copy()\n",
        "\n",
        "    if len(df) <= 30:\n",
        "        print(\"⚠️ Dados insuficientes para treino.\")\n",
        "        return None\n",
        "\n",
        "    # Define splits\n",
        "    splits = {\n",
        "        \"70-30\": int(0.7 * len(df)),\n",
        "        \"80-20\": int(0.8 * len(df)),\n",
        "        \"85-15\": int(0.85 * len(df))\n",
        "    }\n",
        "\n",
        "    resultados = []\n",
        "\n",
        "    for nome, split_idx in splits.items():\n",
        "        df_train = df.iloc[:split_idx]\n",
        "        df_test = df.iloc[split_idx:]\n",
        "\n",
        "        scaler_x = MinMaxScaler()\n",
        "        scaler_y = MinMaxScaler()\n",
        "\n",
        "        X_train_scaled = scaler_x.fit_transform(df_train[feature_cols])\n",
        "        y_train_scaled = scaler_y.fit_transform(df_train[target_cols])\n",
        "\n",
        "        X_test_scaled = scaler_x.transform(df_test[feature_cols])\n",
        "        y_test_scaled = scaler_y.transform(df_test[target_cols])\n",
        "\n",
        "        window_size = 15\n",
        "\n",
        "        # Prepara sequências\n",
        "        def create_sequences(X, y, window_size):\n",
        "            X_seq, y_seq = [], []\n",
        "            for i in range(window_size, len(X)):\n",
        "                X_seq.append(X[i-window_size:i])\n",
        "                y_seq.append(y[i])\n",
        "            return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "        X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, window_size)\n",
        "        X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, window_size)\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(64, input_shape=(window_size, len(feature_cols))))\n",
        "        model.add(Dense(3))\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "        es = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "        rlrop = ReduceLROnPlateau(patience=5, factor=0.5, verbose=1)\n",
        "\n",
        "        model.fit(X_train_seq, y_train_seq, validation_data=(X_test_seq, y_test_seq),\n",
        "                  epochs=150, batch_size=32, verbose=0, callbacks=[es, rlrop])\n",
        "\n",
        "        y_pred_scaled = model.predict(X_test_seq)\n",
        "        y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "        y_true = scaler_y.inverse_transform(y_test_seq)\n",
        "\n",
        "        r2_close = r2_score(y_true[:,2], y_pred[:,2])\n",
        "        mae_close = mean_absolute_error(y_true[:,2], y_pred[:,2])\n",
        "        rmse_close = mean_squared_error(y_true[:,2], y_pred[:,2], squared=False)\n",
        "\n",
        "        resultados.append({\n",
        "            \"Split\": nome,\n",
        "            \"R2_Close\": r2_close,\n",
        "            \"MAE_Close\": mae_close,\n",
        "            \"RMSE_Close\": rmse_close,\n",
        "            \"model\": model,\n",
        "            \"scaler_x\": scaler_x,\n",
        "            \"scaler_y\": scaler_y,\n",
        "            \"feature_cols\": feature_cols,\n",
        "            \"target_cols\": target_cols,\n",
        "            \"window_size\": window_size\n",
        "        })\n",
        "\n",
        "    df_resultados = pd.DataFrame([{k: v for k, v in r.items() if k != \"model\"} for r in resultados])\n",
        "    df_resultados.to_csv(\"/content/comparativo_splits_BTCUSD_1h.csv\", index=False)\n",
        "    print(\"\\n📋 Comparativo de splits salvo em /content/comparativo_splits_BTCUSD_1h.csv\")\n",
        "\n",
        "    # Escolhe o melhor\n",
        "    melhor = max(resultados, key=lambda x: x[\"R2_Close\"])\n",
        "\n",
        "    # Salva o melhor modelo\n",
        "    path_model = \"/content/models/lstm_model_BTCUSD_1h.h5\"\n",
        "    melhor[\"model\"].save(path_model)\n",
        "    joblib.dump({\n",
        "        \"scaler_x\": melhor[\"scaler_x\"],\n",
        "        \"scaler_y\": melhor[\"scaler_y\"],\n",
        "        \"feature_cols\": melhor[\"feature_cols\"],\n",
        "        \"target_cols\": melhor[\"target_cols\"],\n",
        "        \"window_size\": melhor[\"window_size\"]\n",
        "    }, path_model.replace(\".h5\", \"_meta.pkl\"))\n",
        "\n",
        "    print(f\"\\n✅ Melhor split: {melhor['Split']} salvo como modelo definitivo!\")\n",
        "    print(f\"🏆 R2_Close: {melhor['R2_Close']:.4f} | MAE: {melhor['MAE_Close']:.2f} | RMSE: {melhor['RMSE_Close']:.2f}\")\n",
        "    return df_resultados"
      ],
      "metadata": {
        "id": "9ZcMUiYFGIQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IWVkUU4TJbw2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}