{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laribar/SmartAITraderBot/blob/main/Vers%C3%A3o_Treino_80_20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üßπ Resetar ambiente\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "# ‚úÖ Reinstalar corretamente\n",
        "!pip install numpy==1.26.4\n",
        "!pip install ta yfinance python-binance xgboost==2.0.3"
      ],
      "metadata": {
        "id": "8S0MHRqVG9Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "sbFtz1rkKdQJ",
        "outputId": "4c6ce68b-4b13-4e2a-e5c9-9683c8f0668b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S7AQQF5BKcd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE tudo que estiver bugado\n",
        "!pip uninstall -y numpy pandas scipy scikit-learn tensorflow keras xgboost\n",
        "\n",
        "# Agora instala as vers√µes corretas\n",
        "!pip install numpy==1.26.4 pandas==2.2.2 scipy==1.12.0 scikit-learn==1.4.2 tensorflow==2.18.0 keras==2.18.0 xgboost==2.0.3 ta yfinance python-binance\n"
      ],
      "metadata": {
        "id": "qwU99PKfIsOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzDCUpKv0IhK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "155d6224-bf2a-4ebe-dbb7-cdd2f7c0a243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting ta\n",
            "  Using cached ta-0.11.0-py3-none-any.whl\n",
            "Collecting yfinance\n",
            "  Using cached yfinance-0.2.57-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting python-binance\n",
            "  Using cached python_binance-1.0.28-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting numpy (from ta)\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting pandas (from ta)\n",
            "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting requests>=2.31 (from yfinance)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting multitasking>=0.0.7 (from yfinance)\n",
            "  Using cached multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting platformdirs>=2.0.0 (from yfinance)\n",
            "  Using cached platformdirs-4.3.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pytz>=2022.5 (from yfinance)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting frozendict>=2.3.4 (from yfinance)\n",
            "  Using cached frozendict-2.4.6-py311-none-any.whl.metadata (23 kB)\n",
            "Collecting peewee>=3.16.2 (from yfinance)\n",
            "  Using cached peewee-3.17.9-cp311-cp311-linux_x86_64.whl\n",
            "Collecting beautifulsoup4>=4.11.1 (from yfinance)\n",
            "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting six (from python-binance)\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting dateparser (from python-binance)\n",
            "  Using cached dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting aiohttp (from python-binance)\n",
            "  Using cached aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting websockets (from python-binance)\n",
            "  Using cached websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting pycryptodome (from python-binance)\n",
            "  Using cached pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4>=4.11.1->yfinance)\n",
            "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typing-extensions>=4.0.0 (from beautifulsoup4>=4.11.1->yfinance)\n",
            "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas->ta)\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->ta)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests>=2.31->yfinance)\n",
            "  Using cached charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests>=2.31->yfinance)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.31->yfinance)\n",
            "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests>=2.31->yfinance)\n",
            "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->python-binance)\n",
            "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->python-binance)\n",
            "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->python-binance)\n",
            "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->python-binance)\n",
            "  Using cached frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->python-binance)\n",
            "  Using cached multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->python-binance)\n",
            "  Using cached propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->python-binance)\n",
            "  Using cached yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
            "Collecting regex!=2019.02.19,!=2021.8.27,>=2015.06.24 (from dateparser->python-binance)\n",
            "  Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting tzlocal>=0.2 (from dateparser->python-binance)\n",
            "  Using cached tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Using cached yfinance-0.2.57-py2.py3-none-any.whl (113 kB)\n",
            "Using cached python_binance-1.0.28-py2.py3-none-any.whl (130 kB)\n",
            "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
            "Using cached frozendict-2.4.6-py311-none-any.whl (16 kB)\n",
            "Using cached multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "Using cached platformdirs-4.3.7-py3-none-any.whl (18 kB)\n",
            "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "Using cached dateparser-1.2.1-py3-none-any.whl (295 kB)\n",
            "Using cached pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Using cached websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
            "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "Using cached charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
            "Using cached frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "Using cached propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
            "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
            "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Using cached tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
            "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Using cached yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
            "Installing collected packages: pytz, peewee, multitasking, websockets, urllib3, tzlocal, tzdata, typing-extensions, soupsieve, six, regex, pycryptodome, propcache, platformdirs, numpy, multidict, idna, frozenlist, frozendict, charset-normalizer, certifi, attrs, aiohappyeyeballs, yarl, requests, python-dateutil, beautifulsoup4, aiosignal, pandas, dateparser, aiohttp, yfinance, ta, python-binance\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: peewee\n",
            "    Found existing installation: peewee 3.17.9\n",
            "    Uninstalling peewee-3.17.9:\n",
            "      Successfully uninstalled peewee-3.17.9\n",
            "  Attempting uninstall: multitasking\n",
            "    Found existing installation: multitasking 0.0.11\n",
            "    Uninstalling multitasking-0.0.11:\n",
            "      Successfully uninstalled multitasking-0.0.11\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 5.3.1\n",
            "    Uninstalling tzlocal-5.3.1:\n",
            "      Successfully uninstalled tzlocal-5.3.1\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: soupsieve\n",
            "    Found existing installation: soupsieve 2.7\n",
            "    Uninstalling soupsieve-2.7:\n",
            "      Successfully uninstalled soupsieve-2.7\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: pycryptodome\n",
            "    Found existing installation: pycryptodome 3.22.0\n",
            "    Uninstalling pycryptodome-3.22.0:\n",
            "      Successfully uninstalled pycryptodome-3.22.0\n",
            "  Attempting uninstall: propcache\n",
            "    Found existing installation: propcache 0.3.1\n",
            "    Uninstalling propcache-0.3.1:\n",
            "      Successfully uninstalled propcache-0.3.1\n",
            "  Attempting uninstall: platformdirs\n",
            "    Found existing installation: platformdirs 4.3.7\n",
            "    Uninstalling platformdirs-4.3.7:\n",
            "      Successfully uninstalled platformdirs-4.3.7\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: multidict\n",
            "    Found existing installation: multidict 6.4.3\n",
            "    Uninstalling multidict-6.4.3:\n",
            "      Successfully uninstalled multidict-6.4.3\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: frozenlist\n",
            "    Found existing installation: frozenlist 1.6.0\n",
            "    Uninstalling frozenlist-1.6.0:\n",
            "      Successfully uninstalled frozenlist-1.6.0\n",
            "  Attempting uninstall: frozendict\n",
            "    Found existing installation: frozendict 2.4.6\n",
            "    Uninstalling frozendict-2.4.6:\n",
            "      Successfully uninstalled frozendict-2.4.6\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.1\n",
            "    Uninstalling charset-normalizer-3.4.1:\n",
            "      Successfully uninstalled charset-normalizer-3.4.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.4.26\n",
            "    Uninstalling certifi-2025.4.26:\n",
            "      Successfully uninstalled certifi-2025.4.26\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.3.0\n",
            "    Uninstalling attrs-25.3.0:\n",
            "      Successfully uninstalled attrs-25.3.0\n",
            "  Attempting uninstall: aiohappyeyeballs\n",
            "    Found existing installation: aiohappyeyeballs 2.6.1\n",
            "    Uninstalling aiohappyeyeballs-2.6.1:\n",
            "      Successfully uninstalled aiohappyeyeballs-2.6.1\n",
            "  Attempting uninstall: yarl\n",
            "    Found existing installation: yarl 1.20.0\n",
            "    Uninstalling yarl-1.20.0:\n",
            "      Successfully uninstalled yarl-1.20.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.13.4\n",
            "    Uninstalling beautifulsoup4-4.13.4:\n",
            "      Successfully uninstalled beautifulsoup4-4.13.4\n",
            "  Attempting uninstall: aiosignal\n",
            "    Found existing installation: aiosignal 1.3.2\n",
            "    Uninstalling aiosignal-1.3.2:\n",
            "      Successfully uninstalled aiosignal-1.3.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.3\n",
            "    Uninstalling pandas-2.2.3:\n",
            "      Successfully uninstalled pandas-2.2.3\n",
            "  Attempting uninstall: dateparser\n",
            "    Found existing installation: dateparser 1.2.1\n",
            "    Uninstalling dateparser-1.2.1:\n",
            "      Successfully uninstalled dateparser-1.2.1\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.11.18\n",
            "    Uninstalling aiohttp-3.11.18:\n",
            "      Successfully uninstalled aiohttp-3.11.18\n",
            "  Attempting uninstall: yfinance\n",
            "    Found existing installation: yfinance 0.2.57\n",
            "    Uninstalling yfinance-0.2.57:\n",
            "      Successfully uninstalled yfinance-0.2.57\n",
            "  Attempting uninstall: ta\n",
            "    Found existing installation: ta 0.11.0\n",
            "    Uninstalling ta-0.11.0:\n",
            "      Successfully uninstalled ta-0.11.0\n",
            "  Attempting uninstall: python-binance\n",
            "    Found existing installation: python-binance 1.0.28\n",
            "    Uninstalling python-binance-1.0.28:\n",
            "      Successfully uninstalled python-binance-1.0.28\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\n",
            "scipy 1.12.0 requires numpy<1.29.0,>=1.22.4, but you have numpy 2.2.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 attrs-25.3.0 beautifulsoup4-4.13.4 certifi-2025.4.26 charset-normalizer-3.4.1 dateparser-1.2.1 frozendict-2.4.6 frozenlist-1.6.0 idna-3.10 multidict-6.4.3 multitasking-0.0.11 numpy-2.2.5 pandas-2.2.3 peewee-3.17.9 platformdirs-4.3.7 propcache-0.3.1 pycryptodome-3.22.0 python-binance-1.0.28 python-dateutil-2.9.0.post0 pytz-2025.2 regex-2024.11.6 requests-2.32.3 six-1.17.0 soupsieve-2.7 ta-0.11.0 typing-extensions-4.13.2 tzdata-2025.2 tzlocal-5.3.1 urllib3-2.4.0 websockets-15.0.1 yarl-1.20.0 yfinance-0.2.57\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "charset_normalizer",
                  "dateutil",
                  "frozendict",
                  "multitasking",
                  "peewee",
                  "pytz",
                  "requests",
                  "six",
                  "ta",
                  "yfinance"
                ]
              },
              "id": "0dd63e644924401ebb317360638f4834"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost==2.0.3\n",
            "  Using cached xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting numpy (from xgboost==2.0.3)\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting scipy (from xgboost==2.0.3)\n",
            "  Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "Installing collected packages: numpy, scipy, xgboost\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.12.0\n",
            "    Uninstalling scipy-1.12.0:\n",
            "      Successfully uninstalled scipy-1.12.0\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.0.3\n",
            "    Uninstalling xgboost-2.0.3:\n",
            "      Successfully uninstalled xgboost-2.0.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5 scipy-1.15.2 xgboost-2.0.3\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Collecting scipy==1.12.0\n",
            "  Using cached scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting numpy<1.29.0,>=1.22.4 (from scipy==1.12.0)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy, scipy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.2\n",
            "    Uninstalling scipy-1.15.2:\n",
            "      Successfully uninstalled scipy-1.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 scipy-1.12.0\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.12.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow)\n",
            "  Downloading keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-dtypes, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.9.2 ml-dtypes-0.5.1 tensorboard-2.19.0 tensorflow-2.19.0\n"
          ]
        }
      ],
      "source": [
        "# ========================================================\n",
        "# üöÄ Instala ambiente correto para rodar o sistema\n",
        "# ========================================================\n",
        "\n",
        "!pip install numpy==1.26.4\n",
        "\n",
        "# Instala pacotes principais (sem travar em vers√µes antigas quebradas)\n",
        "!pip install --upgrade --force-reinstall ta yfinance python-binance\n",
        "\n",
        "# Corrige vers√µes espec√≠ficas necess√°rias\n",
        "!pip install --upgrade --force-reinstall xgboost==2.0.3\n",
        "!pip install --upgrade --force-reinstall numpy==1.26.4\n",
        "!pip install --upgrade --force-reinstall scipy==1.12.0\n",
        "!pip install scikit-learn\n",
        "!pip install tensorflow\n",
        "!pip install mplfinance\n",
        "# Mostra as vers√µes instaladas para confer√™ncia\n",
        "import xgboost\n",
        "import numpy\n",
        "import scipy\n",
        "print(\"‚úÖ Instala√ß√£o conclu√≠da!\")\n",
        "print(f\"xgboost version: {xgboost.__version__}\")\n",
        "print(f\"numpy version: {numpy.__version__}\")\n",
        "print(f\"scipy version: {scipy.__version__}\")\n",
        "\n",
        "# üîÑ (Opcional) Reinicia o runtime para carregar as libs novas:\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Caminho da pasta onde est√£o seus modelos\n",
        "folder_path = '/content/models'\n",
        "\n",
        "# Nome do arquivo zip que ser√° criado\n",
        "zip_filename = 'models_backup'\n",
        "\n",
        "# Compactar toda a pasta /models em um arquivo .zip\n",
        "shutil.make_archive(zip_filename, 'zip', folder_path)\n",
        "\n",
        "print(f\"‚úÖ Pasta '{folder_path}' compactada como '{zip_filename}.zip'.\")\n",
        "\n",
        "# Fazer download do zip para seu computador\n",
        "files.download(f'{zip_filename}.zip')\n"
      ],
      "metadata": {
        "id": "MHeKuQSrUdTR",
        "outputId": "251cc932-25c6-475c-9fb0-562b8e183093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Pasta '/content/models' compactada como 'models_backup.zip'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cf97cb65-0ce2-4c52-8c93-a240e8f2f432\", \"models_backup.zip\", 1664118)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 1. IMPORTA√á√ïES\n",
        "# ====================================================\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ta\n",
        "import requests\n",
        "import time  # Para usar time.sleep()\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import pytz\n",
        "import glob\n",
        "import json\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import joblib\n",
        "import mplfinance as mpf\n",
        "from xgboost import XGBClassifier, callback\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import classification_report\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "# Fuso hor√°rio do Brasil\n",
        "BR_TZ = pytz.timezone(\"America/Sao_Paulo\")\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# BLOCO 1 - CONFIGURA√á√ÉO DE PASTAS E IMPORTS EXTRA\n",
        "# ====================================================\n",
        "import os\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Criar pasta onde os modelos ser√£o salvos\n",
        "os.makedirs(\"/content/models\", exist_ok=True)\n",
        "# ====================================================\n",
        "# BLOCO 2 - SALVAR E CARREGAR MODELOS TREINADOS\n",
        "# ====================================================\n",
        "def get_model_path(asset, interval, model_type=\"xgb\"):\n",
        "  asset_clean = asset.replace(\"-\", \"\")\n",
        "  ext = \"joblib\" if model_type == \"xgb\" else \"h5\"\n",
        "  return f\"/content/models/{model_type}_model_{asset_clean}_{interval}.{ext}\"\n",
        "\n",
        "# --- XGBoost ---\n",
        "def save_xgb_model(model, asset, interval):\n",
        "  path = get_model_path(asset, interval, model_type=\"xgb\")\n",
        "  joblib.dump(model, path)\n",
        "  print(f\"üíæ Modelo XGBoost salvo em: {path}\")\n",
        "\n",
        "def load_xgb_model(asset, interval):\n",
        "  path = get_model_path(asset, interval, model_type=\"xgb\")\n",
        "  if os.path.exists(path):\n",
        "      print(f\"üìÇ Modelo XGBoost carregado de: {path}\")\n",
        "      return joblib.load(path)\n",
        "  return None\n",
        "\n",
        "# --- LSTM ---\n",
        "def save_lstm_model(model, asset, interval):\n",
        "  path = get_model_path(asset, interval, model_type=\"lstm\")\n",
        "  model.save(path)\n",
        "  print(f\"üíæ Modelo LSTM salvo em: {path}\")\n",
        "\n",
        "  # Salvar metadados no novo formato\n",
        "  meta_path = path.replace(\".h5\", \"_meta.pkl\").replace(\".keras\", \"_meta.pkl\")\n",
        "  joblib.dump({\n",
        "      \"scaler_x\": model.scaler_x,\n",
        "      \"scaler_y\": model.scaler_y,\n",
        "      \"feature_cols\": model.feature_cols,\n",
        "      \"target_cols\": model.target_cols,\n",
        "      \"window_size\": model.window_size\n",
        "  }, meta_path)\n",
        "  print(f\"üì¶ Metadados salvos em: {meta_path}\")\n",
        "\n",
        "\n",
        "\n",
        "def load_lstm_model(asset, interval, window_size=20):\n",
        "  from tensorflow.keras.models import load_model\n",
        "  import joblib\n",
        "  import os\n",
        "\n",
        "  model_path = get_model_path(asset, interval, model_type=\"lstm\")\n",
        "  meta_path = model_path.replace(\".h5\", \"_meta.pkl\").replace(\".keras\", \"_meta.pkl\")\n",
        "\n",
        "  if not os.path.exists(model_path):\n",
        "      print(f\"üö´ Modelo LSTM N√ÉO encontrado em: {model_path}\")\n",
        "      return None\n",
        "\n",
        "  try:\n",
        "      model = load_model(model_path, compile=False)\n",
        "      print(f\"üìÇ Modelo LSTM encontrado em: {model_path}\")\n",
        "  except Exception as e:\n",
        "      print(f\"‚ùå Erro ao carregar modelo LSTM de {model_path}: {e}\")\n",
        "      return None\n",
        "\n",
        "  # Carrega os metadados\n",
        "  if os.path.exists(meta_path):\n",
        "      try:\n",
        "          meta = joblib.load(meta_path)\n",
        "          model.scaler_x = meta.get(\"scaler_x\")\n",
        "          model.scaler_y = meta.get(\"scaler_y\")\n",
        "          model.feature_cols = meta.get(\"feature_cols\")\n",
        "          model.target_cols = meta.get(\"target_cols\", [\"High\", \"Low\", \"Close\"])\n",
        "          model.window_size = meta.get(\"window_size\", window_size)\n",
        "\n",
        "          # ‚úÖ Compatibilidade com c√≥digos antigos\n",
        "          model.scaler = model.scaler_x\n",
        "\n",
        "          print(f\"üì¶ Metadados carregados de: {meta_path}\")\n",
        "      except Exception as e:\n",
        "          print(f\"‚ö†Ô∏è Erro ao carregar metadados de {meta_path}: {e}\")\n",
        "          model.scaler_x = None\n",
        "          model.scaler_y = None\n",
        "          model.scaler = None\n",
        "          model.feature_cols = None\n",
        "          model.target_cols = [\"High\", \"Low\", \"Close\"]\n",
        "          model.window_size = window_size\n",
        "  else:\n",
        "      print(f\"‚ö†Ô∏è Metadados n√£o encontrados em: {meta_path}\")\n",
        "      model.scaler_x = None\n",
        "      model.scaler_y = None\n",
        "      model.scaler = None\n",
        "      model.feature_cols = None\n",
        "      model.target_cols = [\"High\", \"Low\", \"Close\"]\n",
        "      model.window_size = window_size\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 2. CONFIGURA√á√ïES\n",
        "# ====================================================\n",
        "ASSETS = [\"BTC-USD\", \"ETH-USD\", \"SOL-USD\"]# \"XRP-USD\", \"AVAX-USD\", \"AAVE-USD\", \"DOT-USD\", \"NEAR-USD\", \"ADA-USD\", \"VIRTUAL-USD\", \"PENDLE-USD\"]\n",
        "\n",
        "\n",
        "TIMEFRAMES = [\n",
        "  {\"interval\": \"15m\", \"period\": \"30d\", \"atr\": 0.02},\n",
        "  {\"interval\": \"1h\", \"period\": \"90d\", \"atr\": 0.03},\n",
        "  {\"interval\": \"1d\", \"period\": \"1000d\", \"atr\": 0.05},\n",
        "  {\"interval\": \"1wk\", \"period\": \"max\", \"atr\": 0.08}  # üëà Adicionado o semanal\n",
        "]\n",
        "\n",
        "TELEGRAM_TOKEN = \"8142008777:AAHvP5uHzEmQqR4xKyu_bfm0Vf3C8cYbmj0\"\n",
        "TELEGRAM_CHAT_ID = \"-4744645054\"\n",
        "ALERTA_VARIACAO_MINIMA = {\n",
        "  \"15m\": 1.0,\n",
        "  \"1h\": 2.0,\n",
        "  \"1d\": 5.0,\n",
        "  \"1wk\": 5.0\n",
        "}\n",
        "\n",
        "ENVIAR_ALERTAS = False  # ‚úÖ True = enviar alertas / False = n√£o enviar alertas\n",
        "MODO_EXECUCAO_CONTINUA = True  # True = Roda 24/7, False = S√≥ manualmente\n",
        "\n",
        "# ====================================================\n",
        "# 3. COLETA DE DADOS\n",
        "# ====================================================\n",
        "def get_stock_data(asset, interval=\"15m\", period=\"30d\", max_retries=3, sleep_sec=5):\n",
        "    import time\n",
        "    import pandas as pd\n",
        "    import yfinance as yf\n",
        "\n",
        "    # Definir datas espec√≠ficas para timeframes longos\n",
        "    usar_datas = interval in [\"1d\", \"1wk\"]\n",
        "    start_date = \"2015-01-01\"\n",
        "    end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            if usar_datas:\n",
        "                print(f\"üìÖ Usando start/end para {asset} ({interval}): {start_date} ‚ûî {end_date}\")\n",
        "                data = yf.download(asset, start=start_date, end=end_date, interval=interval, progress=False, auto_adjust=False)\n",
        "            else:\n",
        "                print(f\"‚è≥ Usando period para {asset} ({interval}): {period}\")\n",
        "                data = yf.download(asset, period=period, interval=interval, progress=False, auto_adjust=False)\n",
        "\n",
        "            if data.empty:\n",
        "                raise ValueError(f\"‚ö†Ô∏è Dados vazios recebidos de {asset} ({interval})\")\n",
        "\n",
        "            if isinstance(data.columns, pd.MultiIndex):\n",
        "                data.columns = data.columns.get_level_values(0)\n",
        "            data.columns = [col.split()[-1] if \" \" in col else col for col in data.columns]\n",
        "            data = data.loc[:, ~data.columns.duplicated()]\n",
        "            col_map = {col: std_col for col in data.columns for std_col in [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"] if std_col.lower() in col.lower()}\n",
        "            data = data.rename(columns=col_map)\n",
        "            data = data[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
        "            if not all(col in data.columns for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]):\n",
        "                raise ValueError(f\"‚ö†Ô∏è Colunas necess√°rias ausentes em {asset} ({interval})\")\n",
        "\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Falha na tentativa {attempt+1} para {asset} ({interval}): {e}\")\n",
        "            time.sleep(sleep_sec)\n",
        "\n",
        "    raise RuntimeError(f\"‚ùå Falha ao baixar dados de {asset} ({interval}) ap√≥s {max_retries} tentativas.\")\n",
        "\n",
        "\n",
        "\n",
        "def safe_read_csv(filepath):\n",
        "  import os\n",
        "  import pandas as pd\n",
        "\n",
        "  if not os.path.exists(filepath):\n",
        "      print(f\"‚ö†Ô∏è Arquivo n√£o encontrado: {filepath}\")\n",
        "      return None\n",
        "  if os.path.getsize(filepath) == 0:\n",
        "      print(f\"‚ö†Ô∏è Arquivo est√° vazio: {filepath}\")\n",
        "      return None\n",
        "  try:\n",
        "      df = pd.read_csv(filepath)\n",
        "      if df.empty or len(df.columns) == 0:\n",
        "          print(f\"‚ö†Ô∏è Arquivo inv√°lido (sem colunas): {filepath}\")\n",
        "          return None\n",
        "      return df\n",
        "  except pd.errors.EmptyDataError:\n",
        "      print(f\"‚ö†Ô∏è Erro: arquivo sem colunas: {filepath}\")\n",
        "      return None\n",
        "  except Exception as e:\n",
        "      print(f\"‚ö†Ô∏è Erro inesperado ao ler CSV: {e}\")\n",
        "      return None\n",
        "\n",
        "def criar_prediction_log_padrao(filepath=\"/content/prediction_log.csv\", backup_dir=\"/content/prediction_backups\"):\n",
        "    import pandas as pd\n",
        "    import os\n",
        "    from datetime import datetime\n",
        "\n",
        "    colunas_padroes = [\n",
        "        \"Asset\", \"Timeframe\", \"Date\", \"Price\", \"Signal\", \"Confidence\", \"AdjustedProb\",\n",
        "        \"TP1\", \"TP2\", \"SL\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\",\n",
        "        \"LSTM_Predicted\", \"TargetPrice\",\n",
        "        \"LSTM_High_Predicted\", \"LSTM_Low_Predicted\",\n",
        "        \"Entry\", \"Acertou\", \"Resultado\", \"PrecoSaida\", \"LucroEstimado\", \"DuracaoMin\", \"Capital Atual\"\n",
        "    ]\n",
        "\n",
        "    # Cria a pasta de backups se n√£o existir\n",
        "    os.makedirs(backup_dir, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"üìÑ Criando novo prediction_log.csv em: {filepath}\")\n",
        "        df_vazio = pd.DataFrame(columns=colunas_padroes)\n",
        "        df_vazio.to_csv(filepath, index=False)\n",
        "    else:\n",
        "        try:\n",
        "            df_existente = pd.read_csv(filepath)\n",
        "            missing_cols = [col for col in colunas_padroes if col not in df_existente.columns]\n",
        "            if missing_cols:\n",
        "                print(f\"‚öôÔ∏è Adicionando colunas faltantes: {missing_cols}\")\n",
        "                for col in missing_cols:\n",
        "                    df_existente[col] = None\n",
        "                df_existente.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erro ao ler log existente. Recriando vazio. Erro: {e}\")\n",
        "            df_vazio = pd.DataFrame(columns=colunas_padroes)\n",
        "            df_vazio.to_csv(filepath, index=False)\n",
        "\n",
        "    # üéØ Backup autom√°tico para seguran√ßa\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    backup_path = os.path.join(backup_dir, f\"prediction_log_{timestamp}.csv\")\n",
        "    try:\n",
        "        import shutil\n",
        "        shutil.copy(filepath, backup_path)\n",
        "        print(f\"‚úÖ Backup do log salvo em: {backup_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Falha ao criar backup do log: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # ====================================================\n",
        "  # 4. INDICADORES T√âCNICOS\n",
        "  # ====================================================\n",
        "def calculate_indicators(data):\n",
        "    data = data.copy().reset_index(drop=True)\n",
        "    for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]:\n",
        "        data[col] = data[col].astype(float)\n",
        "\n",
        "    # Indicadores cl√°ssicos\n",
        "    try:\n",
        "        data[\"RSI\"] = ta.momentum.RSIIndicator(close=data[\"Close\"], window=14).rsi()\n",
        "        data[\"SMA_50\"] = ta.trend.SMAIndicator(close=data[\"Close\"], window=50).sma_indicator()\n",
        "        data[\"SMA_200\"] = ta.trend.SMAIndicator(close=data[\"Close\"], window=200).sma_indicator()\n",
        "\n",
        "        macd = ta.trend.MACD(close=data[\"Close\"])\n",
        "        data[\"MACD\"] = macd.macd()\n",
        "        data[\"MACD_Signal\"] = macd.macd_signal()\n",
        "\n",
        "        bb = ta.volatility.BollingerBands(close=data[\"Close\"], window=20)\n",
        "        data[\"Bollinger_Upper\"] = bb.bollinger_hband()\n",
        "        data[\"Bollinger_Lower\"] = bb.bollinger_lband()\n",
        "\n",
        "        adx = ta.trend.ADXIndicator(high=data[\"High\"], low=data[\"Low\"], close=data[\"Close\"], window=14)\n",
        "        data[\"ADX\"] = adx.adx()\n",
        "\n",
        "        stoch = ta.momentum.StochasticOscillator(high=data[\"High\"], low=data[\"Low\"], close=data[\"Close\"], window=14)\n",
        "        data[\"Stoch_K\"] = stoch.stoch()\n",
        "        data[\"Stoch_D\"] = stoch.stoch_signal()\n",
        "\n",
        "        # Indicadores adicionais\n",
        "        data[\"ATR\"] = ta.volatility.AverageTrueRange(high=data[\"High\"], low=data[\"Low\"], close=data[\"Close\"]).average_true_range()\n",
        "        data[\"ROC\"] = ta.momentum.ROCIndicator(close=data[\"Close\"], window=12).roc()\n",
        "        data[\"OBV\"] = ta.volume.OnBalanceVolumeIndicator(close=data[\"Close\"], volume=data[\"Volume\"]).on_balance_volume()\n",
        "        data[\"CCI\"] = ta.trend.CCIIndicator(high=data[\"High\"], low=data[\"Low\"], close=data[\"Close\"], window=20).cci()\n",
        "\n",
        "        ichimoku = ta.trend.IchimokuIndicator(high=data[\"High\"], low=data[\"Low\"], window1=9, window2=26)\n",
        "        data[\"Tenkan_Sen\"] = ichimoku.ichimoku_conversion_line()\n",
        "        data[\"Kijun_Sen\"] = ichimoku.ichimoku_base_line()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erro ao calcular indicadores: {e}\")\n",
        "\n",
        "    # VWAP\n",
        "    try:\n",
        "        data[\"TP\"] = (data[\"High\"] + data[\"Low\"] + data[\"Close\"]) / 3\n",
        "        data[\"VWAP\"] = (data[\"TP\"] * data[\"Volume\"]).cumsum() / (data[\"Volume\"].replace(0, np.nan).cumsum())\n",
        "        data.drop(\"TP\", axis=1, inplace=True)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erro ao calcular VWAP: {e}\")\n",
        "\n",
        "    # Candlestick patterns\n",
        "    try:\n",
        "        data[\"Doji\"] = ((abs(data[\"Close\"] - data[\"Open\"]) / (data[\"High\"] - data[\"Low\"] + 1e-9)) < 0.1).astype(int)\n",
        "        data[\"Engulfing\"] = ((data[\"Open\"].shift(1) > data[\"Close\"].shift(1)) & (data[\"Open\"] < data[\"Close\"]) &\n",
        "                            (data[\"Close\"] > data[\"Open\"].shift(1)) & (data[\"Open\"] < data[\"Close\"].shift(1))).astype(int)\n",
        "        data[\"Hammer\"] = (((data[\"High\"] - data[\"Low\"]) > 3 * abs(data[\"Open\"] - data[\"Close\"])) &\n",
        "                        ((data[\"Close\"] - data[\"Low\"]) / (data[\"High\"] - data[\"Low\"] + 1e-9) > 0.6) &\n",
        "                        ((data[\"Open\"] - data[\"Low\"]) / (data[\"High\"] - data[\"Low\"] + 1e-9) > 0.6)).astype(int)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erro ao calcular padr√µes de candle: {e}\")\n",
        "        # Garante que existam\n",
        "        data[\"Doji\"] = 0\n",
        "        data[\"Engulfing\"] = 0\n",
        "        data[\"Hammer\"] = 0\n",
        "\n",
        "    data.dropna(inplace=True)\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 4. MODELOS DE MACHINE LEARNING (XGBoost + LSTM)\n",
        "# ====================================================\n",
        "\n",
        "def get_feature_columns(df, include_lstm_pred=False):\n",
        "    \"\"\"\n",
        "    Retorna a lista de colunas de features para os modelos.\n",
        "    Se include_lstm_pred=True, inclui a coluna LSTM_PRED para uso no XGBoost.\n",
        "    \"\"\"\n",
        "    base_features = [\n",
        "        'Open', 'High', 'Low', 'Close', 'Volume',\n",
        "        'SMA_5', 'SMA_20', 'EMA_12', 'EMA_26',\n",
        "        'RSI', 'MACD', 'MACD_signal', 'MACD_hist',\n",
        "        'BB_upper', 'BB_middle', 'BB_lower',\n",
        "        'ATR', 'CCI', 'ROC', 'OBV'\n",
        "    ]\n",
        "    if include_lstm_pred:\n",
        "        base_features.append(\"LSTM_PRED\")\n",
        "    return [col for col in base_features if col in df.columns]\n",
        "\n",
        "\n",
        "def get_lstm_feature_columns():\n",
        "    return [\n",
        "        \"Close\", \"High\", \"Low\",  # üü¢ Agora inclui as tr√™s colunas principais como features tamb√©m\n",
        "        \"RSI\", \"MACD\", \"MACD_Signal\", \"SMA_50\", \"SMA_200\",\n",
        "        \"Bollinger_Upper\", \"Bollinger_Lower\",\n",
        "        \"ADX\", \"Stoch_K\", \"Stoch_D\",\n",
        "        \"ATR\", \"ROC\", \"OBV\", \"CCI\",\n",
        "        \"Tenkan_Sen\", \"Kijun_Sen\", \"VWAP\",\n",
        "        \"Doji\", \"Engulfing\", \"Hammer\"\n",
        "    ]\n",
        "\n",
        "\n",
        "def prepare_lstm_data(data, feature_cols=None, target_cols=[\"High\", \"Low\", \"Close\"], window_size=20):\n",
        "    if feature_cols is None:\n",
        "        feature_cols = get_lstm_feature_columns()\n",
        "\n",
        "    missing = [col for col in feature_cols + target_cols if col not in data.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"‚ùå Colunas ausentes no DataFrame: {missing}\")\n",
        "\n",
        "    df = data[feature_cols + target_cols].dropna().astype(float)\n",
        "    if len(df) < window_size + 1:\n",
        "        raise ValueError(f\"‚ö†Ô∏è Dados insuficientes: {len(df)} rows, necess√°rio m√≠nimo {window_size + 1}\")\n",
        "\n",
        "    # Escalonamento separado\n",
        "    scaler_x = MinMaxScaler()\n",
        "    scaler_y = MinMaxScaler()\n",
        "\n",
        "    scaled_X = scaler_x.fit_transform(df[feature_cols])\n",
        "    scaled_y = scaler_y.fit_transform(df[target_cols])\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_X[i - window_size:i])\n",
        "        y.append(scaled_y[i])  # Previs√£o para o instante i\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    print(f\"‚úÖ prepare_lstm_data | X.shape: {X.shape}, y.shape: {y.shape}\")\n",
        "    return X, y, scaler_x, scaler_y\n",
        "\n",
        "\n",
        "import time  # j√° est√° importado no seu c√≥digo\n",
        "\n",
        "def train_lstm_model(df, *, asset, interval, window_size=20, force_retrain=False):\n",
        "    feature_cols = get_lstm_feature_columns()\n",
        "    target_cols = [\"High\", \"Low\", \"Close\"]\n",
        "\n",
        "    df = df.dropna(subset=feature_cols + target_cols)\n",
        "\n",
        "    if len(df) <= window_size:\n",
        "        raise ValueError(\"Dados insuficientes para treino do LSTM.\")\n",
        "\n",
        "    df_features = df[feature_cols]\n",
        "    df_targets = df[target_cols]\n",
        "\n",
        "    scaler_x = MinMaxScaler()\n",
        "    scaler_y = MinMaxScaler()\n",
        "\n",
        "    scaled_features = scaler_x.fit_transform(df_features)\n",
        "    scaled_targets = scaler_y.fit_transform(df_targets)\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_features[i - window_size:i])\n",
        "        y.append(scaled_targets[i])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    print(f\"‚úÖ train_lstm_model | X.shape: {X.shape}, y.shape: {y.shape}\")\n",
        "\n",
        "    model_path = get_model_path(asset, interval, model_type=\"lstm\")\n",
        "    meta_path = model_path.replace(\".h5\", \"_meta.pkl\")\n",
        "\n",
        "    if not force_retrain and os.path.exists(model_path):\n",
        "        model = load_lstm_model(asset, interval)\n",
        "        if model and all(hasattr(model, attr) for attr in [\"scaler_x\", \"scaler_y\", \"feature_cols\", \"window_size\", \"target_cols\"]):\n",
        "            return model\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Modelo existente n√£o cont√©m atributos. Ser√° refeito.\")\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(32, return_sequences=False))\n",
        "    model.add(Dense(3))  # 3 sa√≠das: High, Low, Close\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # ‚è±Ô∏è Cronometrar o treinamento\n",
        "    start_time = time.time()\n",
        "\n",
        "    es = EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss')\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6, verbose=1)\n",
        "\n",
        "    history = model.fit(\n",
        "        X, y,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[es, reduce_lr],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    final_loss = history.history['loss'][-1]\n",
        "    print(f\"‚úÖ Treinamento LSTM conclu√≠do em {elapsed_time:.2f}s | Loss final: {final_loss:.6f}\")\n",
        "\n",
        "    model.scaler_x = scaler_x\n",
        "    model.scaler_y = scaler_y\n",
        "    model.scaler = scaler_x\n",
        "    model.feature_cols = feature_cols\n",
        "    model.target_cols = target_cols\n",
        "    model.window_size = window_size\n",
        "\n",
        "    model.save(model_path)\n",
        "    joblib.dump({\n",
        "        \"scaler_x\": scaler_x,\n",
        "        \"scaler_y\": scaler_y,\n",
        "        \"feature_cols\": feature_cols,\n",
        "        \"target_cols\": target_cols,\n",
        "        \"window_size\": window_size\n",
        "    }, meta_path)\n",
        "\n",
        "    print(f\"üíæ Modelo LSTM salvo em: {model_path}\")\n",
        "    print(f\"üì¶ Metadados salvos em: {meta_path}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_lstm_model_diario(df, *, asset, interval, window_size=60, force_retrain=False):\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    import os\n",
        "    import time\n",
        "    import joblib\n",
        "    import numpy as np\n",
        "\n",
        "    feature_cols = get_lstm_feature_columns()\n",
        "    target_cols = [\"High\", \"Low\", \"Close\"]\n",
        "\n",
        "    df = df.dropna(subset=feature_cols + target_cols)\n",
        "\n",
        "    if len(df) <= window_size:\n",
        "        raise ValueError(\"Dados insuficientes para treino do LSTM.\")\n",
        "\n",
        "    df_features = df[feature_cols]\n",
        "    df_targets = df[target_cols]\n",
        "\n",
        "    scaler_x = MinMaxScaler()\n",
        "    scaler_y = MinMaxScaler()\n",
        "\n",
        "    scaled_features = scaler_x.fit_transform(df_features)\n",
        "    scaled_targets = scaler_y.fit_transform(df_targets)\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_features[i - window_size:i])\n",
        "        y.append(scaled_targets[i])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    model_path = get_model_path(asset, interval, model_type=\"lstm\")\n",
        "    meta_path = model_path.replace(\".h5\", \"_meta.pkl\")\n",
        "\n",
        "    if not force_retrain and os.path.exists(model_path):\n",
        "        model = load_lstm_model(asset, interval)\n",
        "        if model and all(hasattr(model, attr) for attr in [\"scaler_x\", \"scaler_y\", \"feature_cols\", \"window_size\", \"target_cols\"]):\n",
        "            return model\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Modelo existente n√£o cont√©m atributos corretos. Ser√° refeito.\")\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(LSTM(64, return_sequences=False))\n",
        "    model.add(Dense(3))  # High, Low, Close\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Cronometrar o treino\n",
        "    start_time = time.time()\n",
        "\n",
        "    es = EarlyStopping(patience=20, restore_best_weights=True, monitor='val_loss')\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n",
        "\n",
        "    history = model.fit(\n",
        "        X, y,\n",
        "        epochs=300,\n",
        "        batch_size=64,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[es, reduce_lr],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"‚úÖ Treinamento LSTM Di√°rio conclu√≠do em {elapsed_time:.2f}s | Loss final: {history.history['loss'][-1]:.6f}\")\n",
        "\n",
        "    # Atribuir atributos\n",
        "    model.scaler_x = scaler_x\n",
        "    model.scaler_y = scaler_y\n",
        "    model.scaler = scaler_x\n",
        "    model.feature_cols = feature_cols\n",
        "    model.target_cols = target_cols\n",
        "    model.window_size = window_size\n",
        "\n",
        "    # Salvar\n",
        "    model.save(model_path)\n",
        "    joblib.dump({\n",
        "        \"scaler_x\": scaler_x,\n",
        "        \"scaler_y\": scaler_y,\n",
        "        \"feature_cols\": feature_cols,\n",
        "        \"target_cols\": target_cols,\n",
        "        \"window_size\": window_size\n",
        "    }, meta_path)\n",
        "\n",
        "    print(f\"üíæ Modelo LSTM Di√°rio salvo em: {model_path}\")\n",
        "    print(f\"üì¶ Metadados salvos em: {meta_path}\")\n",
        "    return model\n",
        "\n",
        "def train_ml_model(data, asset=None, interval=None, verbose=False, force_retrain=False):\n",
        "    from xgboost import XGBClassifier\n",
        "    from sklearn.model_selection import TimeSeriesSplit\n",
        "    from sklearn.metrics import classification_report\n",
        "    import time\n",
        "\n",
        "    if asset and interval:\n",
        "        if not force_retrain:\n",
        "            existing_model = load_xgb_model(asset, interval)\n",
        "            if existing_model is not None:\n",
        "                print(f\"‚úÖ Modelo XGBoost j√° existente para {asset} ({interval}), carregado.\")\n",
        "                return existing_model\n",
        "\n",
        "    if len(data) < 100:\n",
        "        return None\n",
        "\n",
        "    df = data.copy()\n",
        "    df = calculate_indicators(df)\n",
        "\n",
        "    try:\n",
        "        lstm_model = train_lstm_model(df, asset=asset, interval=interval, window_size=20, force_retrain=force_retrain)\n",
        "\n",
        "        if lstm_model:\n",
        "            print(\"‚úÖ Features usadas no LSTM:\")\n",
        "            print(lstm_model.feature_cols)\n",
        "\n",
        "            print(\"‚úÖ √öltimos dados de entrada:\")\n",
        "            print(df[lstm_model.feature_cols].tail(3))\n",
        "\n",
        "            print(\"‚úÖ Valores m√≠nimos do scaler X:\")\n",
        "            print(lstm_model.scaler_x.data_min_)\n",
        "            print(\"‚úÖ Valores m√°ximos do scaler X:\")\n",
        "            print(lstm_model.scaler_x.data_max_)\n",
        "\n",
        "        if lstm_model is not None:\n",
        "            lstm_preds = []\n",
        "            for i in range(len(df)):\n",
        "                sub_df = df.iloc[:i+1]\n",
        "                if len(sub_df) < lstm_model.window_size:\n",
        "                    lstm_preds.append(np.nan)\n",
        "                else:\n",
        "                    try:\n",
        "                        pred = predict_with_lstm(lstm_model, sub_df)\n",
        "                        lstm_preds.append(pred.get(\"Close\", np.nan))\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è Erro ao prever com LSTM: {e}\")\n",
        "                        lstm_preds.append(np.nan)\n",
        "            df[\"LSTM_PRED\"] = lstm_preds\n",
        "        else:\n",
        "            df[\"LSTM_PRED\"] = np.nan\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erro ao gerar LSTM_PRED: {e}\")\n",
        "        df[\"LSTM_PRED\"] = np.nan\n",
        "\n",
        "    if \"LSTM_PRED\" not in df.columns:\n",
        "        print(\"‚ùå Coluna 'LSTM_PRED' n√£o foi gerada. Abortando treino do XGBoost.\")\n",
        "        return None\n",
        "\n",
        "    df[\"Future_Close\"] = df[\"Close\"].shift(-5)\n",
        "    df[\"Future_Return\"] = df[\"Future_Close\"] / df[\"Close\"] - 1\n",
        "    df = df[(df[\"Future_Return\"] > 0.015) | (df[\"Future_Return\"] < -0.015)].copy()\n",
        "    df[\"Signal\"] = np.where(df[\"Future_Return\"] > 0.015, 1, 0)\n",
        "\n",
        "    features = get_feature_columns(df, include_lstm_pred=True)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    missing_features = [f for f in features if f not in df.columns]\n",
        "    if missing_features:\n",
        "        print(f\"‚ùå Features ausentes: {missing_features}\")\n",
        "        return None\n",
        "\n",
        "    X = df[features]\n",
        "    y = df[\"Signal\"]\n",
        "\n",
        "    if len(np.unique(y)) < 2:\n",
        "        return None\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    for train_index, val_index in tscv.split(X):\n",
        "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "        break\n",
        "\n",
        "    if len(np.unique(y_train)) < 2:\n",
        "        return None\n",
        "\n",
        "    scale_pos_weight = len(y_train[y_train == 0]) / max(1, len(y_train[y_train == 1]))\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric=\"logloss\",\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # ‚è±Ô∏è Cronometrar o treinamento\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"‚úÖ Treinamento XGBoost conclu√≠do em {elapsed_time:.2f}s\")\n",
        "\n",
        "    y_pred = model.predict(X_val)\n",
        "    report = classification_report(y_val, y_pred, output_dict=True, zero_division=0)\n",
        "\n",
        "    model.validation_score = {\n",
        "        \"accuracy\": report.get(\"accuracy\"),\n",
        "        \"precision\": report.get(\"1\", {}).get(\"precision\"),\n",
        "        \"recall\": report.get(\"1\", {}).get(\"recall\"),\n",
        "        \"f1\": report.get(\"1\", {}).get(\"f1-score\")\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"‚úÖ Valida√ß√£o: {model.validation_score}\")\n",
        "\n",
        "    if asset and interval:\n",
        "        save_xgb_model(model, asset, interval)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict_with_lstm(model, df, asset=\"N/A\", interval=\"N/A\"):\n",
        "    \"\"\"\n",
        "    Faz a previs√£o com LSTM usando a √∫ltima janela de dados,\n",
        "    corrige problemas comuns automaticamente e registra previs√µes descartadas.\n",
        "    \"\"\"\n",
        "    if not all(hasattr(model, attr) for attr in ['scaler_x', 'scaler_y', 'feature_cols', 'window_size']):\n",
        "        raise AttributeError(\"‚ùå O modelo LSTM n√£o possui os atributos necess√°rios (scaler_x, scaler_y, feature_cols, window_size).\")\n",
        "\n",
        "    df = df.copy().dropna(subset=model.feature_cols)\n",
        "    if len(df) < model.window_size:\n",
        "        raise ValueError(\"‚ö†Ô∏è Dados insuficientes para previs√£o com LSTM.\")\n",
        "\n",
        "    last_window = df[model.feature_cols].values[-model.window_size:]\n",
        "    scaled_window = model.scaler_x.transform(last_window)\n",
        "    X_input = np.expand_dims(scaled_window, axis=0)\n",
        "\n",
        "    pred_scaled = model.predict(X_input, verbose=0)[0].reshape(1, -1)\n",
        "    pred_descaled = model.scaler_y.inverse_transform(pred_scaled)[0]\n",
        "\n",
        "    high, low, close = float(pred_descaled[0]), float(pred_descaled[1]), float(pred_descaled[2])\n",
        "\n",
        "    # üîí Corre√ß√£o 1: High deve ser maior ou igual ao Low\n",
        "    if low > high:\n",
        "        print(f\"‚ö†Ô∏è Corrigindo invers√£o de High/Low na previs√£o LSTM. High={high:.2f}, Low={low:.2f}\")\n",
        "        high, low = max(high, low), min(high, low)\n",
        "\n",
        "    # üîí Corre√ß√£o 2: Close deve ficar entre Low e High\n",
        "    if close < low or close > high:\n",
        "        print(f\"‚ö†Ô∏è Ajustando Close fora da faixa. Antes: {close:.2f}\")\n",
        "        close = max(min(close, high), low)\n",
        "        print(f\"‚úÖ Close ajustado para: {close:.2f}\")\n",
        "\n",
        "    # üîí Corre√ß√£o 3: Previs√µes absurdas (varia√ß√£o maior que 50%)\n",
        "    preco_atual = df[\"Close\"].iloc[-1]\n",
        "    if preco_atual > 0:\n",
        "        variacao_permitida = 0.5  # 50%\n",
        "        if abs(close - preco_atual) / preco_atual > variacao_permitida:\n",
        "            print(f\"‚ùå Previs√£o absurda detectada. Atual={preco_atual:.2f} Previsto={close:.2f}\")\n",
        "            log_previsao_absurda(asset=asset, interval=interval, preco_atual=preco_atual, close_previsto=close)\n",
        "            return {\"High\": None, \"Low\": None, \"Close\": None}\n",
        "\n",
        "    return {\n",
        "        \"High\": round(high, 4),\n",
        "        \"Low\": round(low, 4),\n",
        "        \"Close\": round(close, 4)\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_feature_importance(model, feature_names, top_n=15):\n",
        "    \"\"\"\n",
        "    Plota a import√¢ncia das features do modelo XGBoost.\n",
        "    \"\"\"\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        importances = model.feature_importances_\n",
        "    else:\n",
        "        print(\"‚ùå O modelo n√£o possui 'feature_importances_'.\")\n",
        "        return\n",
        "\n",
        "    indices = np.argsort(importances)[-top_n:][::-1]\n",
        "    top_features = [feature_names[i] for i in indices]\n",
        "    top_importances = importances[indices]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(top_features[::-1], top_importances[::-1])\n",
        "    plt.xlabel('Import√¢ncia')\n",
        "    plt.title('Top Features Importantes')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 5. UTILIT√ÅRIOS\n",
        "# ====================================================\n",
        "# ====================================================\n",
        "# FUN√á√ÉO GLOBAL DE CONVERS√ÉO ESCALAR\n",
        "# ====================================================\n",
        "def to_scalar(val):\n",
        "    try:\n",
        "        if isinstance(val, pd.Series):\n",
        "            return float(val.iloc[0])\n",
        "        elif isinstance(val, (np.ndarray, list)):\n",
        "            return float(val[0])\n",
        "        elif pd.isna(val):\n",
        "            return np.nan\n",
        "        else:\n",
        "            return float(val)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Falha ao converter valor escalar: {val} | erro: {e}\")\n",
        "        return np.nan\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def limpar_model_results():\n",
        "    arquivos = glob.glob(\"/content/model_results_*.csv\")\n",
        "    if not arquivos:\n",
        "        print(\"üìÇ Nenhum arquivo model_results_*.csv encontrado.\")\n",
        "        return\n",
        "\n",
        "def plot_entrada_lstm(df, feature_cols):\n",
        "    import matplotlib.pyplot as plt\n",
        "    df_plot = df[feature_cols].tail(100).copy()\n",
        "    df_plot.plot(figsize=(12, 5), title=\"üìä √öltimas 100 entradas das features LSTM\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def gerar_resumo_ultimos_sinais(asset, interval, n=15, path=\"/content/prediction_log.csv\"):\n",
        "    df_log = safe_read_csv(path)\n",
        "    if df_log is None or df_log.empty:\n",
        "        return \"üì≠ Sem sinais anteriores registrados.\"\n",
        "\n",
        "    df_log = df_log[(df_log[\"Asset\"] == asset) & (df_log[\"Timeframe\"] == interval)].copy()\n",
        "    df_log[\"Date\"] = pd.to_datetime(df_log[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "    df_log = df_log.sort_values(\"Date\", ascending=False).head(n)\n",
        "\n",
        "    linhas = []\n",
        "    for _, row in df_log.iterrows():\n",
        "        data_str = row[\"Date\"].strftime(\"%d/%m %H:%M\")\n",
        "        sinal = \"COMPRA\" if row[\"Signal\"] == 1 else \"VENDA\" if row[\"Signal\"] == 0 else \"NEUTRO\"\n",
        "        emoji = \"‚úîÔ∏è\" if row.get(\"Resultado\") == \"TP1\" else \"‚ùå\" if row.get(\"Resultado\") == \"SL\" else \"‚ûñ\"\n",
        "        lucro = row.get(\"LucroEstimado\", \"\")\n",
        "        lucro_str = f\" | Lucro: {lucro:+.2f}\" if pd.notna(lucro) else \"\"\n",
        "        linhas.append(f\"{emoji} {sinal} | {row['Asset']} | {data_str}{lucro_str}\")\n",
        "\n",
        "    # Acur√°cia dos √∫ltimos sinais (apenas TP1 e SL considerados)\n",
        "    df_valid = df_log[df_log[\"Resultado\"].isin([\"TP1\", \"SL\"])]\n",
        "    if not df_valid.empty:\n",
        "        acertos = (df_valid[\"Resultado\"] == \"TP1\").sum()\n",
        "        total = len(df_valid)\n",
        "        acuracia = round(100 * acertos / total, 2)\n",
        "        linhas.append(f\"\\nüìà <b>Acur√°cia:</b> {acuracia}% ({acertos}/{total})\")\n",
        "\n",
        "    return \"üìä <b>√öltimos Sinais:</b>\\n\" + \"\\n\".join(linhas)\n",
        "\n",
        "\n",
        "def gerar_ranking_lucro(path=\"/content/prediction_log.csv\", top_n=5):\n",
        "    df = safe_read_csv(path)\n",
        "    if df is None or df.empty or \"LucroEstimado\" not in df.columns:\n",
        "        return \"üì≠ Sem dados de lucro dispon√≠veis.\"\n",
        "\n",
        "    df = df.dropna(subset=[\"Asset\", \"LucroEstimado\"])\n",
        "    df = df[df[\"Resultado\"].isin([\"TP1\", \"SL\", \"Sem alvo\"])]\n",
        "    df_grouped = df.groupby(\"Asset\")[\"LucroEstimado\"].sum().sort_values(ascending=False).head(top_n)\n",
        "\n",
        "    linhas = [\"üèÜ <b>Top Ativos por Lucro Total:</b>\"]\n",
        "    for ativo, lucro in df_grouped.items():\n",
        "        emoji = \"üü¢\" if lucro > 0 else \"üî¥\" if lucro < 0 else \"‚ö™\"\n",
        "        linhas.append(f\"{emoji} {ativo}: ${lucro:+.2f}\")\n",
        "    return \"\\n\".join(linhas)\n",
        "\n",
        "def gerar_resumo_por_padrao(asset, interval, path=\"/content/prediction_log.csv\"):\n",
        "    df = safe_read_csv(path)\n",
        "    if df is None or df.empty:\n",
        "        return \"üì≠ Sem sinais anteriores registrados.\"\n",
        "\n",
        "    df = df[(df[\"Asset\"] == asset) & (df[\"Timeframe\"] == interval)]\n",
        "    subset_cols = [\"Doji\", \"Engulfing\", \"Hammer\"]\n",
        "    subset_cols = [col for col in subset_cols if col in df.columns]\n",
        "    if subset_cols:\n",
        "        df = df.dropna(subset=subset_cols)\n",
        "\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "\n",
        "    ultimos = df.sort_values(\"Date\", ascending=False).head(50)\n",
        "\n",
        "    contagem = {\n",
        "        \"Doji\": ultimos[\"Doji\"].sum() if \"Doji\" in ultimos.columns else 0,\n",
        "        \"Engolfo\": ultimos[\"Engulfing\"].sum() if \"Engulfing\" in ultimos.columns else 0,\n",
        "        \"Martelo\": ultimos[\"Hammer\"].sum() if \"Hammer\" in ultimos.columns else 0\n",
        "    }\n",
        "\n",
        "\n",
        "    linhas = [\"üîé <b>Padr√µes Recentes Detectados:</b>\"]\n",
        "    for nome, qtd in contagem.items():\n",
        "        if qtd > 0:\n",
        "            linhas.append(f\"‚Ä¢ {nome}: {int(qtd)} ocorr√™ncia(s)\")\n",
        "    return \"\\n\".join(linhas) if len(linhas) > 1 else \"‚ö™ Nenhum padr√£o t√©cnico detectado recentemente.\"\n",
        "\n",
        "def generate_explanation(row, prediction, feature_importance=None):\n",
        "    \"\"\"\n",
        "    Gera explica√ß√£o t√©cnica com base em indicadores e no sinal previsto (compra/venda).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        explicacao = []\n",
        "\n",
        "        if prediction == 1:\n",
        "            explicacao.append(\"üü¢ O modelo prev√™ uma tend√™ncia de ALTA (compra).\")\n",
        "        elif prediction == 0:\n",
        "            explicacao.append(\"üî¥ O modelo prev√™ uma tend√™ncia de BAIXA (venda).\")\n",
        "        else:\n",
        "            explicacao.append(\"‚ö™ Tend√™ncia neutra ‚Äî sem sinal claro.\")\n",
        "\n",
        "        # Indicadores cl√°ssicos\n",
        "        if \"RSI\" in row:\n",
        "            if row[\"RSI\"] < 30:\n",
        "                explicacao.append(\"‚Ä¢ RSI indica sobrevenda (RSI < 30).\")\n",
        "            elif row[\"RSI\"] > 70:\n",
        "                explicacao.append(\"‚Ä¢ RSI indica sobrecompra (RSI > 70).\")\n",
        "\n",
        "        if \"MACD\" in row and \"MACD_Signal\" in row:\n",
        "            if row[\"MACD\"] > row[\"MACD_Signal\"]:\n",
        "                explicacao.append(\"‚Ä¢ MACD cruzando para cima da linha de sinal (potencial alta).\")\n",
        "            else:\n",
        "                explicacao.append(\"‚Ä¢ MACD abaixo da linha de sinal (potencial queda).\")\n",
        "\n",
        "        if \"SMA_50\" in row and \"SMA_200\" in row:\n",
        "            if row[\"SMA_50\"] > row[\"SMA_200\"]:\n",
        "                explicacao.append(\"‚Ä¢ SMA 50 acima da 200 (tend√™ncia de alta no m√©dio prazo).\")\n",
        "            else:\n",
        "                explicacao.append(\"‚Ä¢ SMA 50 abaixo da 200 (tend√™ncia de baixa no m√©dio prazo).\")\n",
        "\n",
        "        if \"ADX\" in row and row[\"ADX\"] > 20:\n",
        "            explicacao.append(\"‚Ä¢ ADX > 20 (tend√™ncia direcional presente).\")\n",
        "\n",
        "        # Padr√µes de candle\n",
        "        if row.get(\"Doji\") == 1:\n",
        "            explicacao.append(\"‚Ä¢ Padr√£o Doji detectado (poss√≠vel revers√£o).\")\n",
        "        if row.get(\"Engulfing\") == 1:\n",
        "            explicacao.append(\"‚Ä¢ Padr√£o de engolfo detectado (revers√£o poss√≠vel).\")\n",
        "        if row.get(\"Hammer\") == 1:\n",
        "            explicacao.append(\"‚Ä¢ Padr√£o de martelo identificado (alta poss√≠vel).\")\n",
        "\n",
        "        # Import√¢ncia de features (se dispon√≠vel)\n",
        "        if feature_importance:\n",
        "            explicacao.append(\"\\nüìä Principais influ√™ncias do modelo:\")\n",
        "            top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "            for name, weight in top_features:\n",
        "                explicacao.append(f\"‚Ä¢ {name}: peso {weight:.3f}\")\n",
        "\n",
        "        return \"\\n\".join(explicacao)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ö†Ô∏è Erro ao gerar explica√ß√£o: {str(e)}\"\n",
        "\n",
        "def enviar_grafico_previsao_futura(df_previsao, timeframe, asset):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.dates as mdates\n",
        "    import os\n",
        "    import pandas as pd\n",
        "\n",
        "    if df_previsao is None or not all(k in df_previsao for k in [\"Date\", \"High\", \"Low\", \"Close\"]):\n",
        "        print(f\"‚ö†Ô∏è Dados de previs√£o futura incompletos para {asset} ({timeframe})\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(df_previsao)\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "    df.set_index(\"Date\", inplace=False)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        date = df[\"Date\"].iloc[i]\n",
        "        high = df[\"High\"].iloc[i]\n",
        "        low = df[\"Low\"].iloc[i]\n",
        "        close = df[\"Close\"].iloc[i]\n",
        "\n",
        "        plt.vlines(date, ymin=low, ymax=high, color=\"blue\", linewidth=2, label=\"Proje√ß√£o\" if i == 0 else \"\")\n",
        "        plt.plot(date, close, marker=\"o\", color=\"blue\")\n",
        "\n",
        "        # R√≥tulo com valor previsto\n",
        "        plt.annotate(f\"{close:.0f}\", (date, close), xytext=(0, 8),\n",
        "                     textcoords=\"offset points\", ha='center', fontsize=8, color=\"blue\")\n",
        "\n",
        "    plt.title(f\"üîÆ Proje√ß√£o Futura (LSTM) ‚Äî {asset} ({timeframe})\")\n",
        "    plt.xlabel(\"Data\")\n",
        "    plt.ylabel(\"Pre√ßo Projetado\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d/%m %H:%M', tz=BR_TZ))\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    image_path = f\"/tmp/projecao_futura_{asset.replace('-', '')}_{timeframe}.png\"\n",
        "    plt.savefig(image_path)\n",
        "    plt.close()\n",
        "    print(f\"‚úÖ Gr√°fico de proje√ß√£o futura salvo: {image_path}\")\n",
        "\n",
        "    # Enviar para Telegram\n",
        "    if os.path.exists(image_path):\n",
        "        with open(image_path, \"rb\") as img:\n",
        "            url = f\"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendPhoto\"\n",
        "            files = {\"photo\": img}\n",
        "            data = {\n",
        "                \"chat_id\": TELEGRAM_CHAT_ID,\n",
        "                \"caption\": f\"üîÆ Proje√ß√£o Futura ‚Äî {asset} ({timeframe})\"\n",
        "            }\n",
        "            response = requests.post(url, data=data, files=files)\n",
        "            if response.status_code == 200:\n",
        "                print(\"‚úÖ Gr√°fico de proje√ß√£o futura enviado ao Telegram.\")\n",
        "            else:\n",
        "                print(f\"‚ùå Erro ao enviar gr√°fico: {response.status_code} - {response.text}\")\n",
        "\n",
        "\n",
        "import mplfinance as mpf\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "\n",
        "import mplfinance as mpf\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "\n",
        "def plotar_candles_com_previsao(\n",
        "    df_candles,\n",
        "    pred_lstm_dicts,\n",
        "    title=\"üìä Hist√≥rico + Previs√£o LSTM\",\n",
        "    asset=\"BTC-USD\",\n",
        "    timeframe=\"15m\",\n",
        "    save_path=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Plota 10 candles reais + previs√µes futuras com mplfinance (tipo candlestick).\n",
        "    \"\"\"\n",
        "\n",
        "    # üõ†Ô∏è Garantir que 'Date' √© datetime\n",
        "    df_candles = df_candles.copy()\n",
        "    if \"Date\" not in df_candles.columns:\n",
        "        df_candles[\"Date\"] = df_candles.index\n",
        "    df_candles[\"Date\"] = pd.to_datetime(df_candles[\"Date\"])\n",
        "\n",
        "    df_plot = df_candles.tail(10).reset_index(drop=True)[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\"]].copy()\n",
        "    df_plot[\"Volume\"] = 0  # placeholder para evitar erro do mplfinance\n",
        "\n",
        "    last_date = df_plot[\"Date\"].iloc[-1]\n",
        "\n",
        "    timeframe_delta = {\n",
        "        \"15m\": timedelta(minutes=15),\n",
        "        \"1h\": timedelta(hours=1),\n",
        "        \"1d\": timedelta(days=1),\n",
        "        \"1wk\": timedelta(weeks=1)\n",
        "    }.get(timeframe, timedelta(hours=1))\n",
        "\n",
        "    # üîÆ Adiciona os candles futuros previstos\n",
        "    for i, pred in enumerate(pred_lstm_dicts):\n",
        "        if any(pred.get(k) is None for k in [\"High\", \"Low\", \"Close\"]):\n",
        "            continue\n",
        "\n",
        "        future_time = last_date + timeframe_delta * (i + 1)\n",
        "\n",
        "        candle = {\n",
        "            \"Date\": future_time,\n",
        "            \"Open\": df_plot[\"Close\"].iloc[-1] if i == 0 else pred_lstm_dicts[i - 1][\"Close\"],\n",
        "            \"High\": pred[\"High\"],\n",
        "            \"Low\": pred[\"Low\"],\n",
        "            \"Close\": pred[\"Close\"],\n",
        "            \"Volume\": 0\n",
        "        }\n",
        "\n",
        "        df_plot = pd.concat([df_plot, pd.DataFrame([candle])], ignore_index=True)\n",
        "\n",
        "    df_plot.set_index(\"Date\", inplace=True)\n",
        "    df_plot.index = pd.to_datetime(df_plot.index)\n",
        "\n",
        "    # üé® Estilo visual do gr√°fico\n",
        "    mc = mpf.make_marketcolors(up='g', down='r', inherit=True)\n",
        "    s = mpf.make_mpf_style(marketcolors=mc, gridstyle=':', facecolor='white')\n",
        "\n",
        "    # üìà Plotar ou salvar\n",
        "    if save_path:\n",
        "        mpf.plot(df_plot, type='candle', style=s,\n",
        "                 title=f\"{title} ‚Äî {asset} ({timeframe})\",\n",
        "                 ylabel='Pre√ßo', volume=False, savefig=save_path)\n",
        "        print(f\"üíæ Gr√°fico salvo em: {save_path}\")\n",
        "    else:\n",
        "        mpf.plot(df_plot, type='candle', style=s,\n",
        "                 title=f\"{title} ‚Äî {asset} ({timeframe})\",\n",
        "                 ylabel='Pre√ßo', volume=False)\n",
        "\n",
        "\n",
        "\n",
        "def calculate_targets(price_row, signal, atr_multiplier=0.02, min_dist_percent=0.001):\n",
        "    \"\"\"\n",
        "    Calcula TP e SL com base em Low como entrada e High como alvo.\n",
        "    Usa atr_multiplier para margem no SL.\n",
        "    Se dist√¢ncia Entry-SL for muito pequena, invalida o trade.\n",
        "    \"\"\"\n",
        "    if signal == 1:  # Compra\n",
        "        entry = price_row.get(\"Low\", None)\n",
        "        tp1 = price_row.get(\"High\", None)\n",
        "        sl = entry - (entry * atr_multiplier) if entry is not None else None\n",
        "    elif signal == 0:  # Venda\n",
        "        entry = price_row.get(\"High\", None)\n",
        "        tp1 = price_row.get(\"Low\", None)\n",
        "        sl = entry + (entry * atr_multiplier) if entry is not None else None\n",
        "    else:\n",
        "        return {\"Entry\": None, \"TP1\": None, \"TP2\": None, \"SL\": None}\n",
        "\n",
        "    # üìã Prote√ß√µes de seguran√ßa\n",
        "    if any(v is None or np.isnan(v) for v in [entry, tp1, sl]):\n",
        "        print(\"‚ö†Ô∏è Targets inv√°lidos detectados ‚Äî retornando None.\")\n",
        "        return {\"Entry\": None, \"TP1\": None, \"TP2\": None, \"SL\": None}\n",
        "\n",
        "    # ‚ö° Verifica dist√¢ncia m√≠nima entre Entry e SL\n",
        "    if entry != 0 and abs(entry - sl) / entry < min_dist_percent:\n",
        "        print(f\"üö´ Trade descartado: dist√¢ncia Entry-SL muito pequena ({abs(entry - sl):.2f} | {abs(entry - sl) / entry:.4%})\")\n",
        "        return {\"Entry\": None, \"TP1\": None, \"TP2\": None, \"SL\": None}\n",
        "\n",
        "    # üõ°Ô∏è C√°lculo de TP2 baseado na dist√¢ncia\n",
        "    distancia = abs(entry - sl)\n",
        "    tp2 = entry + 2 * distancia if signal == 1 else entry - 2 * distancia\n",
        "\n",
        "    return {\n",
        "        \"Entry\": round(entry, 4),\n",
        "        \"TP1\": round(tp1, 4),\n",
        "        \"TP2\": round(tp2, 4),\n",
        "        \"SL\": round(sl, 4)\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def log_previsao_absurda(asset, interval, preco_atual, close_previsto):\n",
        "    try:\n",
        "        path = \"/content/previsoes_descartadas.csv\"\n",
        "        row = {\n",
        "            \"Data\": datetime.now(BR_TZ).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"Asset\": asset,\n",
        "            \"Timeframe\": interval,\n",
        "            \"PrecoAtual\": preco_atual,\n",
        "            \"PrevistoClose\": close_previsto,\n",
        "            \"Variacao(%)\": round((close_previsto - preco_atual) / preco_atual * 100, 2)\n",
        "        }\n",
        "        df = pd.DataFrame([row])\n",
        "        if os.path.exists(path):\n",
        "            df.to_csv(path, mode=\"a\", header=False, index=False)\n",
        "        else:\n",
        "            df.to_csv(path, index=False)\n",
        "        print(f\"üßæ Previs√£o absurda registrada em: {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Falha ao logar previs√£o absurda: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def send_telegram_message(message):\n",
        "    url = f\"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage\"\n",
        "    payload = {\"chat_id\": TELEGRAM_CHAT_ID, \"text\": message, \"parse_mode\": \"HTML\"}\n",
        "    response = requests.post(url, json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        print(\"üì® Mensagem enviada com sucesso!\")\n",
        "    else:\n",
        "        print(f\"‚ùå Erro ao enviar mensagem: {response.status_code} - {response.text}\")\n",
        "\n",
        "def predict_next_closes(data, n_steps=5):\n",
        "    df = data.copy().reset_index(drop=True)\n",
        "    features = get_feature_columns(df)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    X = df[features]\n",
        "    y = df[\"Close\"].shift(-1).dropna()\n",
        "    X = X.loc[y.index]\n",
        "\n",
        "    if len(X) < 100:\n",
        "        return [None] * n_steps\n",
        "\n",
        "    model = RandomForestRegressor(n_estimators=200, max_depth=8, random_state=42)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    last_row = df[features].iloc[-1].copy()\n",
        "    preds = []\n",
        "\n",
        "    for step in range(n_steps):\n",
        "        X_input = pd.DataFrame([last_row], columns=features)\n",
        "        next_close = model.predict(X_input)[0]\n",
        "        preds.append(round(next_close, 2))\n",
        "\n",
        "        # Simula avan√ßo do mercado\n",
        "        last_row[\"Close\"] = next_close\n",
        "        if \"SMA_50\" in last_row:\n",
        "            last_row[\"SMA_50\"] = last_row[\"SMA_50\"] * 0.9 + next_close * 0.1\n",
        "        if \"SMA_200\" in last_row:\n",
        "            last_row[\"SMA_200\"] = last_row[\"SMA_200\"] * 0.95 + next_close * 0.05\n",
        "        if \"VWAP\" in last_row:\n",
        "            last_row[\"VWAP\"] = last_row[\"VWAP\"] * 0.95 + next_close * 0.05\n",
        "        if \"RSI\" in last_row:\n",
        "            last_row[\"RSI\"] = min(100, max(0, last_row[\"RSI\"] + np.random.normal(0, 0.5)))\n",
        "        if \"MACD\" in last_row:\n",
        "            last_row[\"MACD\"] += np.random.normal(0, 0.3)\n",
        "        if \"MACD_Signal\" in last_row:\n",
        "            last_row[\"MACD_Signal\"] += np.random.normal(0, 0.2)\n",
        "\n",
        "        last_row = last_row[features]\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "def evaluate_past_predictions(results_file=\"/content/prediction_log.csv\", lookahead_candles=5):\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    import yfinance as yf\n",
        "    import matplotlib.pyplot as plt\n",
        "    from datetime import timedelta\n",
        "\n",
        "    df = safe_read_csv(results_file)\n",
        "    if df is None or df.empty:\n",
        "        print(\"üì≠ Nenhum log de previs√£o encontrado ou o arquivo est√° vazio.\")\n",
        "        return\n",
        "\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "    print(f\"üìä Avaliando {len(df)} previs√µes salvas...\")\n",
        "\n",
        "    evaluation = []\n",
        "\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        asset = row[\"Asset\"]\n",
        "        interval = row[\"Timeframe\"]\n",
        "        prediction_time = row[\"Date\"]\n",
        "        predicted_signal = row[\"Signal\"]\n",
        "        predicted_target = row.get(\"TargetPrice\", None)\n",
        "\n",
        "        try:\n",
        "            candles = yf.download(asset, start=prediction_time, interval=interval, progress=False)\n",
        "            candles = candles[candles.index > prediction_time]\n",
        "\n",
        "            if candles.empty or len(candles) < lookahead_candles:\n",
        "                continue\n",
        "\n",
        "            candles = candles.head(lookahead_candles)\n",
        "            final_close = candles[\"Close\"].iloc[-1]\n",
        "\n",
        "            if predicted_signal == 1:\n",
        "                result = \"Acertou\" if final_close >= predicted_target else \"Errou\"\n",
        "            elif predicted_signal == 0:\n",
        "                result = \"Acertou\" if final_close <= predicted_target else \"Errou\"\n",
        "            else:\n",
        "                result = \"Neutro\"\n",
        "\n",
        "            if predicted_target:\n",
        "                perc_change = ((final_close - predicted_target) / predicted_target) * 100\n",
        "                abs_error = final_close - predicted_target\n",
        "            else:\n",
        "                perc_change = None\n",
        "                abs_error = None\n",
        "\n",
        "            acertou = 1 if result == \"Acertou\" else 0\n",
        "\n",
        "            evaluation.append({\n",
        "                \"Ativo\": asset,\n",
        "                \"Timeframe\": interval,\n",
        "                \"Data Previs√£o\": prediction_time.strftime(\"%Y-%m-%d %H:%M\"),\n",
        "                \"Sinal Previsto\": \"Compra\" if predicted_signal == 1 else \"Venda\" if predicted_signal == 0 else \"Neutro\",\n",
        "                \"Valor Projetado (LSTM)\": round(predicted_target, 2) if predicted_target else None,\n",
        "                \"Resultado\": result,\n",
        "                \"Valor Real\": round(final_close, 2),\n",
        "                \"Varia√ß√£o Real\": f\"{perc_change:+.2f}%\" if perc_change is not None else \"N/A\",\n",
        "                \"Erro Absoluto\": f\"{abs_error:+.2f}\" if abs_error is not None else \"N/A\",\n",
        "                \"Acertou\": acertou\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erro ao avaliar {asset} em {prediction_time}: {e}\")\n",
        "            continue\n",
        "\n",
        "    df_eval = pd.DataFrame(evaluation)\n",
        "\n",
        "    # üìä Resumo de acertos e erros\n",
        "    resumo = df_eval.groupby([\"Ativo\", \"Timeframe\", \"Resultado\"]).size().unstack(fill_value=0)\n",
        "    resumo[\"Total\"] = resumo.sum(axis=1)\n",
        "    resumo[\"Acur√°cia (%)\"] = (resumo.get(\"Acertou\", 0) / resumo[\"Total\"] * 100).round(2)\n",
        "    display(resumo)\n",
        "\n",
        "    # üìà Gr√°fico de barras\n",
        "    resumo_plot = resumo[[\"Acertou\", \"Errou\"]] if \"Errou\" in resumo.columns else resumo[[\"Acertou\"]]\n",
        "    resumo_plot.plot(kind=\"bar\", figsize=(10, 5), title=\"üìä Acertos vs Erros por Ativo e Timeframe\")\n",
        "    plt.ylabel(\"Quantidade de Sinais\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis=\"y\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # üìÑ Tabela completa das previs√µes\n",
        "    display(df_eval)\n",
        "\n",
        "    # üîÑ Atualizar o prediction_log.csv com a coluna 'Acertou'\n",
        "    try:\n",
        "        df_log = safe_read_csv(results_file)\n",
        "        df_log[\"Date\"] = pd.to_datetime(df_log[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "\n",
        "\n",
        "        for _, row in df_eval.iterrows():\n",
        "            dt = pd.to_datetime(row[\"Data Previs√£o\"])\n",
        "            mask = (df_log[\"Date\"] == dt) & (df_log[\"Asset\"] == row[\"Ativo\"]) & (df_log[\"Timeframe\"] == row[\"Timeframe\"])\n",
        "            df_log.loc[mask, \"Acertou\"] = row[\"Acertou\"]\n",
        "\n",
        "        df_log.to_csv(results_file, index=False)\n",
        "        print(\"‚úÖ Log de previs√µes atualizado com coluna 'Acertou'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao atualizar o prediction_log.csv com 'Acertou': {e}\")\n",
        "\n",
        "    return df_eval\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def clear_models(model_dir=\"/content/models\"):\n",
        "    import shutil\n",
        "\n",
        "    if os.path.exists(model_dir):\n",
        "        print(f\"üßπ Limpando todos os modelos salvos em: {model_dir}\")\n",
        "        shutil.rmtree(model_dir)\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        print(\"‚úÖ Modelos deletados com sucesso.\")\n",
        "    else:\n",
        "        print(\"üìÇ Nenhuma pasta de modelos encontrada para limpar.\")\n",
        "\n",
        "\n",
        "\n",
        "def plot_prediction_performance_por_timeframe(log_path=\"/content/prediction_log.csv\"):\n",
        "    if not os.path.exists(log_path):\n",
        "        print(\"üì≠ Nenhum log encontrado.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(log_path)\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "    df = df.dropna(subset=[\"TargetPrice\", \"Price\", \"Timeframe\"])\n",
        "\n",
        "    for timeframe in df[\"Timeframe\"].unique():\n",
        "        df_tf = df[df[\"Timeframe\"] == timeframe].copy()\n",
        "        df_tf[\"Erro\"] = df_tf[\"Price\"] - df_tf[\"TargetPrice\"]\n",
        "        df_tf[\"AbsError\"] = abs(df_tf[\"Erro\"])\n",
        "        df_tf[\"Dia\"] = df_tf[\"Date\"].dt.date\n",
        "\n",
        "        if df_tf.empty:\n",
        "            continue\n",
        "\n",
        "        # Erro absoluto m√©dio por dia\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        df_grouped = df_tf.groupby(\"Dia\")[\"AbsError\"].mean()\n",
        "        plt.plot(df_grouped.index, df_grouped.values, marker=\"o\")\n",
        "        plt.title(f\"üìà Erro Absoluto M√©dio por Dia - {timeframe}\")\n",
        "        plt.xlabel(\"Data\")\n",
        "        plt.ylabel(\"Erro ($)\")\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"/tmp/erro_absoluto_{timeframe}.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Dispers√£o do valor previsto x real\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.scatter(df_tf[\"TargetPrice\"], df_tf[\"Price\"], alpha=0.6)\n",
        "        plt.plot([df_tf[\"TargetPrice\"].min(), df_tf[\"TargetPrice\"].max()],\n",
        "                [df_tf[\"TargetPrice\"].min(), df_tf[\"TargetPrice\"].max()], 'r--', label=\"Perfeito\")\n",
        "        plt.title(f\"üéØ Previs√£o LSTM vs Pre√ßo Real - {timeframe}\")\n",
        "        plt.xlabel(\"Valor Previsto\")\n",
        "        plt.ylabel(\"Valor Real\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        path_img = f\"/tmp/previsao_vs_real_{timeframe}.png\"\n",
        "        plt.savefig(path_img)\n",
        "        plt.close()\n",
        "        print(f\"‚úÖ Gr√°fico salvo: {path_img}\")\n",
        "\n",
        "def enviar_graficos_desempenho_por_timeframe():\n",
        "    import glob\n",
        "    from pathlib import Path\n",
        "\n",
        "    timeframes = [\"15m\", \"1h\", \"1d\"]  # Edite se tiver outros\n",
        "    path_base = \"/tmp\"\n",
        "\n",
        "    for tf in timeframes:\n",
        "        # Gr√°fico 1: Previs√£o vs Real\n",
        "        grafico_pred = f\"{path_base}/previsao_vs_real_{tf}.png\"\n",
        "        if os.path.exists(grafico_pred):\n",
        "            with open(grafico_pred, \"rb\") as img:\n",
        "                url = f\"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendPhoto\"\n",
        "                files = {\"photo\": img}\n",
        "                data = {\n",
        "                    \"chat_id\": TELEGRAM_CHAT_ID,\n",
        "                    \"caption\": f\"üìà Previs√£o LSTM vs Real ‚Äî {tf}\"\n",
        "                }\n",
        "                r = requests.post(url, data=data, files=files)\n",
        "                print(f\"‚úÖ Enviado: previsao_vs_real_{tf}.png\")\n",
        "\n",
        "        # Gr√°fico 2: Erro absoluto por dia\n",
        "        grafico_erro = f\"{path_base}/erro_absoluto_{tf}.png\"\n",
        "        if os.path.exists(grafico_erro):\n",
        "            with open(grafico_erro, \"rb\") as img:\n",
        "                url = f\"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendPhoto\"\n",
        "                files = {\"photo\": img}\n",
        "                data = {\n",
        "                    \"chat_id\": TELEGRAM_CHAT_ID,\n",
        "                    \"caption\": f\"üìä Erro Absoluto por Dia ‚Äî {tf}\"\n",
        "                }\n",
        "                r = requests.post(url, data=data, files=files)\n",
        "                print(f\"‚úÖ Enviado: erro_absoluto_{tf}.png\")\n",
        "\n",
        "def enviar_grafico_lucro_por_confianca(log_path=\"/content/prediction_log.csv\"):\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    if not os.path.exists(log_path):\n",
        "        print(\"üì≠ Nenhum log encontrado.\")\n",
        "        return\n",
        "\n",
        "    df = safe_read_csv(log_path)\n",
        "    if \"AdjustedProb\" not in df.columns or \"TP1\" not in df.columns or \"Price\" not in df.columns:\n",
        "        print(\"‚ö†Ô∏è Colunas necess√°rias n√£o encontradas no log.\")\n",
        "        return\n",
        "\n",
        "    df = df.dropna(subset=[\"AdjustedProb\", \"TP1\", \"Price\"])\n",
        "    df[\"LucroEstimado\"] = df[\"TP1\"] - df[\"Price\"]\n",
        "    df[\"FaixaConfian√ßa\"] = pd.cut(df[\"AdjustedProb\"], bins=[0, 0.6, 0.7, 0.8, 0.9, 1.0], labels=[\"‚â§60%\", \"60-70%\", \"70-80%\", \"80-90%\", \">90%\"])\n",
        "\n",
        "    lucro_medio = df.groupby(\"FaixaConfian√ßa\")[\"LucroEstimado\"].mean()\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    lucro_medio.plot(kind=\"bar\", color=\"skyblue\")\n",
        "    plt.title(\"üìä Lucro Estimado M√©dio por Faixa de Confian√ßa\")\n",
        "    plt.ylabel(\"Lucro Estimado ($)\")\n",
        "    plt.xlabel(\"Faixa de Confian√ßa Ajustada\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    path = \"/tmp/lucro_por_confianca.png\"\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "    with open(path, \"rb\") as img:\n",
        "        url = f\"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendPhoto\"\n",
        "        files = {\"photo\": img}\n",
        "        data = {\n",
        "            \"chat_id\": TELEGRAM_CHAT_ID,\n",
        "            \"caption\": \"üìä Lucro m√©dio estimado por faixa de confian√ßa ajustada\"\n",
        "        }\n",
        "        response = requests.post(url, data=data, files=files)\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ Gr√°fico de lucro por confian√ßa enviado.\")\n",
        "        else:\n",
        "            print(f\"‚ùå Falha ao enviar gr√°fico: {response.status_code} - {response.text}\")\n",
        "\n",
        "def adjust_signal_based_on_history(asset, timeframe, max_lookback=20, min_signals=5):\n",
        "    try:\n",
        "        df = safe_read_csv(\"prediction_log.csv\")\n",
        "        if df is None:\n",
        "            print(\"‚ö†Ô∏è Ignorando leitura do prediction_log.csv pois est√° vazio ou ausente.\")\n",
        "            return 1.0  # Retorna confian√ßa padr√£o\n",
        "\n",
        "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "\n",
        "        df = df[(df[\"Asset\"] == asset) & (df[\"Timeframe\"] == timeframe)]\n",
        "\n",
        "        if len(df) < min_signals or \"Acertou\" not in df.columns:\n",
        "            return 1.0\n",
        "\n",
        "        recent = df.sort_values(\"Date\", ascending=False).head(max_lookback)\n",
        "        acuracia = recent[\"Acertou\"].mean()\n",
        "        return acuracia\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erro ao ajustar com hist√≥rico: {e}\")\n",
        "        return 1.0\n",
        "\n",
        "def gerar_grafico_previsao_vs_real(log_path=\"/content/prediction_log.csv\", output_path=\"/tmp/previsao_vs_real.png\"):\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    df = safe_read_csv(log_path)\n",
        "    if df is None or df.empty or \"TargetPrice\" not in df.columns or \"Price\" not in df.columns:\n",
        "        print(\"‚ö†Ô∏è Log inv√°lido ou colunas ausentes.\")\n",
        "        return None\n",
        "\n",
        "    df = df.dropna(subset=[\"TargetPrice\", \"Price\"]).tail(20)  # √∫ltimos 20 sinais\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(df[\"Date\"], df[\"Price\"], label=\"üìà Pre√ßo Real\", marker=\"o\")\n",
        "    plt.plot(df[\"Date\"], df[\"TargetPrice\"], label=\"üîÆ Previs√£o LSTM\", marker=\"x\")\n",
        "    plt.title(\"üìä Previs√£o LSTM vs Pre√ßo Real\")\n",
        "    plt.xlabel(\"Data\")\n",
        "    plt.ylabel(\"Pre√ßo\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "    print(f\"‚úÖ Gr√°fico salvo em: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "def enviar_grafico_previsao_real(df, timeframe, asset):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.dates as mdates\n",
        "    import os\n",
        "\n",
        "    df = df[df[\"Asset\"] == asset].copy()\n",
        "    if df.empty:\n",
        "        print(f\"‚ö†Ô∏è Nenhum dado para {asset} ({timeframe}) no gr√°fico.\")\n",
        "        return\n",
        "\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        date = df[\"Date\"].iloc[i]\n",
        "        high = df[\"LSTM_High_Predicted\"].iloc[i]\n",
        "        low = df[\"LSTM_Low_Predicted\"].iloc[i]\n",
        "        close = df[\"TargetPrice\"].iloc[i]\n",
        "\n",
        "        cor = \"blue\"\n",
        "        plt.vlines(date, ymin=low, ymax=high, color=cor, linewidth=2)\n",
        "        plt.plot(date, close, marker=\"o\", color=cor)\n",
        "\n",
        "    plt.plot(df[\"Date\"], df[\"Price\"], label=\"üìà Pre√ßo Real\", marker=\"x\", color=\"black\")\n",
        "    plt.title(f\"üìä Proje√ß√£o LSTM (High/Low/Close) ‚Äî {asset} ({timeframe})\")\n",
        "    plt.xlabel(\"Data\")\n",
        "    plt.ylabel(\"Pre√ßo\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d/%m %H:%M', tz=BR_TZ))\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    path = f\"/tmp/previsao_vs_real_{asset.replace('-', '')}_{timeframe}.png\"\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        with open(path, \"rb\") as img:\n",
        "            url = f\"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendPhoto\"\n",
        "            files = {\"photo\": img}\n",
        "            data = {\n",
        "                \"chat_id\": TELEGRAM_CHAT_ID,\n",
        "                \"caption\": f\"üìä Proje√ß√£o LSTM ‚Äî {asset} ({timeframe})\"\n",
        "            }\n",
        "            response = requests.post(url, data=data, files=files)\n",
        "            if response.status_code == 200:\n",
        "                print(\"‚úÖ Gr√°fico enviado ao Telegram.\")\n",
        "            else:\n",
        "                print(f\"‚ùå Erro ao enviar gr√°fico: {response.status_code} - {response.text}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def enviar_grafico_carteira():\n",
        "    image_path = \"/tmp/evolucao_carteira.png\"\n",
        "    if os.path.exists(image_path):\n",
        "        with open(image_path, \"rb\") as img:\n",
        "            url = f\"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendPhoto\"\n",
        "            files = {\"photo\": img}\n",
        "            data = {\n",
        "                \"chat_id\": TELEGRAM_CHAT_ID,\n",
        "                \"caption\": \"üíº Evolu√ß√£o da carteira virtual com base nos sinais do bot\"\n",
        "            }\n",
        "            response = requests.post(url, data=data, files=files)\n",
        "            if response.status_code == 200:\n",
        "                print(\"‚úÖ Gr√°fico da carteira enviado ao Telegram.\")\n",
        "            else:\n",
        "                print(f\"‚ùå Erro ao enviar imagem: {response.status_code} - {response.text}\")\n",
        "\n",
        "# üìä C√°lculo autom√°tico do atr_multiplier baseado nos √∫ltimos candles\n",
        "def calcular_atr_auto(dataframe, intervalo=\"15m\", n=50, fator_ajuste=1.2):\n",
        "    try:\n",
        "        df_recent = dataframe.tail(n)\n",
        "        if df_recent.empty or not all(col in df_recent.columns for col in [\"High\", \"Low\", \"Close\"]):\n",
        "            return 0.03  # valor padr√£o caso n√£o tenha dados\n",
        "\n",
        "        # C√°lculo da m√©dia da varia√ß√£o percentual entre High e Low\n",
        "        media_range_pct = ((df_recent[\"High\"] - df_recent[\"Low\"]) / df_recent[\"Close\"]).mean()\n",
        "        atr_multiplier = round(media_range_pct * fator_ajuste, 4)\n",
        "\n",
        "        # Valor m√≠nimo e m√°ximo para manter limites razo√°veis\n",
        "        atr_multiplier = max(0.01, min(atr_multiplier, 0.08))\n",
        "        return atr_multiplier\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erro no ajuste autom√°tico do ATR: {e}\")\n",
        "        return 0.03  # fallback padr√£o\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 5.1 CARTEIRA VIRTUAL PARA SIMULA√á√ÉO\n",
        "# ====================================================\n",
        "# ====================================================\n",
        "# 5.1 CARTEIRA VIRTUAL PARA SIMULA√á√ÉO\n",
        "# ====================================================\n",
        "\n",
        "carteira_virtual = {\n",
        "    \"capital_inicial\": 10000.0,\n",
        "    \"capital_atual\": 10000.0,\n",
        "    \"capital_maximo\": 10000.0,  # para c√°lculo de drawdown\n",
        "    \"historico_capital\": [],    # track evolu√ß√£o do capital\n",
        "    \"em_operacao\": False,\n",
        "}\n",
        "\n",
        "\n",
        "def to_scalar(val):\n",
        "    try:\n",
        "        if isinstance(val, pd.Series):\n",
        "            return float(val.iloc[0])\n",
        "        elif isinstance(val, (np.ndarray, list)):\n",
        "            return float(val[0])\n",
        "        elif val is None:\n",
        "            return np.nan\n",
        "        else:\n",
        "            return float(val)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Falha ao converter valor escalar: {val} | erro: {e}\")\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "def salvar_carteira_virtual(filepath=\"/content/carteira_virtual.json\"):\n",
        "    with open(filepath, \"w\") as f:\n",
        "        json.dump(carteira_virtual, f)\n",
        "    print(f\"üíæ Carteira virtual salva em: {filepath}\")\n",
        "\n",
        "\n",
        "def carregar_carteira_virtual(filepath=\"/content/carteira_virtual.json\"):\n",
        "    global carteira_virtual\n",
        "    if os.path.exists(filepath):\n",
        "        with open(filepath, \"r\") as f:\n",
        "            carteira_virtual = json.load(f)\n",
        "        print(f\"üìÇ Carteira virtual carregada de: {filepath}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Arquivo de carteira n√£o encontrado. Usando valores padr√µes.\")\n",
        "\n",
        "def exibir_status_carteira():\n",
        "    ci = carteira_virtual.get(\"capital_inicial\", 10000.0)\n",
        "    ca = carteira_virtual.get(\"capital_atual\", 10000.0)\n",
        "    cm = carteira_virtual.get(\"capital_maximo\", ca)\n",
        "\n",
        "    roi = ((ca / ci) - 1) * 100\n",
        "    drawdown = (1 - (ca / cm)) * 100\n",
        "\n",
        "    print(\"\\nüí∞ Status da Carteira Virtual:\")\n",
        "    print(f\"‚Ä¢ Capital Inicial: ${ci:,.2f}\")\n",
        "    print(f\"‚Ä¢ Capital Atual  : ${ca:,.2f}\")\n",
        "    print(f\"‚Ä¢ ROI Acumulado  : {roi:+.2f}%\")\n",
        "    print(f\"‚Ä¢ Drawdown Atual : {drawdown:.2f}%\\n\")\n",
        "\n",
        "def simular_trade(row, df):\n",
        "    try:\n",
        "        asset = row[\"Asset\"]\n",
        "        timeframe = row[\"Timeframe\"]\n",
        "        signal_time = pd.to_datetime(row[\"Date\"], utc=True).astimezone(BR_TZ)\n",
        "\n",
        "        preco_entrada = float(row[\"Entry\"])\n",
        "        tp1 = float(row[\"TP1\"])\n",
        "        sl = float(row[\"SL\"])\n",
        "\n",
        "        if df.index.tz is None:\n",
        "            df.index = df.index.tz_localize(pytz.UTC).tz_convert(BR_TZ)\n",
        "        else:\n",
        "            df.index = df.index.tz_convert(BR_TZ)\n",
        "\n",
        "        df_future = df[df.index > signal_time]\n",
        "        if df_future.empty or not all(col in df_future.columns for col in [\"High\", \"Low\", \"Close\"]):\n",
        "            raise ValueError(\"Candles futuros indispon√≠veis ou incompletos.\")\n",
        "\n",
        "        # üü° Taxas e slippage\n",
        "        taxa_percentual = 0.001  # 0.1% por opera√ß√£o\n",
        "        slippage_percentual = 0.002  # 0.2% por opera√ß√£o\n",
        "\n",
        "        entrada_executada = False\n",
        "        for i, (idx, candle) in enumerate(df_future.iterrows()):\n",
        "            high = float(candle[\"High\"])\n",
        "            low = float(candle[\"Low\"])\n",
        "\n",
        "            if not entrada_executada:\n",
        "                if row[\"Signal\"] == 1 and low <= preco_entrada:\n",
        "                    preco_real_entrada = low\n",
        "                    entrada_executada = True\n",
        "                    entrada_idx = idx\n",
        "                elif row[\"Signal\"] == 0 and high >= preco_entrada:\n",
        "                    preco_real_entrada = high\n",
        "                    entrada_executada = True\n",
        "                    entrada_idx = idx\n",
        "                continue\n",
        "\n",
        "            if entrada_executada:\n",
        "                preco_max = float(candle[\"High\"])\n",
        "                preco_min = float(candle[\"Low\"])\n",
        "\n",
        "                if row[\"Signal\"] == 1:\n",
        "                    if preco_min <= sl:\n",
        "                        resultado = \"SL\"\n",
        "                        preco_saida = sl\n",
        "                        break\n",
        "                    elif preco_max >= tp1:\n",
        "                        resultado = \"TP1\"\n",
        "                        preco_saida = tp1\n",
        "                        break\n",
        "                elif row[\"Signal\"] == 0:\n",
        "                    if preco_max >= sl:\n",
        "                        resultado = \"SL\"\n",
        "                        preco_saida = sl\n",
        "                        break\n",
        "                    elif preco_min <= tp1:\n",
        "                        resultado = \"TP1\"\n",
        "                        preco_saida = tp1\n",
        "                        break\n",
        "\n",
        "        else:\n",
        "            if entrada_executada:\n",
        "                resultado = \"Sem alvo\"\n",
        "                preco_saida = df_future[\"Close\"].iloc[-1]\n",
        "            else:\n",
        "                return {\n",
        "                    \"Resultado\": \"Sem execu√ß√£o\",\n",
        "                    \"PrecoSaida\": None,\n",
        "                    \"LucroEstimado\": None,\n",
        "                    \"DuracaoMin\": None,\n",
        "                    \"Capital Atual\": carteira_virtual[\"capital_atual\"],\n",
        "                    \"Quantidade\": None,\n",
        "                    \"ROI\": None,\n",
        "                    \"Drawdown\": None\n",
        "                }\n",
        "\n",
        "        capital_disponivel = carteira_virtual[\"capital_atual\"]\n",
        "        risco_por_trade = 0.01\n",
        "        risco_trade = abs(preco_real_entrada - sl)\n",
        "\n",
        "        if risco_trade <= 0:\n",
        "            capital_por_trade = capital_disponivel * 0.01\n",
        "        else:\n",
        "            capital_por_trade = (capital_disponivel * risco_por_trade) / risco_trade\n",
        "\n",
        "        quantidade = capital_por_trade\n",
        "        if quantidade * preco_real_entrada > capital_disponivel * 0.10:\n",
        "            quantidade = (capital_disponivel * 0.10) / preco_real_entrada\n",
        "\n",
        "        quantidade = max(quantidade, 0.0001)\n",
        "\n",
        "        # üî• C√°lculo de lucro com taxa + slippage\n",
        "        if row[\"Signal\"] == 1:\n",
        "            lucro_total = (preco_saida - preco_real_entrada) * quantidade\n",
        "        else:\n",
        "            lucro_total = (preco_real_entrada - preco_saida) * quantidade\n",
        "\n",
        "        custo_total = (preco_real_entrada + preco_saida) * quantidade * (taxa_percentual + slippage_percentual)\n",
        "        lucro_total -= custo_total\n",
        "\n",
        "        carteira_virtual[\"capital_atual\"] += lucro_total\n",
        "        carteira_virtual[\"historico_capital\"].append(carteira_virtual[\"capital_atual\"])\n",
        "\n",
        "        if carteira_virtual[\"capital_atual\"] > carteira_virtual[\"capital_maximo\"]:\n",
        "            carteira_virtual[\"capital_maximo\"] = carteira_virtual[\"capital_atual\"]\n",
        "\n",
        "        drawdown = 1 - (carteira_virtual[\"capital_atual\"] / carteira_virtual[\"capital_maximo\"])\n",
        "        roi = (carteira_virtual[\"capital_atual\"] / carteira_virtual[\"capital_inicial\"]) - 1\n",
        "        duracao = (idx - entrada_idx).total_seconds() / 60 if entrada_executada else None\n",
        "\n",
        "        return {\n",
        "            \"Resultado\": resultado,\n",
        "            \"PrecoSaida\": preco_saida,\n",
        "            \"LucroEstimado\": round(lucro_total, 2),\n",
        "            \"DuracaoMin\": round(duracao, 1) if duracao is not None else None,\n",
        "            \"Capital Atual\": round(carteira_virtual[\"capital_atual\"], 2),\n",
        "            \"Quantidade\": round(quantidade, 6),\n",
        "            \"ROI\": round(roi * 100, 2),\n",
        "            \"Drawdown\": round(drawdown * 100, 2)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro inesperado na simula√ß√£o: {e}\")\n",
        "        return {\n",
        "            \"Resultado\": \"Erro\",\n",
        "            \"PrecoSaida\": None,\n",
        "            \"LucroEstimado\": None,\n",
        "            \"DuracaoMin\": None,\n",
        "            \"Capital Atual\": carteira_virtual[\"capital_atual\"],\n",
        "            \"Quantidade\": None,\n",
        "            \"ROI\": None,\n",
        "            \"Drawdown\": None\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "def plotar_grafico_previsao_futura(df_previsao, timeframe, asset):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.dates as mdates\n",
        "\n",
        "    if df_previsao is None or not all(k in df_previsao for k in [\"Date\", \"High\", \"Low\", \"Close\"]):\n",
        "        print(f\"‚ö†Ô∏è Dados de previs√£o futura incompletos para {asset} ({timeframe})\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(df_previsao)\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        date = df[\"Date\"].iloc[i]\n",
        "        high = df[\"High\"].iloc[i]\n",
        "        low = df[\"Low\"].iloc[i]\n",
        "        close = df[\"Close\"].iloc[i]\n",
        "\n",
        "        plt.vlines(date, ymin=low, ymax=high, color=\"blue\", linewidth=2)\n",
        "        plt.plot(date, close, marker=\"o\", color=\"blue\")\n",
        "        plt.annotate(f\"{close:.0f}\", (date, close), xytext=(0, 8),\n",
        "                     textcoords=\"offset points\", ha='center', fontsize=8, color=\"blue\")\n",
        "\n",
        "    plt.title(f\"üîÆ Proje√ß√£o Futura (LSTM) ‚Äî {asset} ({timeframe})\")\n",
        "    plt.xlabel(\"Data\")\n",
        "    plt.ylabel(\"Pre√ßo Projetado\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d/%m %H:%M', tz=BR_TZ))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "def plotar_grafico_previsao_real(df, timeframe, asset):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.dates as mdates\n",
        "\n",
        "    df = df[df[\"Asset\"] == asset].copy()\n",
        "    if df.empty:\n",
        "        print(f\"‚ö†Ô∏è Nenhum dado para {asset} ({timeframe}) no gr√°fico.\")\n",
        "        return\n",
        "\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        date = df[\"Date\"].iloc[i]\n",
        "        high = df[\"LSTM_High_Predicted\"].iloc[i]\n",
        "        low = df[\"LSTM_Low_Predicted\"].iloc[i]\n",
        "        close = df[\"TargetPrice\"].iloc[i]\n",
        "        real = df[\"Price\"].iloc[i]\n",
        "\n",
        "        cor = \"green\" if close >= real else \"red\"\n",
        "        plt.vlines(date, ymin=low, ymax=high, color=cor, linewidth=2)\n",
        "        plt.plot(date, close, marker=\"o\", color=cor)\n",
        "\n",
        "        plt.annotate(f\"{close:.0f}\", (date, close), xytext=(0, 8),\n",
        "                     textcoords=\"offset points\", ha='center', fontsize=8, color=cor)\n",
        "        plt.annotate(f\"{real:.0f}\", (date, real), xytext=(0, -12),\n",
        "                     textcoords=\"offset points\", ha='center', fontsize=8, color=\"black\")\n",
        "\n",
        "    plt.plot(df[\"Date\"], df[\"Price\"], label=\"üìà Pre√ßo Real\", marker=\"x\", color=\"black\")\n",
        "    plt.title(f\"üìä Proje√ß√£o LSTM (High/Low/Close) ‚Äî {asset} ({timeframe})\")\n",
        "    plt.xlabel(\"Data\")\n",
        "    plt.ylabel(\"Pre√ßo\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d/%m %H:%M', tz=BR_TZ))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plotar_grafico_carteira_virtual(log_path=\"/content/prediction_log.csv\"):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import os\n",
        "\n",
        "    if not os.path.exists(log_path):\n",
        "        print(\"‚ùå Arquivo de log n√£o encontrado.\")\n",
        "        return\n",
        "\n",
        "    df = safe_read_csv(log_path)\n",
        "    if df is None or df.empty:\n",
        "        print(\"‚ö†Ô∏è Log de previs√µes vazio ou inv√°lido.\")\n",
        "        return\n",
        "\n",
        "    df = df.dropna(subset=[\"Date\", \"Capital Atual\", \"Resultado\"])\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "    df = df[df[\"Resultado\"].isin([\"TP1\", \"SL\", \"Sem alvo\"])]\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"‚ö†Ô∏è Nenhuma simula√ß√£o v√°lida para exibir no gr√°fico.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    cor_map = {\"TP1\": \"green\", \"SL\": \"red\", \"Sem alvo\": \"orange\"}\n",
        "    cores = df[\"Resultado\"].map(cor_map).fillna(\"gray\")\n",
        "\n",
        "    plt.scatter(df[\"Date\"], df[\"Capital Atual\"], c=cores, edgecolors=\"black\", s=70)\n",
        "    plt.plot(df[\"Date\"], df[\"Capital Atual\"], linestyle=\"--\", color=\"blue\", alpha=0.7)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        plt.annotate(f\"${row['Capital Atual']:.0f}\", (row[\"Date\"], row[\"Capital Atual\"]),\n",
        "                    textcoords=\"offset points\", xytext=(0, 6), ha='center', fontsize=8)\n",
        "\n",
        "    plt.title(\"üí∞ Evolu√ß√£o da Carteira Virtual\")\n",
        "    plt.xlabel(\"Data (BR)\")\n",
        "    plt.ylabel(\"Capital ($)\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plotar_grafico_lucro(df):\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    df_valid = df[df[\"Resultado\"].isin([\"TP1\", \"SL\", \"Sem alvo\"])].copy()\n",
        "    if df_valid.empty:\n",
        "        print(\"‚ö†Ô∏è Nenhum resultado v√°lido para gr√°fico.\")\n",
        "        return\n",
        "\n",
        "    df_valid[\"FaixaConfian√ßa\"] = pd.cut(\n",
        "        df_valid[\"AdjustedProb\"].fillna(0.5),\n",
        "        bins=[0, 0.6, 0.75, 0.9, 1.01],\n",
        "        labels=[\"<60%\", \"60-75%\", \"75-90%\", \">90%\"]\n",
        "    )\n",
        "\n",
        "    lucro_medio = df_valid.groupby(\"FaixaConfian√ßa\")[\"LucroEstimado\"].mean()\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    lucro_medio.plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n",
        "    plt.title(\"üìä Lucro M√©dio por Faixa de Confian√ßa\")\n",
        "    plt.ylabel(\"Lucro Estimado\")\n",
        "    plt.xlabel(\"Faixa de Confian√ßa\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    path = \"lucro_por_faixa.png\"\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "    # ‚úÖ ADICIONE ISTO para mostrar o gr√°fico no log tamb√©m\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    lucro_medio.plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n",
        "    plt.title(\"üìä Lucro M√©dio por Faixa de Confian√ßa (visualiza√ß√£o)\")\n",
        "    plt.ylabel(\"Lucro Estimado\")\n",
        "    plt.xlabel(\"Faixa de Confian√ßa\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"‚úÖ Gr√°fico de lucro por confian√ßa enviado.\")\n",
        "\n",
        "\n",
        "\n",
        "def simular_todos_trades(prediction_log_path=\"prediction_log.csv\", df_candles=None, timeframe=\"15m\"):\n",
        "    print(\"üìä Rodando simula√ß√£o de carteira virtual com sinais do log...\")\n",
        "\n",
        "    if not os.path.exists(prediction_log_path):\n",
        "        print(\"‚ö†Ô∏è Log de previs√µes n√£o encontrado.\")\n",
        "        return\n",
        "\n",
        "    df_log = safe_read_csv(prediction_log_path)\n",
        "    if df_log is None or df_log.empty:\n",
        "        print(\"‚ö†Ô∏è Log vazio.\")\n",
        "        return\n",
        "\n",
        "    df_log[\"Date\"] = pd.to_datetime(df_log[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "\n",
        "    # Define o tempo necess√°rio para permitir simula√ß√£o, baseado no timeframe\n",
        "    intervalo_futuro = {\n",
        "        \"15m\": timedelta(minutes=15 * 5),\n",
        "        \"1h\": timedelta(hours=5),\n",
        "        \"4h\": timedelta(hours=20),\n",
        "        \"1d\": timedelta(days=5),\n",
        "        \"1wk\": timedelta(weeks=5)\n",
        "    }.get(timeframe, timedelta(hours=1))\n",
        "\n",
        "    now = datetime.now(BR_TZ)\n",
        "    resultados = []\n",
        "\n",
        "    for _, row in df_log.iterrows():\n",
        "        signal_time = pd.to_datetime(row[\"Date\"], utc=True).tz_convert(BR_TZ)\n",
        "\n",
        "        if (now - signal_time) < intervalo_futuro:\n",
        "            continue  # sinal ainda recente, ignorar\n",
        "\n",
        "        try:\n",
        "            resultado = simular_trade(row, df_candles)\n",
        "            for key, value in resultado.items():\n",
        "                row[key] = value\n",
        "            resultados.append(row)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erro inesperado na simula√ß√£o: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not resultados:\n",
        "        print(\"üì≠ Nenhum trade foi simulado (ainda).\")\n",
        "        return\n",
        "\n",
        "    df_resultados = pd.DataFrame(resultados)\n",
        "    df_resultados.to_csv(prediction_log_path, index=False)\n",
        "    salvar_carteira_virtual()\n",
        "\n",
        "    print(f\"üìã Log de previs√µes atualizado com resultados e capital: {prediction_log_path}\")\n",
        "    plotar_grafico_lucro(df_resultados)\n",
        "    salvar_grafico_evolucao()\n",
        "\n",
        "\n",
        "\n",
        "def salvar_grafico_evolucao(log_path=\"prediction_log.csv\"):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import os\n",
        "\n",
        "    if not os.path.exists(log_path):\n",
        "        print(\"‚ùå Arquivo de log n√£o encontrado.\")\n",
        "        return\n",
        "\n",
        "    df = safe_read_csv(log_path)\n",
        "    if df is None or df.empty:\n",
        "        print(\"‚ö†Ô∏è Log de previs√µes vazio ou inv√°lido.\")\n",
        "        return\n",
        "\n",
        "    # üõ°Ô∏è Nova prote√ß√£o: checa se as colunas necess√°rias existem\n",
        "    if \"Capital Atual\" not in df.columns or \"Resultado\" not in df.columns:\n",
        "        print(\"üì≠ Sem dados de simula√ß√£o para gerar gr√°fico de evolu√ß√£o.\")\n",
        "        return\n",
        "\n",
        "    df = df.dropna(subset=[\"Date\", \"Capital Atual\", \"Resultado\"])\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], utc=True).dt.tz_convert(BR_TZ)\n",
        "    df = df[df[\"Resultado\"].isin([\"TP1\", \"SL\", \"Sem alvo\"])]\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"‚ö†Ô∏è Nenhuma simula√ß√£o v√°lida para exibir no gr√°fico.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Cores por tipo de resultado\n",
        "    cor_map = {\"TP1\": \"green\", \"SL\": \"red\", \"Sem alvo\": \"orange\"}\n",
        "    cores = df[\"Resultado\"].map(cor_map).fillna(\"gray\")\n",
        "\n",
        "    # Gr√°fico de pontos\n",
        "    plt.scatter(df[\"Date\"], df[\"Capital Atual\"], c=cores, edgecolors=\"black\", s=70)\n",
        "\n",
        "    # Linha de evolu√ß√£o do capital\n",
        "    plt.plot(df[\"Date\"], df[\"Capital Atual\"], linestyle=\"--\", color=\"blue\", alpha=0.7)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        plt.annotate(f\"${row['Capital Atual']:.0f}\", (row[\"Date\"], row[\"Capital Atual\"]),\n",
        "                     textcoords=\"offset points\", xytext=(0, 6), ha='center', fontsize=8)\n",
        "\n",
        "    plt.title(\"üí∞ Evolu√ß√£o da Carteira Virtual\")\n",
        "    plt.xlabel(\"Data (BR)\")\n",
        "    plt.ylabel(\"Capital ($)\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    path = \"/tmp/evolucao_carteira.png\"\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"‚úÖ Gr√°fico da carteira salvo: {path}\")\n",
        "\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 6. EXECU√á√ÉO DAS AN√ÅLISES E ALERTAS\n",
        "# ====================================================\n",
        "def run_analysis(\n",
        "    selected_timeframes=None,\n",
        "    plot_timeframes=[\"15m\", \"1h\"],\n",
        "    alert_timeframes=[\"15m\", \"1h\", \"1d\", \"1wk\"],\n",
        "    retrain_models=False\n",
        "):\n",
        "    criar_prediction_log_padrao()\n",
        "    carregar_carteira_virtual()\n",
        "\n",
        "    log_path = \"/content/prediction_log.csv\"\n",
        "    df_log_old = safe_read_csv(log_path)\n",
        "    if df_log_old is None:\n",
        "        df_log_old = pd.DataFrame(columns=[\"Asset\", \"Timeframe\", \"Date\"])\n",
        "\n",
        "    results = []\n",
        "\n",
        "    if selected_timeframes is None:\n",
        "        selected_timeframes = TIMEFRAMES\n",
        "\n",
        "    for asset in ASSETS:\n",
        "        try:\n",
        "            models = {}\n",
        "            lstm_models = {}\n",
        "            data = {}\n",
        "\n",
        "            for tf in selected_timeframes:\n",
        "                interval = tf['interval']\n",
        "                period = tf['period']\n",
        "\n",
        "                df = get_stock_data(asset, interval, period)\n",
        "                df = calculate_indicators(df)\n",
        "                data[interval] = df\n",
        "\n",
        "                if retrain_models:\n",
        "                    print(f\"üõ†Ô∏è For√ßando treinamento dos modelos para {asset} ({interval})\")\n",
        "                    models[interval] = train_ml_model(df, asset=asset, interval=interval, verbose=True, force_retrain=True)\n",
        "\n",
        "                    if interval in [\"1d\", \"1wk\"]:\n",
        "                        lstm_models[interval] = train_lstm_model_diario(\n",
        "                            df, asset=asset, interval=interval, window_size=60, force_retrain=True\n",
        "                        )\n",
        "                    else:\n",
        "                        lstm_models[interval] = train_lstm_model(\n",
        "                            df, asset=asset, interval=interval, window_size=20, force_retrain=True\n",
        "                        )\n",
        "                else:\n",
        "                    models[interval] = load_xgb_model(asset, interval)\n",
        "                    if models[interval] is None:\n",
        "                        models[interval] = train_ml_model(df, asset=asset, interval=interval, verbose=True, force_retrain=True)\n",
        "\n",
        "                    lstm_models[interval] = load_lstm_model(asset, interval)\n",
        "                    if lstm_models[interval] is None:\n",
        "                        if interval in [\"1d\", \"1wk\"]:\n",
        "                            lstm_models[interval] = train_lstm_model_diario(\n",
        "                                df, asset=asset, interval=interval, window_size=60, force_retrain=True\n",
        "                            )\n",
        "                        else:\n",
        "                            lstm_models[interval] = train_lstm_model(\n",
        "                                df, asset=asset, interval=interval, window_size=20, force_retrain=True\n",
        "                            )\n",
        "\n",
        "            if all(model is None for model in models.values()):\n",
        "                print(f\"‚ö†Ô∏è Nenhum modelo foi treinado para {asset}.\")\n",
        "                continue\n",
        "\n",
        "            for tf in selected_timeframes:\n",
        "                interval = tf['interval']\n",
        "                latest_data = data[interval].iloc[-1]\n",
        "                current_price = data[interval][\"Close\"].iloc[-1]\n",
        "\n",
        "                predicted_price_lstm = None\n",
        "                pred_high = None\n",
        "                pred_low = None\n",
        "\n",
        "                try:\n",
        "                    lstm_model = lstm_models.get(interval)\n",
        "                    if lstm_model:\n",
        "                        pred_lstm = predict_with_lstm(lstm_model, data[interval], asset=asset, interval=interval)\n",
        "\n",
        "                        if pred_lstm is None or any(pred_lstm.get(k) is None for k in [\"High\", \"Low\", \"Close\"]):\n",
        "                            print(f\"‚ö†Ô∏è Previs√£o LSTM inv√°lida para {asset} ({interval}) ‚Äî pulando an√°lise.\")\n",
        "                            continue\n",
        "\n",
        "                        predicted_price_lstm = pred_lstm[\"Close\"]\n",
        "                        pred_high = pred_lstm[\"High\"]\n",
        "                        pred_low = pred_lstm[\"Low\"]\n",
        "\n",
        "                        print(f\"\\nüîç {asset} ({interval})\")\n",
        "                        print(f\"   Pre√ßo atual: ${current_price:,.2f}\")\n",
        "                        print(f\"   Previs√£o LSTM: Close=${predicted_price_lstm:,.2f} | High=${pred_high:,.2f} | Low=${pred_low:,.2f}\")\n",
        "\n",
        "                        # üìè Verifica√ß√£o din√¢mica da dist√¢ncia m√≠nima\n",
        "                        ultimos_n_candles = 50\n",
        "                        df_candles_hist = data[interval].tail(ultimos_n_candles)\n",
        "                        media_range_real = (df_candles_hist[\"High\"] - df_candles_hist[\"Low\"]).mean()\n",
        "                        range_previsto = pred_high - pred_low\n",
        "\n",
        "                        if range_previsto < media_range_real * 0.8:\n",
        "                            print(f\"‚ö†Ô∏è Range previsto pequeno ({range_previsto:.2f} < {media_range_real*0.8:.2f}), ajustando TP1.\")\n",
        "\n",
        "                            # Ajustar Entry e TP1\n",
        "                            entry_price = pred_low\n",
        "                            tp1 = entry_price + media_range_real\n",
        "                            sl = entry_price - (entry_price * 0.01)\n",
        "\n",
        "                            # Lucro l√≠quido m√≠nimo\n",
        "                            taxa_percentual = 0.001\n",
        "                            slippage_percentual = 0.002\n",
        "                            custo_total_percent = taxa_percentual + slippage_percentual\n",
        "\n",
        "                            lucro_bruto = tp1 - entry_price\n",
        "                            lucro_liquido = lucro_bruto * (1 - custo_total_percent)\n",
        "                            lucro_percentual_liquido = lucro_liquido / entry_price\n",
        "\n",
        "                            if lucro_percentual_liquido < 0.002:\n",
        "                                print(f\"üö´ Lucro l√≠quido estimado muito baixo ({lucro_percentual_liquido*100:.2f}%), descartando trade.\")\n",
        "                                results.append({\n",
        "                                    \"Asset\": asset,\n",
        "                                    \"Timeframe\": interval,\n",
        "                                    \"Date\": datetime.now(),\n",
        "                                    \"Price\": current_price,\n",
        "                                    \"Signal\": \"Descartado\",\n",
        "                                    \"Reason\": \"Lucro l√≠quido insuficiente ap√≥s ajuste\",\n",
        "                                    \"Predicted_Close\": predicted_price_lstm,\n",
        "                                    \"Predicted_High\": pred_high,\n",
        "                                    \"Predicted_Low\": pred_low\n",
        "                                })\n",
        "                                continue\n",
        "                            else:\n",
        "                                print(f\"‚úÖ Trade ajustado aceito: lucro l√≠quido = {lucro_percentual_liquido*100:.2f}%\")\n",
        "                        else:\n",
        "                            entry_price = pred_low\n",
        "                            tp1 = pred_high\n",
        "                            sl = entry_price - (entry_price * 0.01)\n",
        "\n",
        "                        distancia = abs(entry_price - sl)\n",
        "                        tp2 = entry_price + 2 * distancia\n",
        "\n",
        "                        print(f\"üìà Entrada: {entry_price:.2f} | TP1: {tp1:.2f} | SL: {sl:.2f} | TP2: {tp2:.2f}\")\n",
        "\n",
        "                        rr_ratio = (tp1 - entry_price) / (entry_price - sl) if (entry_price - sl) != 0 else None\n",
        "\n",
        "                        if rr_ratio is None or rr_ratio < 1.0:\n",
        "                            print(f\"üö´ R/R baixo ({rr_ratio}), descartando sinal.\")\n",
        "                            results.append({\n",
        "                                \"Asset\": asset,\n",
        "                                \"Timeframe\": interval,\n",
        "                                \"Date\": datetime.now(),\n",
        "                                \"Price\": current_price,\n",
        "                                \"Signal\": \"Descartado\",\n",
        "                                \"Reason\": \"Risk/Reward baixo\",\n",
        "                                \"Predicted_Close\": predicted_price_lstm,\n",
        "                                \"Predicted_High\": pred_high,\n",
        "                                \"Predicted_Low\": pred_low\n",
        "                            })\n",
        "                            continue\n",
        "\n",
        "                        rr_ratio = round(rr_ratio, 2)\n",
        "\n",
        "                        # üß† Ensemble LSTM + XGBoost\n",
        "                        model_xgb = models.get(interval)\n",
        "                        xgb_signal = 1  # Default: aceita\n",
        "\n",
        "                        if model_xgb is not None:\n",
        "                            try:\n",
        "                                features_xgb = get_feature_columns(data[interval], include_lstm_pred=False)\n",
        "                                df_features_now = data[interval].iloc[[-1]][features_xgb]\n",
        "                                xgb_signal = model_xgb.predict(df_features_now)[0]\n",
        "                                print(f\"üìà XGBoost sinalizou: {'COMPRA' if xgb_signal == 1 else 'VENDA'}\")\n",
        "                            except Exception as e:\n",
        "                                print(f\"‚ö†Ô∏è Erro ao usar XGBoost para validar: {e}\")\n",
        "\n",
        "                        if xgb_signal != 1:\n",
        "                            print(f\"üö´ Trade cancelado pelo XGBoost.\")\n",
        "                            results.append({\n",
        "                                \"Asset\": asset,\n",
        "                                \"Timeframe\": interval,\n",
        "                                \"Date\": datetime.now(),\n",
        "                                \"Price\": current_price,\n",
        "                                \"Signal\": \"Descartado\",\n",
        "                                \"Reason\": \"XGBoost n√£o confirmou o sinal\",\n",
        "                                \"Predicted_Close\": predicted_price_lstm,\n",
        "                                \"Predicted_High\": pred_high,\n",
        "                                \"Predicted_Low\": pred_low\n",
        "                            })\n",
        "                            continue\n",
        "\n",
        "                        # ‚úÖ Se passou tudo: sinal aceito\n",
        "                        print(f\"‚úÖ Sinal final aceito com RR {rr_ratio}.\")\n",
        "\n",
        "                        ajuste = adjust_signal_based_on_history(asset, interval)\n",
        "                        val_score = model_xgb.validation_score if model_xgb and hasattr(model_xgb, \"validation_score\") else {}\n",
        "\n",
        "                        results.append({\n",
        "                            \"Asset\": asset,\n",
        "                            \"Timeframe\": interval,\n",
        "                            \"Date\": datetime.now(),\n",
        "                            \"Price\": current_price,\n",
        "                            \"Signal\": 1,\n",
        "                            \"Reason\": \"Aceito\",\n",
        "                            \"Confidence\": None,\n",
        "                            \"AdjustedProb\": round(ajuste, 2),\n",
        "                            \"TP1\": tp1,\n",
        "                            \"TP2\": tp2,\n",
        "                            \"SL\": sl,\n",
        "                            \"Entry\": entry_price,\n",
        "                            \"Accuracy\": val_score.get(\"accuracy\"),\n",
        "                            \"Precision\": val_score.get(\"precision\"),\n",
        "                            \"Recall\": val_score.get(\"recall\"),\n",
        "                            \"F1\": val_score.get(\"f1\"),\n",
        "                            \"LSTM_Predicted\": predicted_price_lstm,\n",
        "                            \"TargetPrice\": predicted_price_lstm,\n",
        "                            \"LSTM_High_Predicted\": pred_high,\n",
        "                            \"LSTM_Low_Predicted\": pred_low\n",
        "                        })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"[!] Erro inesperado na previs√£o/sinal: {e}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erro inesperado ao processar {asset}: {e}\")\n",
        "            continue\n",
        "\n",
        "    df_results = pd.DataFrame(results)\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    filename = f\"model_results_{timestamp}.csv\"\n",
        "    df_results.to_csv(filename, index=False)\n",
        "    print(f\"\\nüìÅ Resultados salvos em: {filename}\")\n",
        "\n",
        "    if df_log_old is not None:\n",
        "        df_log_combined = pd.concat([df_log_old, df_results], ignore_index=True).fillna(\"\")\n",
        "        df_log_combined.to_csv(log_path, index=False)\n",
        "        print(f\"üìã Log de previs√µes atualizado em: {log_path}\")\n",
        "    else:\n",
        "        df_results.to_csv(log_path, index=False)\n",
        "        print(f\"üîÑ Log de previs√µes criado em: {log_path}\")\n",
        "\n",
        "    salvar_carteira_virtual()\n",
        "    exibir_status_carteira()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 7. AGENDAMENTO E EXECU√á√ÉO AUTOM√ÅTICA COM THREADS\n",
        "# ====================================================\n",
        "import time\n",
        "import threading\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Fun√ß√£o que checa se √© hora de rodar\n",
        "def is_time_to_run(interval):\n",
        "    now = datetime.now(BR_TZ)\n",
        "\n",
        "    if interval == \"15m\" and now.minute % 15 == 0:\n",
        "        return True\n",
        "    elif interval == \"1h\" and now.minute == 0:\n",
        "        return True\n",
        "    elif interval == \"1d\" and now.hour == 0 and now.minute == 0:\n",
        "        return True\n",
        "    elif interval == \"1wk\" and now.weekday() == 0 and now.hour == 0 and now.minute == 0:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Fun√ß√£o de agendamento para cada timeframe\n",
        "def agendar_analise_timeframe(tf_config):\n",
        "    interval = tf_config[\"interval\"]\n",
        "    ultimo_print = datetime.now(BR_TZ)\n",
        "\n",
        "    while True:\n",
        "        now = datetime.now(BR_TZ)\n",
        "\n",
        "        if is_time_to_run(interval):\n",
        "            print(f\"\\nüöÄ [{interval}] Rodando an√°lise √†s {now.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "            try:\n",
        "                run_analysis(\n",
        "                    selected_timeframes=[tf_config],\n",
        "                    plot_timeframes=[\"1h\"],\n",
        "                    alert_timeframes=[\"15m\", \"1h\", \"1d\", \"1wk\"],\n",
        "                    retrain_models=False\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Erro durante an√°lise de {interval}: {e}\")\n",
        "            time.sleep(60)  # Aguarda 1 minuto ap√≥s rodar\n",
        "        else:\n",
        "            if (now - ultimo_print).total_seconds() > 300:\n",
        "                print(f\"‚è≥ [{interval}] Aguardando pr√≥xima execu√ß√£o... {now.strftime('%H:%M:%S')}\")\n",
        "                ultimo_print = now\n",
        "            time.sleep(30)\n",
        "\n",
        "# üî• Prote√ß√£o para iniciar threads apenas uma vez\n",
        "if \"threads_iniciadas\" not in globals():\n",
        "    print(\"üßµ Iniciando threads para execu√ß√£o cont√≠nua (modo 24/7)...\")\n",
        "\n",
        "    threads = []\n",
        "    for tf_config in TIMEFRAMES:\n",
        "        t = threading.Thread(target=agendar_analise_timeframe, args=(tf_config,), daemon=True)\n",
        "        t.start()\n",
        "        threads.append(t)\n",
        "\n",
        "    threads_iniciadas = True\n",
        "\n",
        "    print(\"‚úÖ Threads iniciadas com sucesso. Sistema aguardando pr√≥ximos hor√°rios de execu√ß√£o...\")\n",
        "    # ‚è≥ Mant√©m o programa vivo mesmo depois de iniciar as threads\n",
        "    while True:\n",
        "        time.sleep(60)\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Threads j√° estavam iniciadas ‚Äî sistema aguardando pr√≥ximas execu√ß√µes...\")\n",
        "    # ‚è≥ Tamb√©m mant√©m o programa vivo aqui\n",
        "    while True:\n",
        "        time.sleep(60)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FRgd1-k06BB",
        "outputId": "be7d66e3-890e-4e2d-e3c3-96afd93c41eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Threads j√° estavam iniciadas ‚Äî sistema aguardando pr√≥ximas execu√ß√µes...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£ Escolha o timeframe que voc√™ quer simular\n",
        "timeframe = \"15m\"  # Pode ser \"15m\", \"1h\", \"1d\", etc.\n",
        "\n",
        "# 2Ô∏è‚É£ Escolha o ativo que voc√™ quer simular\n",
        "asset = \"BTC-USD\"\n",
        "\n",
        "# 3Ô∏è‚É£ Baixar os candles correspondentes ao timeframe\n",
        "if timeframe in [\"1d\", \"1wk\"]:\n",
        "    # Para di√°rio ou semanal, usar start/end para pegar mais dados\n",
        "    start_date = \"2015-01-01\"\n",
        "    end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    df_candles = yf.download(asset, start=start_date, end=end_date, interval=timeframe, progress=False, auto_adjust=False)\n",
        "else:\n",
        "    # Para 15m e 1h, usar period\n",
        "    df_candles = yf.download(asset, period=\"60d\", interval=timeframe, progress=False, auto_adjust=False)\n",
        "\n",
        "# 4Ô∏è‚É£ Corrigir colunas, se necess√°rio\n",
        "if isinstance(df_candles.columns, pd.MultiIndex):\n",
        "    df_candles.columns = df_candles.columns.get_level_values(0)\n",
        "df_candles.columns = [col.split()[-1] if \" \" in col else col for col in df_candles.columns]\n",
        "df_candles = df_candles.loc[:, ~df_candles.columns.duplicated()]\n",
        "col_map = {col: std_col for col in df_candles.columns for std_col in [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"] if std_col.lower() in col.lower()}\n",
        "df_candles = df_candles.rename(columns=col_map)\n",
        "df_candles = df_candles[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
        "\n",
        "# 5Ô∏è‚É£ Agora simular todos os trades com a nova l√≥gica de compra no Low e venda no High\n",
        "simular_todos_trades(prediction_log_path=\"/content/prediction_log.csv\", df_candles=df_candles, timeframe=timeframe)\n",
        "\n",
        "# 6Ô∏è‚É£ Depois de simular, voc√™ ver√° o gr√°fico de lucro e a carteira atualizada\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekhEltOT71bq",
        "outputId": "4b280ee1-1b11-4a56-9c27-b8c09ca1a697"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Rodando simula√ß√£o de carteira virtual com sinais do log...\n",
            "‚ö†Ô∏è Arquivo inv√°lido (sem colunas): /content/prediction_log.csv\n",
            "‚ö†Ô∏è Log vazio.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä Avalia√ß√£o dos desvios das previs√µes LSTM passadas\n",
        "from datetime import timedelta\n",
        "\n",
        "def evaluate_past_predictions(results_file=\"/content/prediction_log.csv\", lookahead_candles=5):\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    import yfinance as yf\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    if not os.path.exists(results_file):\n",
        "        print(\"üì≠ Nenhum log de previs√£o encontrado.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(results_file)\n",
        "    if df.empty or \"Asset\" not in df.columns:\n",
        "        print(\"‚ö†Ô∏è Log vazio ou inv√°lido.\")\n",
        "        return\n",
        "\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "    print(f\"üìä Avaliando {len(df)} previs√µes registradas...\")\n",
        "\n",
        "    evaluation = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        asset = row[\"Asset\"]\n",
        "        timeframe = row[\"Timeframe\"]\n",
        "        prediction_time = row[\"Date\"]\n",
        "        predicted_target = row.get(\"TargetPrice\", None)\n",
        "\n",
        "        if pd.isna(predicted_target):\n",
        "            continue\n",
        "\n",
        "        # Define intervalo do Yahoo com base no timeframe\n",
        "        if timeframe == \"15m\":\n",
        "            intervalo = \"15m\"\n",
        "        elif timeframe == \"1h\":\n",
        "            intervalo = \"60m\"\n",
        "        else:\n",
        "            continue  # s√≥ 15m e 1h por enquanto\n",
        "\n",
        "        try:\n",
        "            # Coletar candles futuros\n",
        "            df_future = yf.download(asset, start=prediction_time, interval=intervalo, progress=False)\n",
        "            df_future = df_future[df_future.index > prediction_time]\n",
        "\n",
        "            if df_future.empty or len(df_future) < lookahead_candles:\n",
        "                continue\n",
        "\n",
        "            df_future = df_future.head(lookahead_candles)\n",
        "            preco_real = df_future[\"Close\"].iloc[-1]\n",
        "\n",
        "            erro = preco_real - predicted_target\n",
        "            perc_erro = (erro / predicted_target) * 100\n",
        "\n",
        "            evaluation.append({\n",
        "                \"Ativo\": asset,\n",
        "                \"Timeframe\": timeframe,\n",
        "                \"Data Previs√£o\": prediction_time.strftime(\"%Y-%m-%d %H:%M\"),\n",
        "                \"Pre√ßo Previsto\": round(predicted_target, 2),\n",
        "                \"Pre√ßo Real\": round(preco_real, 2),\n",
        "                \"Erro Absoluto ($)\": round(erro, 2),\n",
        "                \"Erro Percentual (%)\": round(perc_erro, 2)\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erro ao avaliar {asset} ({timeframe}) em {prediction_time}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Criar DataFrame final\n",
        "    df_eval = pd.DataFrame(evaluation)\n",
        "\n",
        "    if df_eval.empty:\n",
        "        print(\"üì≠ Nenhuma avalia√ß√£o foi poss√≠vel ainda (precisa de mais candles futuros).\")\n",
        "        return\n",
        "\n",
        "    # Exibir resultados\n",
        "    print(\"\\nüìà Resultados de Desvio das Previs√µes:\")\n",
        "    display(df_eval)\n",
        "\n",
        "    # Mostrar m√©dias\n",
        "    print(\"\\nüìä Desvios M√©dios:\")\n",
        "    print(df_eval.groupby(\"Timeframe\").agg({\n",
        "        \"Erro Absoluto ($)\": \"mean\",\n",
        "        \"Erro Percentual (%)\": \"mean\"\n",
        "    }).round(2))\n",
        "\n",
        "    # Gr√°fico de Erro Absoluto\n",
        "    df_eval.groupby(\"Timeframe\")[\"Erro Absoluto ($)\"].mean().plot(kind=\"barh\", title=\"üìà Erro Absoluto M√©dio por Timeframe\", xlabel=\"Erro ($)\", ylabel=\"Timeframe\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Gr√°fico de Erro Percentual\n",
        "    df_eval.groupby(\"Timeframe\")[\"Erro Percentual (%)\"].mean().plot(kind=\"barh\", title=\"üìâ Erro Percentual M√©dio por Timeframe\", xlabel=\"Erro (%)\", ylabel=\"Timeframe\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# üöÄ Rodar avalia√ß√£o\n",
        "evaluate_past_predictions()\n"
      ],
      "metadata": {
        "id": "s3YJvyy3Ct3L",
        "outputId": "113a6ed3-b8d8-43bd-d27d-428d4d8c73ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Avaliando 2 previs√µes registradas...\n",
            "üì≠ Nenhuma avalia√ß√£o foi poss√≠vel ainda (precisa de mais candles futuros).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£ Escolha o timeframe que voc√™ quer simular\n",
        "timeframe = \"15m\"  # Pode ser \"15m\", \"1h\", \"1d\", etc.\n",
        "\n",
        "# 2Ô∏è‚É£ Escolha o ativo que voc√™ quer simular\n",
        "asset = \"BTC-USD\"\n",
        "\n",
        "# 3Ô∏è‚É£ Baixar os candles correspondentes ao timeframe\n",
        "if timeframe in [\"1d\", \"1wk\"]:\n",
        "    # Para di√°rio ou semanal, usar start/end para pegar mais dados\n",
        "    start_date = \"2015-01-01\"\n",
        "    end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    df_candles = yf.download(asset, start=start_date, end=end_date, interval=timeframe, progress=False, auto_adjust=False)\n",
        "else:\n",
        "    # Para 15m e 1h, usar period\n",
        "    df_candles = yf.download(asset, period=\"60d\", interval=timeframe, progress=False, auto_adjust=False)\n",
        "\n",
        "# 4Ô∏è‚É£ Corrigir colunas, se necess√°rio\n",
        "if isinstance(df_candles.columns, pd.MultiIndex):\n",
        "    df_candles.columns = df_candles.columns.get_level_values(0)\n",
        "df_candles.columns = [col.split()[-1] if \" \" in col else col for col in df_candles.columns]\n",
        "df_candles = df_candles.loc[:, ~df_candles.columns.duplicated()]\n",
        "col_map = {col: std_col for col in df_candles.columns for std_col in [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"] if std_col.lower() in col.lower()}\n",
        "df_candles = df_candles.rename(columns=col_map)\n",
        "df_candles = df_candles[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
        "\n",
        "# 5Ô∏è‚É£ Agora simular todos os trades com a nova l√≥gica de compra no Low e venda no High\n",
        "simular_todos_trades(prediction_log_path=\"/content/prediction_log.csv\", df_candles=df_candles, timeframe=timeframe)\n",
        "\n",
        "# 6Ô∏è‚É£ Depois de simular, voc√™ ver√° o gr√°fico de lucro e a carteira atualizada\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "049npxu9CtB2",
        "outputId": "4c28acbc-a4b5-4bfe-858f-6b5e773c64ff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Rodando simula√ß√£o de carteira virtual com sinais do log...\n",
            "üì≠ Nenhum trade foi simulado (ainda).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# üìö Imports adicionais\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import os\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# üåé Timezone Brasil\n",
        "BR_TZ = pytz.timezone(\"America/Sao_Paulo\")\n",
        "\n",
        "# üìÇ Pasta dos modelos salvos\n",
        "MODELS_DIR = \"/content/models\"\n",
        "\n",
        "# üìÑ Fun√ß√£o para carregar modelo e metadados\n",
        "def carregar_lstm_completo(asset, interval):\n",
        "    asset_clean = asset.replace(\"-\", \"\")\n",
        "    path_h5 = f\"{MODELS_DIR}/lstm_model_{asset_clean}_{interval}.h5\"\n",
        "    path_pkl = f\"{MODELS_DIR}/lstm_model_{asset_clean}_{interval}_meta.pkl\"\n",
        "\n",
        "    if not os.path.exists(path_h5) or not os.path.exists(path_pkl):\n",
        "        return None\n",
        "\n",
        "    model = load_model(path_h5, compile=False)\n",
        "    meta = joblib.load(path_pkl)\n",
        "\n",
        "    model.scaler_x = meta.get(\"scaler_x\")\n",
        "    model.scaler_y = meta.get(\"scaler_y\")\n",
        "    model.feature_cols = meta.get(\"feature_cols\")\n",
        "    model.target_cols = meta.get(\"target_cols\", [\"High\", \"Low\", \"Close\"])\n",
        "    model.window_size = meta.get(\"window_size\", 20)\n",
        "\n",
        "    return model\n",
        "\n",
        "# üìà Avaliador\n",
        "def avaliar_modelo(asset, interval, periodo=\"60d\", verbose=True):\n",
        "    try:\n",
        "        model = carregar_lstm_completo(asset, interval)\n",
        "        if model is None:\n",
        "            if verbose: print(f\"‚ö†Ô∏è Modelo n√£o encontrado para {asset} ({interval})\")\n",
        "            return None\n",
        "\n",
        "        df = yf.download(asset, period=periodo, interval=interval, progress=False)\n",
        "        if df.empty or len(df) < model.window_size + 1:\n",
        "            if verbose: print(f\"‚ö†Ô∏è Dados insuficientes para {asset} ({interval})\")\n",
        "            return None\n",
        "\n",
        "        # Corrigir colunas\n",
        "        if isinstance(df.columns, pd.MultiIndex):\n",
        "            df.columns = df.columns.get_level_values(0)\n",
        "        df = df.rename(columns=lambda x: x.split()[-1] if \" \" in x else x)\n",
        "        df = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].dropna()\n",
        "\n",
        "        # Aplicar indicadores\n",
        "        df = calculate_indicators(df)\n",
        "\n",
        "        # Preparar dados\n",
        "        from sklearn.preprocessing import MinMaxScaler\n",
        "        feature_cols = model.feature_cols\n",
        "        target_cols = model.target_cols\n",
        "\n",
        "        X_list, y_list = [], []\n",
        "        scaler_x = model.scaler_x\n",
        "        scaler_y = model.scaler_y\n",
        "\n",
        "        df_features = df[feature_cols].dropna()\n",
        "        df_targets = df[target_cols].dropna()\n",
        "        min_len = min(len(df_features), len(df_targets))\n",
        "\n",
        "        if min_len <= model.window_size:\n",
        "            if verbose: print(f\"‚ö†Ô∏è Dados insuficientes ap√≥s indicadores para {asset} ({interval})\")\n",
        "            return None\n",
        "\n",
        "        scaled_X = scaler_x.transform(df_features.values)\n",
        "        scaled_y = scaler_y.transform(df_targets.values)\n",
        "\n",
        "        for i in range(model.window_size, min_len):\n",
        "            X_list.append(scaled_X[i-model.window_size:i])\n",
        "            y_list.append(scaled_y[i])\n",
        "\n",
        "        X_array = np.array(X_list)\n",
        "        y_array = np.array(y_list)\n",
        "\n",
        "        preds_scaled = model.predict(X_array, verbose=0)\n",
        "        preds = scaler_y.inverse_transform(preds_scaled)\n",
        "        y_true = scaler_y.inverse_transform(y_array)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for idx, col in enumerate([\"High\", \"Low\", \"Close\"]):\n",
        "            y_true_col = y_true[:, idx]\n",
        "            preds_col = preds[:, idx]\n",
        "\n",
        "            r2 = r2_score(y_true_col, preds_col)\n",
        "            mae = mean_absolute_error(y_true_col, preds_col)\n",
        "            rmse = np.sqrt(mean_squared_error(y_true_col, preds_col))\n",
        "            mape = np.mean(np.abs((y_true_col - preds_col) / y_true_col)) * 100\n",
        "\n",
        "            results[f\"R2_{col}\"] = round(r2, 4)\n",
        "            results[f\"MAE_{col}\"] = round(mae, 4)\n",
        "            results[f\"RMSE_{col}\"] = round(rmse, 4)\n",
        "            results[f\"MAPE_{col}\"] = round(mape, 2)\n",
        "\n",
        "        return {\n",
        "            \"Asset\": asset,\n",
        "            \"Timeframe\": interval,\n",
        "            **results\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao avaliar {asset} ({interval}): {e}\")\n",
        "        return None\n",
        "\n",
        "# üöÄ Avaliar todos\n",
        "def avaliar_todos(assets=None, timeframes=None, periodo=\"60d\", salvar_csv=True):\n",
        "    if assets is None:\n",
        "        assets = [\"BTC-USD\", \"ETH-USD\", \"SOL-USD\", \"XRP-USD\", \"AVAX-USD\", \"AAVE-USD\", \"DOT-USD\", \"NEAR-USD\", \"ADA-USD\", \"VIRTUAL-USD\", \"PENDLE-USD\"]\n",
        "    if timeframes is None:\n",
        "        timeframes = [\"15m\", \"1h\", \"1d\", \"1wk\"]\n",
        "\n",
        "    resultados = []\n",
        "\n",
        "    for asset in assets:\n",
        "        for tf in timeframes:\n",
        "            r = avaliar_modelo(asset, tf, periodo)\n",
        "            if r is not None:\n",
        "                resultados.append(r)\n",
        "\n",
        "    df_avaliacao = pd.DataFrame(resultados)\n",
        "\n",
        "    print(\"\\nüìã Avalia√ß√£o de Performance dos Modelos:\\n\")\n",
        "    display(df_avaliacao)\n",
        "\n",
        "    if salvar_csv:\n",
        "        df_avaliacao.to_csv(\"/content/model_evaluation.csv\", index=False)\n",
        "        print(\"\\n‚úÖ Resultados salvos em: /content/model_evaluation.csv\")\n",
        "\n",
        "    return df_avaliacao\n",
        "\n",
        "# Rodar Avalia√ß√£o\n",
        "avaliar_todos()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HyTIVPYS9FrJ",
        "outputId": "6c8416c8-bc71-4c19-c3e7-8037a853ac3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YF.download() has changed argument auto_adjust default to True\n",
            "‚ö†Ô∏è Dados insuficientes ap√≥s indicadores para BTC-USD (1d)\n",
            "‚ö†Ô∏è Dados insuficientes para BTC-USD (1wk)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para ETH-USD (15m)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para ETH-USD (1h)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para ETH-USD (1d)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para ETH-USD (1wk)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para SOL-USD (15m)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para SOL-USD (1h)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para SOL-USD (1d)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para SOL-USD (1wk)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para XRP-USD (15m)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para XRP-USD (1h)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para XRP-USD (1d)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para XRP-USD (1wk)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para AVAX-USD (15m)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para AVAX-USD (1h)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para AVAX-USD (1d)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para AVAX-USD (1wk)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para AAVE-USD (15m)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para AAVE-USD (1h)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para AAVE-USD (1d)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para AAVE-USD (1wk)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para DOT-USD (15m)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para DOT-USD (1h)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para DOT-USD (1d)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para DOT-USD (1wk)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para NEAR-USD (15m)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para NEAR-USD (1h)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para NEAR-USD (1d)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para NEAR-USD (1wk)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para ADA-USD (15m)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para ADA-USD (1h)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para ADA-USD (1d)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para ADA-USD (1wk)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para VIRTUAL-USD (15m)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para VIRTUAL-USD (1h)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para VIRTUAL-USD (1d)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para VIRTUAL-USD (1wk)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para PENDLE-USD (15m)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para PENDLE-USD (1h)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para PENDLE-USD (1d)\n",
            "‚ö†Ô∏è Modelo n√£o encontrado para PENDLE-USD (1wk)\n",
            "\n",
            "üìã Avalia√ß√£o de Performance dos Modelos:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     Asset Timeframe  R2_High  MAE_High  RMSE_High  MAPE_High  R2_Low  \\\n",
              "0  BTC-USD       15m   0.9818  458.6571   558.4924       0.54  0.9822   \n",
              "1  BTC-USD        1h   0.9410  787.2112  1058.5294       0.93  0.9527   \n",
              "\n",
              "    MAE_Low  RMSE_Low  MAPE_Low  R2_Close  MAE_Close  RMSE_Close  MAPE_Close  \n",
              "0  435.9791  556.1906      0.51    0.9841   382.7834    524.2499        0.45  \n",
              "1  739.6224  979.3861      0.89    0.9324   876.7661   1153.0452        1.04  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c3b53c0-efb3-4e1e-96ae-f2f73897d74a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Asset</th>\n",
              "      <th>Timeframe</th>\n",
              "      <th>R2_High</th>\n",
              "      <th>MAE_High</th>\n",
              "      <th>RMSE_High</th>\n",
              "      <th>MAPE_High</th>\n",
              "      <th>R2_Low</th>\n",
              "      <th>MAE_Low</th>\n",
              "      <th>RMSE_Low</th>\n",
              "      <th>MAPE_Low</th>\n",
              "      <th>R2_Close</th>\n",
              "      <th>MAE_Close</th>\n",
              "      <th>RMSE_Close</th>\n",
              "      <th>MAPE_Close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BTC-USD</td>\n",
              "      <td>15m</td>\n",
              "      <td>0.9818</td>\n",
              "      <td>458.6571</td>\n",
              "      <td>558.4924</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.9822</td>\n",
              "      <td>435.9791</td>\n",
              "      <td>556.1906</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.9841</td>\n",
              "      <td>382.7834</td>\n",
              "      <td>524.2499</td>\n",
              "      <td>0.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BTC-USD</td>\n",
              "      <td>1h</td>\n",
              "      <td>0.9410</td>\n",
              "      <td>787.2112</td>\n",
              "      <td>1058.5294</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.9527</td>\n",
              "      <td>739.6224</td>\n",
              "      <td>979.3861</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.9324</td>\n",
              "      <td>876.7661</td>\n",
              "      <td>1153.0452</td>\n",
              "      <td>1.04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c3b53c0-efb3-4e1e-96ae-f2f73897d74a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2c3b53c0-efb3-4e1e-96ae-f2f73897d74a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2c3b53c0-efb3-4e1e-96ae-f2f73897d74a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6717ac3b-5eaa-43e9-a0d2-a97eedeb01af\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6717ac3b-5eaa-43e9-a0d2-a97eedeb01af')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6717ac3b-5eaa-43e9-a0d2-a97eedeb01af button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"avaliar_todos()\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Asset\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"BTC-USD\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Timeframe\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"1h\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R2_High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.028849956672411182,\n        \"min\": 0.941,\n        \"max\": 0.9818,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.941\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE_High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 232.32283209664303,\n        \"min\": 458.6571,\n        \"max\": 787.2112,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          787.2112\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE_High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 353.5795535441776,\n        \"min\": 558.4924,\n        \"max\": 1058.5294,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1058.5294\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE_High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27577164466275356,\n        \"min\": 0.54,\n        \"max\": 0.93,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.93\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R2_Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02085965004500313,\n        \"min\": 0.9527,\n        \"max\": 0.9822,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.9527\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE_Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 214.7082364918612,\n        \"min\": 435.9791,\n        \"max\": 739.6224,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          739.6224\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE_Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 299.2444078176316,\n        \"min\": 556.1906,\n        \"max\": 979.3861,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          979.3861\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE_Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.26870057685088805,\n        \"min\": 0.51,\n        \"max\": 0.89,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.89\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R2_Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03655742058734448,\n        \"min\": 0.9324,\n        \"max\": 0.9841,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.9324\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE_Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 349.29851695884,\n        \"min\": 382.7834,\n        \"max\": 876.7661,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          876.7661\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE_Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 444.6254206082295,\n        \"min\": 524.2499,\n        \"max\": 1153.0452,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1153.0452\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE_Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.41719300090006306,\n        \"min\": 0.45,\n        \"max\": 1.04,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.04\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Resultados salvos em: /content/model_evaluation.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Asset Timeframe  R2_High  MAE_High  RMSE_High  MAPE_High  R2_Low  \\\n",
              "0  BTC-USD       15m   0.9818  458.6571   558.4924       0.54  0.9822   \n",
              "1  BTC-USD        1h   0.9410  787.2112  1058.5294       0.93  0.9527   \n",
              "\n",
              "    MAE_Low  RMSE_Low  MAPE_Low  R2_Close  MAE_Close  RMSE_Close  MAPE_Close  \n",
              "0  435.9791  556.1906      0.51    0.9841   382.7834    524.2499        0.45  \n",
              "1  739.6224  979.3861      0.89    0.9324   876.7661   1153.0452        1.04  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2180ea08-e928-4a75-8be2-0a9c1e0ba85e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Asset</th>\n",
              "      <th>Timeframe</th>\n",
              "      <th>R2_High</th>\n",
              "      <th>MAE_High</th>\n",
              "      <th>RMSE_High</th>\n",
              "      <th>MAPE_High</th>\n",
              "      <th>R2_Low</th>\n",
              "      <th>MAE_Low</th>\n",
              "      <th>RMSE_Low</th>\n",
              "      <th>MAPE_Low</th>\n",
              "      <th>R2_Close</th>\n",
              "      <th>MAE_Close</th>\n",
              "      <th>RMSE_Close</th>\n",
              "      <th>MAPE_Close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BTC-USD</td>\n",
              "      <td>15m</td>\n",
              "      <td>0.9818</td>\n",
              "      <td>458.6571</td>\n",
              "      <td>558.4924</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.9822</td>\n",
              "      <td>435.9791</td>\n",
              "      <td>556.1906</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.9841</td>\n",
              "      <td>382.7834</td>\n",
              "      <td>524.2499</td>\n",
              "      <td>0.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BTC-USD</td>\n",
              "      <td>1h</td>\n",
              "      <td>0.9410</td>\n",
              "      <td>787.2112</td>\n",
              "      <td>1058.5294</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.9527</td>\n",
              "      <td>739.6224</td>\n",
              "      <td>979.3861</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.9324</td>\n",
              "      <td>876.7661</td>\n",
              "      <td>1153.0452</td>\n",
              "      <td>1.04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2180ea08-e928-4a75-8be2-0a9c1e0ba85e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2180ea08-e928-4a75-8be2-0a9c1e0ba85e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2180ea08-e928-4a75-8be2-0a9c1e0ba85e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-549df977-0528-4378-91e3-d4b08dcf5c81\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-549df977-0528-4378-91e3-d4b08dcf5c81')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-549df977-0528-4378-91e3-d4b08dcf5c81 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"avaliar_todos()\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Asset\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"BTC-USD\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Timeframe\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"1h\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R2_High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.028849956672411182,\n        \"min\": 0.941,\n        \"max\": 0.9818,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.941\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE_High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 232.32283209664303,\n        \"min\": 458.6571,\n        \"max\": 787.2112,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          787.2112\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE_High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 353.5795535441776,\n        \"min\": 558.4924,\n        \"max\": 1058.5294,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1058.5294\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE_High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27577164466275356,\n        \"min\": 0.54,\n        \"max\": 0.93,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.93\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R2_Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02085965004500313,\n        \"min\": 0.9527,\n        \"max\": 0.9822,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.9527\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE_Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 214.7082364918612,\n        \"min\": 435.9791,\n        \"max\": 739.6224,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          739.6224\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE_Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 299.2444078176316,\n        \"min\": 556.1906,\n        \"max\": 979.3861,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          979.3861\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE_Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.26870057685088805,\n        \"min\": 0.51,\n        \"max\": 0.89,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.89\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R2_Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03655742058734448,\n        \"min\": 0.9324,\n        \"max\": 0.9841,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.9324\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE_Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 349.29851695884,\n        \"min\": 382.7834,\n        \"max\": 876.7661,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          876.7661\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE_Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 444.6254206082295,\n        \"min\": 524.2499,\n        \"max\": 1153.0452,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1153.0452\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE_Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.41719300090006306,\n        \"min\": 0.45,\n        \"max\": 1.04,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.04\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìö Primeiro: carregar os dados de 1d completos\n",
        "asset = \"BTC-USD\"\n",
        "interval = \"1d\"\n",
        "start_date = \"2015-01-01\"\n",
        "end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Usa o get_stock_data com start/end\n",
        "df = yf.download(asset, start=start_date, end=end_date, interval=interval, progress=False, auto_adjust=False)\n",
        "\n",
        "# Corrige colunas se necess√°rio\n",
        "if isinstance(df.columns, pd.MultiIndex):\n",
        "    df.columns = df.columns.get_level_values(0)\n",
        "df.columns = [col.split()[-1] if \" \" in col else col for col in df.columns]\n",
        "df = df.loc[:, ~df.columns.duplicated()]\n",
        "col_map = {col: std_col for col in df.columns for std_col in [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"] if std_col.lower() in col.lower()}\n",
        "df = df.rename(columns=col_map)\n",
        "df = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
        "\n",
        "# Calcula os indicadores t√©cnicos\n",
        "df = calculate_indicators(df)\n",
        "\n",
        "# üìà Agora: treina o modelo LSTM Di√°rio especial\n",
        "model = train_lstm_model_diario(\n",
        "    df,\n",
        "    asset=asset,\n",
        "    interval=interval,\n",
        "    window_size=60,\n",
        "    force_retrain=True  # üëà for√ßa recriar mesmo se j√° tiver salvo\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "N7hasnIPneQ_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "c530b9b3-f156-4b45-dfe5-a6131ac94c75"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ [1d] Aguardando pr√≥xima execu√ß√£o... 13:42:56\n",
            "‚è≥ [1wk] Aguardando pr√≥xima execu√ß√£o... 13:42:56\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-a02e5d23e4ff>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# üìà Agora: treina o modelo LSTM Di√°rio especial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m model = train_lstm_model_diario(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0masset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-6b70fd102d22>\u001b[0m in \u001b[0;36mtrain_lstm_model_diario\u001b[0;34m(df, asset, interval, window_size, force_retrain)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0mreduce_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def exibir_carteira_virtual(filepath=\"/content/carteira_virtual.json\"):\n",
        "    try:\n",
        "        with open(filepath, \"r\") as f:\n",
        "            carteira = json.load(f)\n",
        "\n",
        "        print(\"üíº Carteira Virtual:\")\n",
        "        for chave, valor in carteira.items():\n",
        "            if isinstance(valor, float):\n",
        "                print(f\"‚Ä¢ {chave}: {valor:.2f}\")\n",
        "            else:\n",
        "                print(f\"‚Ä¢ {chave}: {valor}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao exibir carteira: {e}\")\n",
        "\n",
        "exibir_carteira_virtual()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvdwA0p13GjX",
        "outputId": "8b2cbcb6-a49e-4545-b900-51612ebb4752"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíº Carteira Virtual:\n",
            "‚Ä¢ capital_inicial: 10000.00\n",
            "‚Ä¢ capital_atual: 10000.00\n",
            "‚Ä¢ capital_maximo: 10000.00\n",
            "‚Ä¢ historico_capital: []\n",
            "‚Ä¢ em_operacao: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exibir_prediction_log()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qidyHubD3Ybb",
        "outputId": "a121b961-9a62-41b5-a350-d2caa66e5aa0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì≠ O prediction_log est√° vazio.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_analysis(\n",
        "    selected_timeframes=[\n",
        "        {\"interval\": \"1h\", \"period\": \"120d\", \"atr\": 0.03}\n",
        "    ],\n",
        "    plot_timeframes=[\"1h\"],\n",
        "    alert_timeframes=[\"1h\"],\n",
        "    retrain_models=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "3lZAmWtI3rf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_models(\"/content/models\")\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def limpar_model_results():\n",
        "    arquivos = glob.glob(\"/content/model_results_*.csv\")\n",
        "    if not arquivos:\n",
        "        print(\"üìÇ Nenhum arquivo model_results_*.csv encontrado.\")\n",
        "        return\n",
        "\n",
        "    for arquivo in arquivos:\n",
        "        try:\n",
        "            os.remove(arquivo)\n",
        "            print(f\"üßπ Arquivo deletado: {arquivo}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erro ao deletar {arquivo}: {e}\")\n",
        "\n",
        "    print(\"‚úÖ Todos os arquivos model_results_*.csv foram removidos.\")\n",
        "\n",
        "limpar_model_results()\n",
        "def limpar_prediction_log(path=\"prediction_log.csv\"):\n",
        "    if not os.path.exists(path):\n",
        "        print(\"‚ö†Ô∏è Arquivo de log n√£o encontrado.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.dropna(subset=[\"Date\"])\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "    df = df[df[\"Date\"].dt.year >= 2000]\n",
        "    df.to_csv(path, index=False)\n",
        "    print(\"‚úÖ Log limpo com sucesso. Entradas de 1970 removidas!\")\n",
        "\n",
        "limpar_prediction_log()\n",
        "import os\n",
        "os.remove(\"/content/prediction_log.csv\")\n",
        "import os\n",
        "if os.path.exists(\"prediction_log.csv\"):\n",
        "    os.remove(\"prediction_log.csv\")\n",
        "    print(\"üßπ Carteira virtual resetada com sucesso.\")"
      ],
      "metadata": {
        "id": "ctphA5Oq6AQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f /content/prediction_log.csv\n",
        "!rm -f /content/model_results_*.csv\n"
      ],
      "metadata": {
        "id": "-TAnU-WJ730p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simular_todos_trades(\n",
        "    prediction_log_path=\"/content/prediction_log.csv\",\n",
        "    df_candles=None,\n",
        "    timeframe=\"1h\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "IXxxZItbV0Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para treinar e comparar splits para BTC-USD no 1h\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "def train_lstm_btc_1h(df):\n",
        "    feature_cols = get_lstm_feature_columns()\n",
        "    target_cols = [\"High\", \"Low\", \"Close\"]\n",
        "\n",
        "    df = df.dropna(subset=feature_cols + target_cols).copy()\n",
        "\n",
        "    if len(df) <= 30:\n",
        "        print(\"‚ö†Ô∏è Dados insuficientes para treino.\")\n",
        "        return None\n",
        "\n",
        "    # Define splits\n",
        "    splits = {\n",
        "        \"70-30\": int(0.7 * len(df)),\n",
        "        \"80-20\": int(0.8 * len(df)),\n",
        "        \"85-15\": int(0.85 * len(df))\n",
        "    }\n",
        "\n",
        "    resultados = []\n",
        "\n",
        "    for nome, split_idx in splits.items():\n",
        "        df_train = df.iloc[:split_idx]\n",
        "        df_test = df.iloc[split_idx:]\n",
        "\n",
        "        scaler_x = MinMaxScaler()\n",
        "        scaler_y = MinMaxScaler()\n",
        "\n",
        "        X_train_scaled = scaler_x.fit_transform(df_train[feature_cols])\n",
        "        y_train_scaled = scaler_y.fit_transform(df_train[target_cols])\n",
        "\n",
        "        X_test_scaled = scaler_x.transform(df_test[feature_cols])\n",
        "        y_test_scaled = scaler_y.transform(df_test[target_cols])\n",
        "\n",
        "        window_size = 15\n",
        "\n",
        "        # Prepara sequ√™ncias\n",
        "        def create_sequences(X, y, window_size):\n",
        "            X_seq, y_seq = [], []\n",
        "            for i in range(window_size, len(X)):\n",
        "                X_seq.append(X[i-window_size:i])\n",
        "                y_seq.append(y[i])\n",
        "            return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "        X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, window_size)\n",
        "        X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, window_size)\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(64, input_shape=(window_size, len(feature_cols))))\n",
        "        model.add(Dense(3))\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "        es = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "        rlrop = ReduceLROnPlateau(patience=5, factor=0.5, verbose=1)\n",
        "\n",
        "        model.fit(X_train_seq, y_train_seq, validation_data=(X_test_seq, y_test_seq),\n",
        "                  epochs=150, batch_size=32, verbose=0, callbacks=[es, rlrop])\n",
        "\n",
        "        y_pred_scaled = model.predict(X_test_seq)\n",
        "        y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "        y_true = scaler_y.inverse_transform(y_test_seq)\n",
        "\n",
        "        r2_close = r2_score(y_true[:,2], y_pred[:,2])\n",
        "        mae_close = mean_absolute_error(y_true[:,2], y_pred[:,2])\n",
        "        rmse_close = mean_squared_error(y_true[:,2], y_pred[:,2], squared=False)\n",
        "\n",
        "        resultados.append({\n",
        "            \"Split\": nome,\n",
        "            \"R2_Close\": r2_close,\n",
        "            \"MAE_Close\": mae_close,\n",
        "            \"RMSE_Close\": rmse_close,\n",
        "            \"model\": model,\n",
        "            \"scaler_x\": scaler_x,\n",
        "            \"scaler_y\": scaler_y,\n",
        "            \"feature_cols\": feature_cols,\n",
        "            \"target_cols\": target_cols,\n",
        "            \"window_size\": window_size\n",
        "        })\n",
        "\n",
        "    df_resultados = pd.DataFrame([{k: v for k, v in r.items() if k != \"model\"} for r in resultados])\n",
        "    df_resultados.to_csv(\"/content/comparativo_splits_BTCUSD_1h.csv\", index=False)\n",
        "    print(\"\\nüìã Comparativo de splits salvo em /content/comparativo_splits_BTCUSD_1h.csv\")\n",
        "\n",
        "    # Escolhe o melhor\n",
        "    melhor = max(resultados, key=lambda x: x[\"R2_Close\"])\n",
        "\n",
        "    # Salva o melhor modelo\n",
        "    path_model = \"/content/models/lstm_model_BTCUSD_1h.h5\"\n",
        "    melhor[\"model\"].save(path_model)\n",
        "    joblib.dump({\n",
        "        \"scaler_x\": melhor[\"scaler_x\"],\n",
        "        \"scaler_y\": melhor[\"scaler_y\"],\n",
        "        \"feature_cols\": melhor[\"feature_cols\"],\n",
        "        \"target_cols\": melhor[\"target_cols\"],\n",
        "        \"window_size\": melhor[\"window_size\"]\n",
        "    }, path_model.replace(\".h5\", \"_meta.pkl\"))\n",
        "\n",
        "    print(f\"\\n‚úÖ Melhor split: {melhor['Split']} salvo como modelo definitivo!\")\n",
        "    print(f\"üèÜ R2_Close: {melhor['R2_Close']:.4f} | MAE: {melhor['MAE_Close']:.2f} | RMSE: {melhor['RMSE_Close']:.2f}\")\n",
        "    return df_resultados"
      ],
      "metadata": {
        "id": "9ZcMUiYFGIQq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IWVkUU4TJbw2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}